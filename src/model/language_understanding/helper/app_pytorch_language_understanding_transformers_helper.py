# -*- coding: utf-8 -*-
# ---- NOTE-OPTIONAL-CODING ----
# -*- coding: latin-1 -*-
"""
This module tests PytorchLanguageUnderstandingTransformersPretainedModelHelper functions.
"""
# ---- NOTE-PYLINT ---- C0302: Too many lines in module
# pylint: disable=C0302

from typing import List

import argparse

import os

import json

# ---- NOTE-PYLINT ---- E0401: Unable to import 'torch' (import-error)
# pylint: disable=E0401
import torch

# from transformers import BertForSequenceClassification

from model.language_understanding.helper.pytorch_language_understanding_transformers_helper \
    import PytorchLanguageUnderstandingTransformersPretainedModelHelper

from utility.debugging_helper.debugging_helper \
    import DebuggingHelper
from utility.configuration_helper.configuration_helper \
    import ConfigurationHelper

UTTERANCE_ROAD_NOT_TAKEN: str = \
    'Two roads diverged in a wood, and I -- ' \
    'I took the one less traveled by, And that has made all the difference.'

def divided_by_norm(a_tensor):
    """
    divided_by_norm(): simple normalization
    """
    return a_tensor / torch.norm(a_tensor, dim=1, keepdim=True)

def main_test_model_tokenizer_bert_base_new():
    """
    The main_test_model_tokenizer_bert_base_new() function can quickly test
    PytorchLanguageUnderstandingTransformersPretainedModelHelper functions.
    """
    # ------------------------------------------------------------------------
    # ---- NOTE-PYLINT ---- R0915: Too many statements
    # pylint: disable=R0915
    # ---- NOTE-PYLINT ---- C0301: Line too long
    # pylint: disable=C0301
    # ---- NOTE-PYLINT ---- R0914: Too many local variables (19/15) (too-many-locals)
    # pylint: disable=R0914
    parser = argparse.ArgumentParser()
    parser.add_argument(
        '--pytorch_transformers_vocab_file',
        default='vocab.txt',
        type=str,
        required=False,
        help=f'pytorch transformers vocab file')
    parser.add_argument(
        '--pytorch_transformers_model_dir_tokenizer',
        default='',
        type=str,
        required=False,
        help='Pytorch Transformers model tokenizer directory.')
    parser.add_argument(
        '--pytorch_transformers_model_file',
        default='pytorch_model.bin',
        type=str,
        required=False,
        help=f'pytorch transformers model file')
    parser.add_argument(
        '--pytorch_transformers_model_dir_learner',
        default='',
        type=str,
        required=False,
        help='Pytorch Transformers model learner directory.')
    parser.add_argument(
        '--pytorch_transformers_model_config_file',
        default='config.json',
        type=str,
        required=False,
        help='pytorch_transformers model configuration file.')
    parser.add_argument(
        '--pytorch_transformers_model_traced_output_file',
        default='',
        type=str,
        required=False,
        help='pytorch_transformers model traced output file.')
    args: argparse.Namespace = parser.parse_args()
    # ------------------------------------------------------------------------
    pytorch_transformers_model_dir_tokenizer: str = \
        args.pytorch_transformers_model_dir_tokenizer
    pytorch_transformers_model_dir_tokenizer = \
        r"/model_dte_proprietary/collected_DTE_pytorch/dte_tnlr-v3-3l_bing_cortana_mixture/tokenizer"
    pytorch_transformers_model_dir_learner: str = \
        args.pytorch_transformers_model_dir_learner
    pytorch_transformers_model_dir_learner = \
        r"/model_dte_proprietary/collected_DTE_pytorch/dte_tnlr-v3-3l_bing_cortana_mixture/model"
    pytorch_transformers_model_traced_output_file = \
        r"/model_dte_proprietary/collected_DTE_pytorch/dte_tnlr-v3-3l_bing_cortana_mixture/pytorch_model_traced.zip"
    vocab_file: str = \
        os.path.join(pytorch_transformers_model_dir_tokenizer, args.pytorch_transformers_vocab_file)
    config_file: str = \
        os.path.join(pytorch_transformers_model_dir_learner, args.pytorch_transformers_model_config_file)
    model_file: str = \
        os.path.join(pytorch_transformers_model_dir_learner, args.pytorch_transformers_model_file)
    # ------------------------------------------------------------------------
    tokenizer = PytorchLanguageUnderstandingTransformersPretainedModelHelper.bert_tokenizer_new( \
        vocab_file)
    DebuggingHelper.write_line_to_system_console_out(
        f'tokenizer={str(tokenizer)}')
    DebuggingHelper.write_line_to_system_console_out(
        f'tokenizer.vocab_size={tokenizer.vocab_size}')
    # ---- NOTE-OUTPUT ---- b'tokenizer.vocab_size=30522'
    # ------------------------------------------------------------------------
    with open(config_file) as config_file_handle:
        config_json = json.load(config_file_handle)
    config_default = PytorchLanguageUnderstandingTransformersPretainedModelHelper.bert_config_new_from_dictionary( \
        {})
    DebuggingHelper.write_line_to_system_console_out(
        f'config_default={str(config_default)}')
    config = PytorchLanguageUnderstandingTransformersPretainedModelHelper.bert_config_new_from_dictionary( \
        config_json)
    DebuggingHelper.write_line_to_system_console_out(
        f'config_json={str(config_json)}')
    DebuggingHelper.write_line_to_system_console_out(
        f'config={str(config)}')
    # ------------------------------------------------------------------------
    model = PytorchLanguageUnderstandingTransformersPretainedModelHelper.bert_model_from_pretrained(model_file, config=config)
    model.eval()
    # ------------------------------------------------------------------------
    utterance_road_not_taken: str = UTTERANCE_ROAD_NOT_TAKEN
    utterance_road_not_taken = ' ' + utterance_road_not_taken # ---- NOTE ---- this is needed for Roberta tokenizer, may not be required for BERT model
    # ------------------------------------------------------------------------
    token_id_lists: List[int] = tokenizer.encode(utterance_road_not_taken)
    DebuggingHelper.write_line_to_system_console_out(
        f'token_id_lists={str(token_id_lists)}')
    DebuggingHelper.write_line_to_system_console_out(
        f'len(token_id_lists)={len(token_id_lists)}')
    # ---- NOTE-PYLINT ---- C0301: Line too long
    # pylint: disable=C0301
    # ---- NOTE-OUTPUT ---- b'token_id_lists=[101, 2048, 4925, 17856, 5999, 1999, 1037, 3536, 1010, 1998, 1045, 1011, 1011, 1045, 2165, 1996, 2028, 2625, 6158, 2011, 1010, 1998, 2008, 2038, 2081, 2035, 1996, 4489, 1012, 102]'
    # ---- NOTE-OUTPUT ---- b'len(token_id_lists)=30'
    encoded = tokenizer.encode_plus(utterance_road_not_taken)
    DebuggingHelper.write_line_to_system_console_out(
        f'encoded={str(encoded)}')
    DebuggingHelper.write_line_to_system_console_out(
        f'len(encoded)={len(encoded)}')
    # ---- NOTE-PYLINT ---- C0301: Line too long
    # pylint: disable=C0301
    # ---- NOTE-OUTPUT ---- b"encoded={'input_ids': [101, 2048, 4925, 17856, 5999, 1999, 1037, 3536, 1010, 1998, 1045, 1011, 1011, 1045, 2165, 1996, 2028, 2625, 6158, 2011, 1010, 1998, 2008, 2038, 2081, 2035, 1996, 4489, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
    # ---- NOTE-OUTPUT ---- b'len(encoded)=3'
    # ------------------------------------------------------------------------
    # ---- NOTE-PYLINT ---- E1102: torch.tensor is not callable (not-callable)
    # pylint: disable=E1102
    input_ids = torch.tensor(tokenizer.encode(utterance_road_not_taken, add_special_tokens=True)).unsqueeze(0)  # ---- Batch size 1
    outputs = model(input_ids)
    # ==== outputs = model(input_ids, masked_lm_labels=input_ids)
    DebuggingHelper.write_line_to_system_console_out(
        f'input_ids={str(input_ids)}')
    DebuggingHelper.write_line_to_system_console_out(
        f'len(outputs)={len(outputs)}')
    # ==== len(outputs)=2
    DebuggingHelper.write_line_to_system_console_out(
        f'len(outputs[0][0][0])={len(outputs[0][0][0])}')
    # ==== len(outputs[0][0][0])=768
    DebuggingHelper.write_line_to_system_console_out(
        f'len(outputs[0][0][1])={len(outputs[0][0][1])}')
    # ==== len(outputs[0][0][1])=768
    DebuggingHelper.write_line_to_system_console_out(
        f'len(outputs[0][0][{len(outputs[0][0])-1}])={len(outputs[0][0][len(outputs[0][0])-1])}')
    # ==== len(outputs[0][0][29])=768
    DebuggingHelper.write_line_to_system_console_out(
        f'len(outputs[0][0])={len(outputs[0][0])}')
    # ==== len(outputs[0][0])=30
    DebuggingHelper.write_line_to_system_console_out(
        f'len(outputs[0])={len(outputs[0])}')
    # ==== len(outputs[0])=1
    # ==== DebuggingHelper.write_line_to_system_console_out(
    # ====     f'outputs[0].get_shape()={outputs[0].get_shape()}')
    DebuggingHelper.write_line_to_system_console_out(
        f'len(outputs[1])={len(outputs[1])}')
    # ==== len(outputs[1])=1
    DebuggingHelper.write_line_to_system_console_out(
        f'len(outputs[1][0])={len(outputs[1][0])}')
    # ==== len(outputs[1][0])=768
    # ==== DebuggingHelper.write_line_to_system_console_out(
    # ====     f'outputs[1].get_shape()={outputs[1].get_shape()}')
    DebuggingHelper.write_line_to_system_console_out(
        f'outputs[0][0][0]={outputs[0][0][0]}')
    # ==== outputs[0][0][0]=tensor([-2.6461e-01, -2.0363e-01,  7.3369e-02,  3.4607e-02, -2.7882e-02,
    # ====     -1.4411e-01,  5.7426e-02,  5.1801e-01, -3.9106e-02, -2.4470e-01,
    # ====      3.5038e-02,  7.0886e-02, -8.6316e-02,  7.9232e-02,  2.0974e-01,
    # ====     -3.6466e-01, -2.7615e-01,  3.2141e-01, -1.7689e-01,  5.2981e-01,
    # ====      2.9504e-01, -8.8848e-02, -1.3967e-01, -2.0979e-01, -9.2797e-02,
    # ====      1.1751e-02, -1.8606e-01,  3.5348e-01,  1.2636e-01, -1.2570e-01,
    # ====     -2.1419e-01,  2.9827e-01,  4.4272e-01,  1.9956e-01, -1.1006e-02,
    # ====      1.8520e-01,  4.7272e-02, -2.0622e-01, -1.1534e-02,  6.0105e-01,
    # ====     -1.8891e-01, -1.1346e-01,  1.3196e-01,  1.7466e-01, -5.5444e-01,
    # ====      3.5640e-01, -9.4742e-02,  5.1780e-01,  3.0898e-01,  7.9808e-02,
    # ====      2.0342e-01, -1.1575e-01,  1.5352e-01,  1.9301e-01,  1.5576e-02,
    # ====     -3.5441e-01,  2.3429e-01, -4.5156e-01,  1.8079e-01, -1.9095e-01,
    # ====      1.2982e-01,  1.1923e-01, -3.8647e-01,  9.8162e-02, -9.4160e-02,
    # ====     -1.0136e-01,  1.0782e-01, -1.1520e-02,  2.1980e-01,  2.5180e-01,
    # ====     -4.1664e-02, -1.6604e-02,  3.3259e-01, -3.7257e-01, -1.8578e-01,
    # ====     -2.7519e-01,  1.3871e-01,  3.3695e-02, -1.6234e-01,  7.0673e-02,
    # ====     -1.0618e-01, -1.2268e-01,  1.9852e-01,  6.5488e-02,  4.2826e-02,
    # ====     -6.5848e-02,  1.3828e-01, -4.1876e-01, -2.3477e-01,  1.1212e-01,
    # ====     -1.9239e-01, -1.7132e-01,  6.4710e-02,  2.1210e-01,  4.3832e-01,
    # ====      1.3649e-01, -2.4878e-03,  4.4615e-02,  1.4066e-01, -1.6090e-01,
    # ====      7.9226e-02,  2.2863e-01,  9.0759e-02,  2.0595e-01,  2.1348e-01,
    # ====      2.0633e-01, -2.7556e-02,  7.6414e-01,  3.2611e-01, -1.5955e-01,
    # ====     -1.4049e-01,  1.7655e-01, -3.9086e-01,  2.4254e-01,  2.9406e-02,
    # ====      9.0184e-02, -2.8814e-01,  8.2897e-02, -3.1311e-01,  1.0830e-01,
    # ====     -2.3850e-01,  7.8164e-02,  3.1546e-01,  1.4278e-01,  1.2098e-01,
    # ====     -1.6499e-01, -7.2838e-02, -2.1829e-01,  3.4395e-01, -4.1535e-02,
    # ====     -2.3849e-01, -5.4405e-02, -1.1924e-01,  1.0269e-01, -2.9531e-01,
    # ====     -2.8271e-01, -2.3441e-01,  7.2043e-02, -2.9719e-01,  9.3838e-02,
    # ====     -1.6869e-01,  2.6995e-01, -6.8553e-02,  1.0581e-01,  2.6102e-01,
    # ====     -1.9109e-03,  8.4615e-02, -1.0025e-01, -4.1328e-01, -7.7803e-02,
    # ====     -2.4363e-02,  3.7933e-01,  2.0088e-02,  2.2328e-01,  1.5602e-01,
    # ====     -1.8225e-02,  8.5486e-02, -1.9210e-01, -3.4859e-01,  2.7433e-01,
    # ====      2.4429e-02, -8.3559e-02,  3.3867e-01, -1.9321e-01,  1.3366e-01,
    # ====     -2.3019e-01, -1.3276e-01, -2.9485e-02,  1.2194e-01, -1.9845e-01,
    # ====     -2.0890e-01, -1.7311e-01,  2.5306e-01,  1.1806e-01,  3.4250e-01,
    # ====      3.6743e-02, -7.0785e-02, -3.2113e-01, -1.9615e-02,  6.7260e-02,
    # ====     -5.9412e-02,  5.0086e-02, -3.2956e-01,  1.1696e-01, -4.2284e-02,
    # ====     -2.6723e-01, -2.0160e-01,  6.7561e-02, -2.2682e-01,  1.2040e-01,
    # ====      9.5166e-02, -1.0936e-01,  8.2189e-02,  2.7617e-01, -1.6620e-01,
    # ====      3.0141e-01, -4.3507e-01,  3.1462e-01, -1.5569e-01, -2.3216e-01,
    # ====     -3.8310e-01, -8.9517e-02,  3.0210e-02,  1.2433e-01,  1.3709e-01,
    # ====      3.6516e-03,  3.3112e-01,  2.4504e-01, -2.7678e-01, -2.0005e-01,
    # ====      9.1313e-02,  1.1598e-01, -9.4609e-02,  2.3024e-01,  2.5520e-01,
    # ====     -1.9752e-02,  8.5216e-02,  1.1928e-01, -1.3479e-02, -1.7245e-01,
    # ====     -3.2884e-01,  1.3540e-01, -2.4940e-01, -1.5850e-01,  1.1291e-01,
    # ====     -1.7870e-01,  4.8411e-01,  1.3323e-02, -1.1678e-01,  2.0504e-01,
    # ====     -4.1399e-01, -5.7793e-02,  3.3229e-01, -4.4903e-02,  1.3638e-01,
    # ====      1.1932e-01, -3.7529e-01, -6.7986e-02, -6.3099e-02,  6.6642e-02,
    # ====     -3.4121e-01,  7.3310e-03, -1.4914e-01,  4.7354e-03, -2.4698e-02,
    # ====     -8.3875e-02, -1.1780e-01,  6.4853e-02, -5.6332e-02, -5.6887e-02,
    # ====      3.3460e-01,  1.3221e-02, -8.8456e-02, -4.2105e-01,  9.4347e-02,
    # ====     -1.8704e-01, -3.2981e-02, -4.8517e-01,  1.6929e-01,  1.3850e-03,
    # ====      1.8559e-01, -2.6754e-01, -7.9623e-02, -3.2562e-01,  1.3168e-02,
    # ====      2.4752e-01,  2.5289e-01, -3.9224e-01, -1.1533e-01, -2.3031e-01,
    # ====      1.8685e-01,  4.2684e-02, -1.8489e-01, -1.3104e-01, -9.9493e-02,
    # ====      1.8844e-01,  6.3197e-02, -4.3004e-01, -9.3259e-04,  3.9579e-01,
    # ====     -8.6845e-02,  6.6229e-02, -2.2420e-04, -1.7010e-01, -4.7064e-03,
    # ====     -2.2135e-01,  3.3608e-01,  7.6052e-03,  1.2614e-02, -1.2141e-01,
    # ====     -1.6066e-01, -4.9116e-01,  5.8025e-02,  1.3658e-01, -3.8006e-01,
    # ====      7.3409e-02,  1.9622e-01, -3.4345e-02, -2.0450e-01,  1.8275e-01,
    # ====      1.1343e-01,  8.1315e-02,  4.0910e-01,  1.4530e-01, -3.9337e-03,
    # ====      3.1517e-02,  1.2860e-02, -2.4900e-01, -3.5774e-01, -1.0194e-01,
    # ====      3.0378e-01, -2.5668e-02, -1.4570e-01,  2.6095e-01, -2.4262e-01,
    # ====     -2.1234e-01,  1.0087e-03,  4.2318e-01,  7.0718e-02,  6.3755e-03,
    # ====     -1.8329e-01, -6.0549e-02, -4.1509e-02,  3.2484e-02,  1.0162e-01,
    # ====     -8.9029e-02,  5.9278e-02, -5.6250e-02,  8.5666e-02,  4.5277e-02,
    # ====      4.1576e-03, -2.7324e-02, -2.6321e-03, -1.5469e-01, -1.8206e-01,
    # ====      1.9268e-01,  4.1067e-01,  1.9187e-01,  1.7414e-01,  1.2333e-01,
    # ====     -2.2608e-01,  7.9653e-02,  1.8854e-01, -2.6272e-01, -2.2209e-01,
    # ====      6.9383e-02, -9.0200e-02, -3.1877e-02, -1.3218e-03, -1.2837e-01,
    # ====      1.3873e-01, -3.4617e-01,  7.5677e-03,  2.2559e-01, -2.2137e-02,
    # ====      2.7324e-01,  2.3485e-01,  9.2602e-03, -3.8325e-01, -1.7957e-01,
    # ====     -1.3419e-01, -1.1085e-01, -4.2435e-02, -2.5487e-01,  2.3207e-01,
    # ====     -4.1326e-02, -2.4448e-01, -2.7540e-01, -3.0751e-01, -3.0569e-01,
    # ====      4.9559e-01, -1.4950e-01,  2.5952e-02, -1.9594e-01, -1.4275e-01,
    # ====     -7.6059e-02, -2.3345e-01,  1.5803e-01, -7.7514e-03,  3.1740e-01,
    # ====      1.3470e-02,  5.8616e-01, -1.8633e-01, -2.7264e-02, -1.4352e-02,
    # ====     -6.3823e-02, -2.5751e-01,  2.2132e-01, -3.3530e-01, -1.7322e-03,
    # ====      1.1851e-01,  7.1747e-02, -6.1663e-02, -7.9370e-02,  1.2178e-01,
    # ====      1.4113e-01,  7.7433e-03,  9.4362e-02,  2.1413e-01, -3.9224e-01,
    # ====      8.9567e-02,  2.5710e-01,  7.5085e-02,  1.5162e-01,  3.4409e-01,
    # ====      3.7166e-02, -3.9380e-01,  5.8182e-02,  2.3750e-01, -1.1639e-01,
    # ====      4.8749e-01, -1.8404e-01, -6.2756e-02, -3.0816e-01, -1.3697e-01,
    # ====      4.8233e-03,  1.8531e-01,  2.0652e-01, -2.6847e-01,  1.4561e-01,
    # ====      8.0255e-02, -1.1806e-01, -6.7657e-02,  2.9665e-02, -1.7337e-01,
    # ====      1.3862e-01,  1.6562e-01, -1.9410e-01,  5.7390e-03,  1.4806e-01,
    # ====      9.2706e-02, -6.7380e-02, -1.0735e-01,  4.0564e-02, -1.6982e-01,
    # ====      3.5526e-02,  2.2909e-02, -3.0660e-02,  1.0405e-01, -1.2595e-01,
    # ====     -1.7543e-01, -1.5751e-04, -4.4486e-01,  5.9281e-02, -2.5956e-02,
    # ====      1.9144e-01, -6.2531e-02,  1.7385e-01, -9.7092e-02,  3.2472e-01,
    # ====     -8.1229e-02,  9.4178e-02, -3.4538e-01,  2.2226e-01, -2.7687e-01,
    # ====     -2.1146e-01,  8.7170e-02, -1.6716e-01, -3.6091e-02,  2.0910e-01,
    # ====      1.3820e-01, -2.8684e-01, -4.4415e-01,  4.4376e-01,  3.0499e-02,
    # ====     -1.2671e-01,  2.2865e-01, -2.6495e-01, -2.5278e-01,  1.8414e-01,
    # ====     -3.0034e-01,  4.2478e-02, -2.6046e-01, -6.6390e-02, -3.7592e-01,
    # ====     -1.7422e-01,  1.1707e-01,  8.2765e-02, -2.0940e-01,  7.2047e-02,
    # ====      5.6236e-02, -3.0797e-01, -8.9740e-02, -5.7259e-02, -1.2272e-01,
    # ====     -5.3406e-02,  1.3450e-01,  6.2486e-02,  3.2015e-01,  1.0430e-01,
    # ====     -6.0523e-02,  2.3147e-01,  1.2914e-01,  1.9695e-01,  2.3997e-01,
    # ====      1.6453e-02,  1.4818e-01, -2.9654e-02,  1.4968e-02, -2.6127e-01,
    # ====      2.2374e-01,  7.9669e-02, -2.6503e-01,  4.8627e-02,  1.7440e-01,
    # ====      1.6380e-01,  7.8886e-02,  8.2267e-02,  1.9266e-01, -3.4614e-01,
    # ====      1.5399e-02,  2.4605e-01,  7.5943e-02,  5.0941e-02,  2.7304e-01,
    # ====     -3.1354e-01,  1.1718e-02, -2.1115e-01,  8.3167e-02,  1.8771e-01,
    # ====     -3.2826e-01, -2.3783e-01,  1.2360e-01, -5.1753e-03,  9.5240e-02,
    # ====      1.7413e-01,  2.5630e-02, -1.0534e-01, -4.7585e-01,  5.1275e-01,
    # ====     -8.5402e-03,  5.4304e-02,  1.2032e-01,  1.0307e-01, -3.8560e-03,
    # ====     -1.7317e-01,  9.9030e-02, -2.0757e-01,  1.2616e-01, -2.3296e-01,
    # ====     -1.8715e-01,  1.2719e-02, -3.7845e-02,  2.2577e-01,  6.4888e-02,
    # ====      2.2906e-01, -1.7816e-01,  1.6627e-02, -6.9271e-02,  3.9562e-01,
    # ====      3.2091e-02, -1.6152e-01, -1.7693e-01, -7.4656e-03, -3.0512e-01,
    # ====      6.4508e-02, -9.6164e-02, -6.5856e-02,  1.4518e-01,  2.0208e-01,
    # ====      3.8740e-01, -4.4397e-01, -1.0347e-01, -2.5444e-02,  2.5264e-02,
    # ====      7.0283e-03,  2.8192e-01, -1.5903e-01, -1.5281e-01,  2.0362e-01,
    # ====      1.2531e-01,  1.5130e-01,  1.6698e-02,  1.2601e-01,  1.5973e-01,
    # ====     -3.2584e-01,  5.5003e-03, -2.8866e-01, -2.3350e-01, -2.0952e-01,
    # ====     -2.0578e-01,  3.5571e-01,  8.5300e-02, -1.4967e-01,  3.7727e-01,
    # ====     -4.1135e-03, -5.2260e-02,  3.4076e-01, -4.1757e-02,  4.5901e-02,
    # ====     -8.7859e-02, -1.5219e-01,  2.8436e-01,  9.1669e-02, -5.9628e-02,
    # ====     -1.4770e-01,  1.3001e-01,  1.7528e-01, -3.2356e-01, -1.4918e-01,
    # ====     -1.0023e-01,  6.5433e-02, -1.0431e-01, -4.1751e-02,  1.3214e-01,
    # ====      2.1592e-02,  7.9181e-02,  8.8145e-02,  1.2713e-01,  3.7962e-02,
    # ====      3.3972e-01,  3.0160e-01,  3.0284e-02, -4.7681e-01,  5.6716e-02,
    # ====      1.0526e-01,  2.7174e-01,  3.1571e-01, -2.2448e-01, -1.8434e-01,
    # ====     -3.0558e-01,  2.5842e-02,  9.2633e-02, -1.2072e-01,  9.2990e-02,
    # ====      1.6772e-01, -6.5534e-02,  2.8821e-01, -1.8787e-01, -1.0918e-01,
    # ====     -2.5714e-02,  6.7075e-03, -1.3934e-01, -2.3482e-02, -2.2967e-01,
    # ====     -3.7293e-01,  2.8298e-01,  2.2518e-02,  3.0597e-01, -5.1878e-02,
    # ====     -1.8124e-01, -1.2897e-01,  1.5932e-01, -1.4275e-01,  1.2437e-01,
    # ====     -1.8036e-01,  1.0918e-01, -1.6441e-01,  1.8474e-03,  6.3394e-02,
    # ====     -8.4879e-02,  2.6818e-01, -1.7149e-01, -1.9312e-01,  7.4047e-02,
    # ====      1.1044e-01, -9.1325e-02,  2.7694e-01, -2.7626e-01,  1.0049e-01,
    # ====      1.2901e-01, -3.4428e-01, -8.0094e-03,  3.5294e-01,  4.5280e-01,
    # ====      2.3015e-01, -1.8469e-01, -2.1385e-01, -5.5550e-02, -1.5806e-01,
    # ====      1.5012e-01,  2.7594e-01,  1.0215e-01,  2.5086e-01,  5.9249e-02,
    # ====      8.6016e-02, -1.5014e-01,  2.5307e-02, -3.3688e-01,  2.7600e-01,
    # ====     -2.1504e-01,  1.2175e-01, -1.5710e-01, -1.1721e-01, -5.4469e-02,
    # ====     -5.4271e-02,  1.4616e-01,  2.7228e-01,  3.8158e-01,  2.0989e-01,
    # ====     -8.3144e-02, -1.4776e-01, -1.1991e-01, -7.6203e-03, -2.7356e-01,
    # ====     -1.8168e-01, -2.8396e-03, -1.9044e-01,  1.0887e-01, -1.3345e-01,
    # ====     -1.7414e-01, -8.7644e-02, -1.2988e-02,  1.0888e-01,  5.7863e-02,
    # ====     -3.5638e-01,  2.2247e-01,  2.3954e-02,  1.2922e-01,  1.7861e-02,
    # ====      4.3485e-02,  1.1234e-01, -1.3109e-02, -1.2601e-01,  2.8826e-01,
    # ====      2.5117e-01,  2.4645e-01, -2.5668e-01,  2.6439e-02, -3.8188e-01,
    # ====      1.1469e-01, -1.1800e-01,  1.5343e-01, -3.9145e-01,  8.2521e-02,
    # ====     -2.7792e-01, -4.5229e-02,  1.9573e-01, -1.3514e-01, -2.4898e-02,
    # ====     -7.6172e-03, -2.6266e-01,  5.5089e-02, -2.0771e-01,  1.9703e-03,
    # ====      5.0695e-02, -2.7184e-01,  2.8412e-01, -4.5590e-02, -1.8116e-01,
    # ====     -1.8842e-01,  8.5472e-03,  9.9711e-03, -2.8700e-01, -5.1674e-01,
    # ====      1.4067e-02,  1.1652e-01,  2.9589e-01,  8.6066e-02, -4.9153e-02,
    # ====     -3.0358e-01, -1.8295e-02, -7.6782e-02,  7.9164e-02, -3.0543e-01,
    # ====     -9.9874e-02, -4.4190e-02,  2.3373e-02, -2.3091e-02, -4.1636e-02,
    # ====      8.5354e-02,  1.6516e-02,  2.3044e-01, -4.9916e-02,  1.8241e-01,
    # ====     -1.0850e-02,  3.5555e-03,  4.2739e-02], grad_fn=<SelectBackward>)
    DebuggingHelper.write_line_to_system_console_out(
        f'outputs[0][0][1]={outputs[0][0][1]}')
    DebuggingHelper.write_line_to_system_console_out(
        f'outputs[0][0][{len(outputs[0][0])-1}]={outputs[0][0][len(outputs[0][0])-1]}')
    DebuggingHelper.write_line_to_system_console_out(
        f'outputs[0][0]={outputs[0][0]}')
    DebuggingHelper.write_line_to_system_console_out(
        f'outputs[0]={outputs[0]}')
    DebuggingHelper.write_line_to_system_console_out(
        f'outputs[1]={outputs[1]}')
    # ==== outputs[1]=tensor([[-5.6840e-02,  1.2102e-01,  5.4267e-02, -1.7882e-01, -5.9378e-02,
    # ====      -2.5190e-02, -1.8542e-01,  1.1330e-01, -8.5156e-02,  2.7449e-02,
    # ====      -1.1533e-01, -6.9949e-02,  2.9072e-02,  7.2202e-02, -1.2615e-01,
    # ====      -6.8474e-02,  1.2424e-01,  3.4015e-02,  4.5800e-02, -1.9750e-01,
    # ====      -3.6082e-02,  1.0659e-01,  8.9335e-02,  2.9048e-03, -1.2068e-01,
    # ====       3.1624e-02,  5.4126e-02, -8.2480e-02,  5.4521e-02,  5.2366e-04,
    # ====       2.1537e-02,  1.3240e-02,  8.7528e-02,  8.2964e-02,  1.0755e-01,
    # ====       1.9001e-01,  1.2937e-01,  2.0770e-01, -8.6263e-02,  2.1567e-01,
    # ====      -5.1314e-02,  3.8273e-02,  1.0613e-01,  2.0139e-01,  1.4725e-01,
    # ====       9.6549e-02, -1.9363e-04,  4.8206e-02,  2.2943e-02,  8.5209e-02,
    # ====      -1.1869e-02, -1.0081e-01, -7.7462e-02, -1.1135e-01, -7.1041e-02,
    # ====      -1.2676e-01, -9.8951e-02, -1.4974e-01, -7.1366e-02, -3.4510e-02,
    # ====      -2.5932e-02,  5.1107e-02, -1.0125e-01,  1.3945e-02,  1.2729e-01,
    # ====      -5.3334e-03, -8.2100e-02,  1.6692e-01,  5.7623e-02,  7.5580e-02,
    # ====      -5.4259e-02, -1.2887e-01, -4.8602e-02,  1.6521e-01,  7.4884e-04,
    # ====      -2.2292e-01, -9.2027e-02, -1.6528e-01, -2.7426e-02,  9.1623e-03,
    # ====      -1.2156e-01,  1.6405e-01,  1.2467e-01, -8.4664e-02,  8.9838e-02,
    # ====       4.6360e-02,  1.3797e-01, -1.5529e-02,  1.2342e-01,  1.2294e-01,
    # ====       2.0543e-02, -8.4751e-02, -2.0567e-01, -6.5298e-02, -2.6013e-01,
    # ====       9.4754e-02,  3.5770e-02,  1.2657e-01,  1.7236e-03,  5.3038e-02,
    # ====      -1.3484e-01, -1.0119e-01,  3.6641e-02, -1.5101e-01, -5.5617e-02,
    # ====       7.6505e-02,  1.5454e-01,  1.0021e-01, -2.8205e-02, -3.7353e-02,
    # ====       3.5241e-02, -6.0153e-02, -7.0000e-02,  6.3366e-02,  2.6344e-03,
    # ====       1.0417e-01,  6.8686e-02, -4.7277e-02, -1.6674e-01,  5.9168e-02,
    # ====       5.5602e-02, -5.6532e-03,  1.4333e-01,  1.5815e-01, -8.2680e-02,
    # ====      -8.1772e-02,  6.1312e-02,  5.9227e-02, -8.6477e-03, -1.4850e-01,
    # ====       3.0425e-02, -1.8356e-01, -4.5714e-02,  1.2168e-01,  1.1220e-01,
    # ====       3.5620e-02,  3.4331e-02,  1.3255e-02, -1.1597e-01,  2.1169e-01,
    # ====       3.0962e-02,  3.5845e-02, -6.0353e-03,  1.2214e-01,  9.5598e-02,
    # ====       6.8402e-03,  1.7808e-02, -1.3455e-01,  2.6569e-01,  1.9537e-02,
    # ====      -7.6120e-02,  1.2628e-01, -3.6476e-02, -5.7976e-02, -1.1293e-01,
    # ====      -1.0386e-01,  6.8322e-02, -7.2434e-02, -1.1541e-01,  1.1876e-01,
    # ====       1.3955e-01, -3.3493e-02, -1.5959e-02, -2.5075e-02, -4.3500e-02,
    # ====       1.0231e-01,  1.8141e-01,  2.4925e-02, -2.5827e-01, -1.1987e-01,
    # ====       4.2719e-02, -7.7398e-02, -1.8067e-01,  8.9664e-02,  2.0637e-01,
    # ====      -4.1326e-02, -6.3219e-02,  2.2464e-01, -1.8563e-01,  1.4122e-02,
    # ====       2.7181e-02, -1.6972e-01,  7.9152e-02,  1.1873e-02,  5.0235e-02,
    # ====      -6.3145e-02,  1.7432e-01,  1.0753e-02,  1.0583e-01,  8.5177e-02,
    # ====       2.9009e-02, -4.8579e-02,  7.3525e-03, -1.2638e-01,  2.2839e-01,
    # ====      -7.3037e-02,  9.3887e-02, -1.2372e-01, -1.6278e-01, -5.6000e-02,
    # ====      -4.5320e-02,  6.5468e-02, -2.3404e-01, -1.7921e-02, -1.6416e-02,
    # ====       1.4703e-01,  3.8114e-02, -7.8833e-02, -1.8485e-01,  2.2809e-01,
    # ====       1.8895e-01, -2.1764e-02, -1.0027e-01, -8.3171e-02,  4.4251e-02,
    # ====      -3.5786e-02,  3.6981e-02, -9.1431e-02, -1.0797e-03, -1.9780e-01,
    # ====      -1.1326e-01,  2.7428e-01, -2.6240e-01, -9.2934e-02, -4.5646e-03,
    # ====       2.3767e-01, -1.0719e-01, -3.9477e-02, -1.2773e-01,  4.7839e-02,
    # ====      -1.0312e-02, -5.4064e-03,  1.9359e-02, -1.7004e-01,  2.5751e-01,
    # ====      -2.7565e-02,  2.8095e-02, -3.9440e-02, -7.3074e-02, -1.0074e-01,
    # ====      -8.4734e-02,  6.0095e-02,  1.9367e-02, -4.3142e-02,  1.4990e-01,
    # ====      -1.9739e-01, -7.0134e-02, -1.4680e-02,  3.4759e-02,  1.2484e-01,
    # ====       1.1169e-01, -3.3875e-02, -8.3157e-03,  4.6777e-02,  5.8146e-05,
    # ====       1.6231e-01,  9.7626e-02, -4.7196e-02,  1.6273e-01, -1.5474e-01,
    # ====      -6.0868e-02, -2.2715e-01,  1.0217e-01,  6.6031e-02, -3.9830e-02,
    # ====      -1.1635e-01, -8.6991e-02,  1.9242e-01, -5.1012e-02,  1.1185e-01,
    # ====       1.1916e-01,  7.5984e-02, -4.4597e-02, -5.2521e-03,  2.4992e-02,
    # ====       6.7423e-02,  1.1898e-01, -1.3785e-01, -1.4824e-01,  5.5593e-02,
    # ====       3.9484e-02,  1.3459e-01,  8.7929e-02,  4.3389e-02, -1.6883e-01,
    # ====      -1.2602e-01, -7.2992e-02,  1.8207e-01, -7.0931e-02, -6.0974e-02,
    # ====       1.1057e-01,  5.4689e-02,  1.9082e-01, -7.4982e-02,  1.5462e-01,
    # ====      -3.8666e-02,  1.1740e-01,  1.4579e-02, -2.3672e-02, -6.8232e-02,
    # ====      -2.3999e-02,  2.0175e-01, -5.2104e-02,  2.4391e-02, -2.4394e-02,
    # ====       2.0393e-02, -9.2739e-03,  3.3686e-02, -1.5459e-01,  2.0375e-02,
    # ====       1.1411e-01, -2.0243e-02, -1.8894e-02, -1.3061e-01,  3.1800e-02,
    # ====       8.2543e-02,  5.7910e-02, -2.8746e-02,  9.3684e-02, -7.5259e-02,
    # ====      -1.7245e-01,  6.5088e-03,  8.7088e-02,  1.8870e-02,  1.0885e-01,
    # ====       2.0748e-01, -2.2548e-01, -1.2258e-01, -1.7985e-01,  2.3052e-02,
    # ====      -3.9963e-02, -1.3563e-01, -3.4311e-02,  4.8642e-02,  4.5684e-03,
    # ====      -9.3549e-02, -2.9623e-02, -5.3273e-02,  6.1488e-02, -2.0620e-01,
    # ====      -4.3695e-03,  6.8880e-02,  5.8324e-02, -7.8947e-02,  2.5591e-01,
    # ====      -1.3376e-01, -1.0903e-02, -1.5758e-01, -1.5212e-01,  1.3203e-01,
    # ====       1.4335e-01,  2.1533e-02, -3.2328e-02,  1.3950e-01, -3.0800e-02,
    # ====       1.8522e-01, -7.5932e-02, -2.9234e-02,  1.3274e-01, -2.5673e-02,
    # ====      -1.6797e-01,  4.2564e-02, -1.4084e-01, -1.0620e-01, -8.6237e-02,
    # ====      -9.4089e-02, -6.8205e-02, -8.2298e-02, -2.2967e-01,  5.2276e-02,
    # ====       1.2497e-01,  4.2834e-02,  3.7271e-02, -1.1625e-01, -3.3587e-02,
    # ====      -7.6508e-02, -2.6111e-02, -1.0315e-01,  3.7803e-02,  8.5726e-02,
    # ====      -1.7161e-01, -3.3666e-02, -3.3890e-01, -3.0868e-02, -3.6872e-02,
    # ====       1.3617e-01, -2.3485e-02,  4.4820e-02,  1.9354e-02,  8.7104e-02,
    # ====       1.7999e-01,  8.5633e-02,  2.2037e-01, -1.0934e-01, -7.9611e-02,
    # ====       1.4716e-01,  1.2329e-01, -1.2114e-01,  1.4964e-02,  8.6994e-02,
    # ====       6.1567e-03,  1.9579e-01,  2.1022e-02, -1.1128e-01, -1.1996e-01,
    # ====       9.4823e-02, -1.4851e-01,  1.1530e-01,  1.6360e-01,  3.5644e-02,
    # ====      -3.3367e-02, -5.5324e-02, -2.6636e-01,  2.0120e-02,  1.7009e-01,
    # ====       5.6741e-03,  5.0008e-02,  4.7960e-02, -2.1037e-02, -5.1161e-02,
    # ====       1.2382e-01,  3.9621e-02, -2.2778e-03,  9.5245e-03, -1.9012e-01,
    # ====       2.1719e-01, -2.0383e-01,  3.6888e-02,  2.4471e-01, -5.2150e-02,
    # ====      -1.6705e-01,  6.0199e-02, -3.8036e-02,  1.3157e-01,  3.3076e-02,
    # ====       8.7510e-02, -2.3924e-01, -4.6927e-02, -2.5458e-02,  1.7245e-01,
    # ====       1.8863e-01, -1.3847e-01, -3.7132e-02,  1.1385e-02,  1.5120e-02,
    # ====       4.5497e-02,  5.1046e-02, -1.7219e-01,  1.1731e-02, -1.3866e-01,
    # ====      -4.0682e-02,  1.0749e-01, -1.2592e-01,  1.3980e-01, -3.6324e-02,
    # ====       5.0127e-02,  1.4574e-01, -5.6216e-02, -1.5001e-01,  2.6982e-02,
    # ====      -4.3810e-02,  1.3381e-01,  2.5009e-02, -9.1449e-02,  9.2682e-02,
    # ====      -5.3324e-02,  1.9692e-01, -1.5413e-01,  9.7126e-03, -1.0213e-01,
    # ====      -7.8610e-02,  1.1937e-01, -1.4239e-01, -2.3183e-02, -1.1179e-02,
    # ====       6.7349e-02, -1.2203e-01, -5.8740e-02, -6.3356e-02,  5.6189e-02,
    # ====       3.1073e-02, -1.3813e-01, -6.4885e-02,  8.6971e-02, -1.0771e-01,
    # ====      -5.8406e-02, -9.5971e-02, -3.4599e-02,  1.0133e-01,  5.7441e-02,
    # ====       1.3762e-01, -8.3359e-03, -6.0381e-03,  2.1311e-02, -1.0956e-01,
    # ====       9.9783e-02, -1.0185e-01, -1.2887e-01, -1.5091e-01,  2.3543e-02,
    # ====      -7.1058e-02,  1.1712e-01,  1.3283e-01,  1.0994e-01, -1.1883e-01,
    # ====      -4.7820e-02,  1.1631e-01, -2.6797e-02, -6.5061e-03,  1.1460e-02,
    # ====      -9.0657e-02,  4.9343e-02, -5.6267e-02,  2.7392e-01, -1.2187e-01,
    # ====      -6.1030e-02,  1.3512e-01,  2.3879e-02,  2.2715e-02, -4.3495e-02,
    # ====      -1.5632e-01, -1.1100e-02, -1.2919e-02, -3.8113e-02, -1.4240e-01,
    # ====      -4.0456e-02, -1.5680e-01,  1.8037e-02, -3.7648e-02, -1.0374e-02,
    # ====      -1.4063e-01, -4.5768e-02, -7.8082e-02,  1.5820e-01, -6.5355e-02,
    # ====       1.8371e-01, -1.0080e-01,  7.8455e-02, -2.0172e-02,  1.3804e-01,
    # ====       6.6452e-02,  1.5630e-01, -4.0152e-02, -2.7048e-01, -2.2910e-02,
    # ====      -2.8097e-02, -5.1309e-02,  9.2576e-02, -3.5763e-02,  1.0911e-01,
    # ====      -8.5753e-02, -3.6841e-02,  2.1108e-02,  6.4374e-03, -3.9886e-02,
    # ====       2.8286e-01, -1.1581e-01,  8.8731e-02,  1.3184e-01, -1.0517e-01,
    # ====       1.7431e-02, -8.0597e-02, -5.8970e-02, -2.1860e-02, -2.6187e-02,
    # ====       1.5130e-01, -4.5952e-03,  6.9265e-03,  2.0330e-01, -2.1218e-01,
    # ====      -1.5337e-01,  1.6562e-01,  1.9303e-02,  1.1466e-01, -1.2989e-01,
    # ====      -1.6727e-01, -8.8449e-02, -1.1809e-01, -7.0278e-03, -9.7973e-02,
    # ====       7.6351e-02, -1.4625e-01, -9.2699e-02,  9.2519e-02, -7.2732e-02,
    # ====      -4.5247e-02, -1.0494e-01, -9.1241e-02,  2.8685e-02,  1.9055e-01,
    # ====      -1.4714e-01,  5.8144e-02,  1.2549e-02,  1.0670e-01,  7.4713e-02,
    # ====      -2.6009e-02,  3.0248e-02, -5.6656e-02, -5.2743e-02, -1.6104e-01,
    # ====      -5.0685e-02, -1.6635e-02,  6.3405e-02,  1.1485e-02, -2.2254e-02,
    # ====      -6.1207e-02, -8.3962e-02, -2.4775e-02,  9.2587e-02,  1.9358e-01,
    # ====       1.6983e-02,  1.4604e-01,  5.2819e-03,  7.7856e-02,  1.4085e-02,
    # ====       1.9709e-01, -8.1646e-02,  1.0534e-01,  2.2378e-01,  1.0342e-01,
    # ====       2.2376e-01,  3.0158e-02,  8.5083e-02,  1.5312e-01, -2.0837e-02,
    # ====       1.9073e-01,  9.1285e-02,  1.8251e-01,  6.7799e-02, -1.1309e-01,
    # ====      -1.1123e-01, -6.1238e-02, -8.1508e-02, -1.0369e-01, -7.4879e-02,
    # ====       5.9149e-02, -5.7186e-02, -8.6395e-02, -1.0828e-02,  5.5153e-02,
    # ====       3.7575e-02,  1.1399e-01, -5.3092e-02, -1.0721e-02,  1.1311e-01,
    # ====      -2.5588e-02, -1.3640e-03,  1.3855e-01,  2.4626e-01, -1.6606e-01,
    # ====       3.2187e-02,  8.5846e-02, -2.3274e-02,  4.8646e-02,  7.5765e-02,
    # ====       1.2570e-01, -8.3909e-02, -9.6536e-02, -1.1373e-01,  5.3732e-03,
    # ====       3.7410e-02,  1.6434e-01, -7.3603e-02,  1.7557e-01,  1.6144e-02,
    # ====      -7.1258e-02, -1.5628e-01, -9.1940e-02, -7.9784e-02,  2.5800e-02,
    # ====      -5.2022e-02,  5.6198e-03, -6.0133e-02,  9.6658e-02, -2.4534e-02,
    # ====       1.7714e-01,  1.1732e-01,  2.4342e-02,  6.4155e-02, -1.0774e-01,
    # ====       1.5409e-02,  1.2822e-02,  7.8627e-02, -2.5923e-01, -1.0239e-02,
    # ====      -8.6221e-03, -1.4151e-01, -1.2256e-01,  6.9481e-02,  8.2255e-02,
    # ====       1.0553e-01,  1.1416e-01,  1.3865e-02,  1.5159e-01,  1.3694e-01,
    # ====      -3.8094e-02,  2.8688e-02,  1.1659e-01,  4.8474e-03, -1.0382e-01,
    # ====      -8.1296e-02, -1.5935e-01,  1.3306e-01,  1.3737e-01,  1.6954e-01,
    # ====      -6.2028e-02,  2.2850e-02, -2.1771e-01,  7.8410e-02, -1.1788e-01,
    # ====      -2.7502e-02,  1.1240e-01, -3.4050e-02,  7.9124e-02, -9.0664e-04,
    # ====      -3.6404e-02, -1.4725e-01, -6.4665e-02,  1.9836e-02, -1.4176e-01,
    # ====      -3.6194e-02,  8.0941e-02,  3.1156e-03, -1.0034e-01, -2.9625e-03,
    # ====       6.8614e-02,  1.9551e-01, -2.0060e-02,  1.9218e-02, -1.4526e-01,
    # ====       6.1712e-02, -7.6964e-02,  1.5801e-01, -4.5995e-03,  1.6324e-01,
    # ====      -8.5096e-02, -6.4856e-02, -8.7744e-02, -4.2445e-02,  1.0386e-01,
    # ====       3.5110e-02, -1.6977e-01,  1.6774e-01, -2.0564e-02,  1.4318e-01,
    # ====       1.7830e-01, -1.8200e-01,  5.6839e-02, -9.1662e-02,  8.1009e-02,
    # ====       7.3567e-02, -3.9488e-02, -1.3566e-03,  1.2073e-01, -7.7891e-02,
    # ====      -5.8577e-02, -2.9609e-02, -2.3159e-02, -9.9118e-02, -9.1168e-02,
    # ====       5.1151e-02,  6.8630e-02,  1.4616e-01, -1.0639e-01, -4.3123e-02,
    # ====      -1.0374e-01, -4.3573e-02,  4.1231e-02]]
    DebuggingHelper.write_line_to_system_console_out(
        f'outputs[1][0]={outputs[1][0]}')
    DebuggingHelper.write_line_to_system_console_out(
        f'outputs={outputs}')
    # ------------------------------------------------------------------------
    output_raw_cls_embedding = outputs[0][:, 0, :]
    # ---- NOTE ---- for single utterance, the above is the same as: output_raw_cls_embedding = outputs[0][0][0]
    DebuggingHelper.write_line_to_system_console_out(
        f'length(output_raw_cls_embedding)={len(output_raw_cls_embedding)}')
    DebuggingHelper.write_line_to_system_console_out(
        f'output_raw_cls_embedding={output_raw_cls_embedding}')
    # ==== output_raw_cls_embedding=tensor([[-2.6461e-01, -2.0363e-01,  7.3369e-02,  3.4607e-02, -2.7882e-02,
    # ====      -1.4411e-01,  5.7426e-02,  5.1801e-01, -3.9106e-02, -2.4470e-01,
    # ====       3.5038e-02,  7.0886e-02, -8.6316e-02,  7.9232e-02,  2.0974e-01,
    # ====      -3.6466e-01, -2.7615e-01,  3.2141e-01, -1.7689e-01,  5.2981e-01,
    # ====       2.9504e-01, -8.8848e-02, -1.3967e-01, -2.0979e-01, -9.2797e-02,
    # ====       1.1751e-02, -1.8606e-01,  3.5348e-01,  1.2636e-01, -1.2570e-01,
    # ====      -2.1419e-01,  2.9827e-01,  4.4272e-01,  1.9956e-01, -1.1006e-02,
    # ====       1.8520e-01,  4.7272e-02, -2.0622e-01, -1.1534e-02,  6.0105e-01,
    # ====      -1.8891e-01, -1.1346e-01,  1.3196e-01,  1.7466e-01, -5.5444e-01,
    # ====       3.5640e-01, -9.4742e-02,  5.1780e-01,  3.0898e-01,  7.9808e-02,
    # ====       2.0342e-01, -1.1575e-01,  1.5352e-01,  1.9301e-01,  1.5576e-02,
    # ====      -3.5441e-01,  2.3429e-01, -4.5156e-01,  1.8079e-01, -1.9095e-01,
    # ====       1.2982e-01,  1.1923e-01, -3.8647e-01,  9.8162e-02, -9.4160e-02,
    # ====      -1.0136e-01,  1.0782e-01, -1.1520e-02,  2.1980e-01,  2.5180e-01,
    # ====      -4.1664e-02, -1.6604e-02,  3.3259e-01, -3.7257e-01, -1.8578e-01,
    # ====      -2.7519e-01,  1.3871e-01,  3.3695e-02, -1.6234e-01,  7.0673e-02,
    # ====      -1.0618e-01, -1.2268e-01,  1.9852e-01,  6.5488e-02,  4.2826e-02,
    # ====      -6.5848e-02,  1.3828e-01, -4.1876e-01, -2.3477e-01,  1.1212e-01,
    # ====      -1.9239e-01, -1.7132e-01,  6.4710e-02,  2.1210e-01,  4.3832e-01,
    # ====       1.3649e-01, -2.4878e-03,  4.4615e-02,  1.4066e-01, -1.6090e-01,
    # ====       7.9226e-02,  2.2863e-01,  9.0759e-02,  2.0595e-01,  2.1348e-01,
    # ====       2.0633e-01, -2.7556e-02,  7.6414e-01,  3.2611e-01, -1.5955e-01,
    # ====      -1.4049e-01,  1.7655e-01, -3.9086e-01,  2.4254e-01,  2.9406e-02,
    # ====       9.0184e-02, -2.8814e-01,  8.2897e-02, -3.1311e-01,  1.0830e-01,
    # ====      -2.3850e-01,  7.8164e-02,  3.1546e-01,  1.4278e-01,  1.2098e-01,
    # ====      -1.6499e-01, -7.2838e-02, -2.1829e-01,  3.4395e-01, -4.1535e-02,
    # ====      -2.3849e-01, -5.4405e-02, -1.1924e-01,  1.0269e-01, -2.9531e-01,
    # ====      -2.8271e-01, -2.3441e-01,  7.2043e-02, -2.9719e-01,  9.3838e-02,
    # ====      -1.6869e-01,  2.6995e-01, -6.8553e-02,  1.0581e-01,  2.6102e-01,
    # ====      -1.9109e-03,  8.4615e-02, -1.0025e-01, -4.1328e-01, -7.7803e-02,
    # ====      -2.4363e-02,  3.7933e-01,  2.0088e-02,  2.2328e-01,  1.5602e-01,
    # ====      -1.8225e-02,  8.5486e-02, -1.9210e-01, -3.4859e-01,  2.7433e-01,
    # ====       2.4429e-02, -8.3559e-02,  3.3867e-01, -1.9321e-01,  1.3366e-01,
    # ====      -2.3019e-01, -1.3276e-01, -2.9485e-02,  1.2194e-01, -1.9845e-01,
    # ====      -2.0890e-01, -1.7311e-01,  2.5306e-01,  1.1806e-01,  3.4250e-01,
    # ====       3.6743e-02, -7.0785e-02, -3.2113e-01, -1.9615e-02,  6.7260e-02,
    # ====      -5.9412e-02,  5.0086e-02, -3.2956e-01,  1.1696e-01, -4.2284e-02,
    # ====      -2.6723e-01, -2.0160e-01,  6.7561e-02, -2.2682e-01,  1.2040e-01,
    # ====       9.5166e-02, -1.0936e-01,  8.2189e-02,  2.7617e-01, -1.6620e-01,
    # ====       3.0141e-01, -4.3507e-01,  3.1462e-01, -1.5569e-01, -2.3216e-01,
    # ====      -3.8310e-01, -8.9517e-02,  3.0210e-02,  1.2433e-01,  1.3709e-01,
    # ====       3.6516e-03,  3.3112e-01,  2.4504e-01, -2.7678e-01, -2.0005e-01,
    # ====       9.1313e-02,  1.1598e-01, -9.4609e-02,  2.3024e-01,  2.5520e-01,
    # ====      -1.9752e-02,  8.5216e-02,  1.1928e-01, -1.3479e-02, -1.7245e-01,
    # ====      -3.2884e-01,  1.3540e-01, -2.4940e-01, -1.5850e-01,  1.1291e-01,
    # ====      -1.7870e-01,  4.8411e-01,  1.3323e-02, -1.1678e-01,  2.0504e-01,
    # ====      -4.1399e-01, -5.7793e-02,  3.3229e-01, -4.4903e-02,  1.3638e-01,
    # ====       1.1932e-01, -3.7529e-01, -6.7986e-02, -6.3099e-02,  6.6642e-02,
    # ====      -3.4121e-01,  7.3310e-03, -1.4914e-01,  4.7354e-03, -2.4698e-02,
    # ====      -8.3875e-02, -1.1780e-01,  6.4853e-02, -5.6332e-02, -5.6887e-02,
    # ====       3.3460e-01,  1.3221e-02, -8.8456e-02, -4.2105e-01,  9.4347e-02,
    # ====      -1.8704e-01, -3.2981e-02, -4.8517e-01,  1.6929e-01,  1.3850e-03,
    # ====       1.8559e-01, -2.6754e-01, -7.9623e-02, -3.2562e-01,  1.3168e-02,
    # ====       2.4752e-01,  2.5289e-01, -3.9224e-01, -1.1533e-01, -2.3031e-01,
    # ====       1.8685e-01,  4.2684e-02, -1.8489e-01, -1.3104e-01, -9.9493e-02,
    # ====       1.8844e-01,  6.3197e-02, -4.3004e-01, -9.3259e-04,  3.9579e-01,
    # ====      -8.6845e-02,  6.6229e-02, -2.2420e-04, -1.7010e-01, -4.7064e-03,
    # ====      -2.2135e-01,  3.3608e-01,  7.6052e-03,  1.2614e-02, -1.2141e-01,
    # ====      -1.6066e-01, -4.9116e-01,  5.8025e-02,  1.3658e-01, -3.8006e-01,
    # ====       7.3409e-02,  1.9622e-01, -3.4345e-02, -2.0450e-01,  1.8275e-01,
    # ====       1.1343e-01,  8.1315e-02,  4.0910e-01,  1.4530e-01, -3.9337e-03,
    # ====       3.1517e-02,  1.2860e-02, -2.4900e-01, -3.5774e-01, -1.0194e-01,
    # ====       3.0378e-01, -2.5668e-02, -1.4570e-01,  2.6095e-01, -2.4262e-01,
    # ====      -2.1234e-01,  1.0087e-03,  4.2318e-01,  7.0718e-02,  6.3755e-03,
    # ====      -1.8329e-01, -6.0549e-02, -4.1509e-02,  3.2484e-02,  1.0162e-01,
    # ====      -8.9029e-02,  5.9278e-02, -5.6250e-02,  8.5666e-02,  4.5277e-02,
    # ====       4.1576e-03, -2.7324e-02, -2.6321e-03, -1.5469e-01, -1.8206e-01,
    # ====       1.9268e-01,  4.1067e-01,  1.9187e-01,  1.7414e-01,  1.2333e-01,
    # ====      -2.2608e-01,  7.9653e-02,  1.8854e-01, -2.6272e-01, -2.2209e-01,
    # ====       6.9383e-02, -9.0200e-02, -3.1877e-02, -1.3218e-03, -1.2837e-01,
    # ====       1.3873e-01, -3.4617e-01,  7.5677e-03,  2.2559e-01, -2.2137e-02,
    # ====       2.7324e-01,  2.3485e-01,  9.2602e-03, -3.8325e-01, -1.7957e-01,
    # ====      -1.3419e-01, -1.1085e-01, -4.2435e-02, -2.5487e-01,  2.3207e-01,
    # ====      -4.1326e-02, -2.4448e-01, -2.7540e-01, -3.0751e-01, -3.0569e-01,
    # ====       4.9559e-01, -1.4950e-01,  2.5952e-02, -1.9594e-01, -1.4275e-01,
    # ====      -7.6059e-02, -2.3345e-01,  1.5803e-01, -7.7514e-03,  3.1740e-01,
    # ====       1.3470e-02,  5.8616e-01, -1.8633e-01, -2.7264e-02, -1.4352e-02,
    # ====      -6.3823e-02, -2.5751e-01,  2.2132e-01, -3.3530e-01, -1.7322e-03,
    # ====       1.1851e-01,  7.1747e-02, -6.1663e-02, -7.9370e-02,  1.2178e-01,
    # ====       1.4113e-01,  7.7433e-03,  9.4362e-02,  2.1413e-01, -3.9224e-01,
    # ====       8.9567e-02,  2.5710e-01,  7.5085e-02,  1.5162e-01,  3.4409e-01,
    # ====       3.7166e-02, -3.9380e-01,  5.8182e-02,  2.3750e-01, -1.1639e-01,
    # ====       4.8749e-01, -1.8404e-01, -6.2756e-02, -3.0816e-01, -1.3697e-01,
    # ====       4.8233e-03,  1.8531e-01,  2.0652e-01, -2.6847e-01,  1.4561e-01,
    # ====       8.0255e-02, -1.1806e-01, -6.7657e-02,  2.9665e-02, -1.7337e-01,
    # ====       1.3862e-01,  1.6562e-01, -1.9410e-01,  5.7390e-03,  1.4806e-01,
    # ====       9.2706e-02, -6.7380e-02, -1.0735e-01,  4.0564e-02, -1.6982e-01,
    # ====       3.5526e-02,  2.2909e-02, -3.0660e-02,  1.0405e-01, -1.2595e-01,
    # ====      -1.7543e-01, -1.5751e-04, -4.4486e-01,  5.9281e-02, -2.5956e-02,
    # ====       1.9144e-01, -6.2531e-02,  1.7385e-01, -9.7092e-02,  3.2472e-01,
    # ====      -8.1229e-02,  9.4178e-02, -3.4538e-01,  2.2226e-01, -2.7687e-01,
    # ====      -2.1146e-01,  8.7170e-02, -1.6716e-01, -3.6091e-02,  2.0910e-01,
    # ====       1.3820e-01, -2.8684e-01, -4.4415e-01,  4.4376e-01,  3.0499e-02,
    # ====      -1.2671e-01,  2.2865e-01, -2.6495e-01, -2.5278e-01,  1.8414e-01,
    # ====      -3.0034e-01,  4.2478e-02, -2.6046e-01, -6.6390e-02, -3.7592e-01,
    # ====      -1.7422e-01,  1.1707e-01,  8.2765e-02, -2.0940e-01,  7.2047e-02,
    # ====       5.6236e-02, -3.0797e-01, -8.9740e-02, -5.7259e-02, -1.2272e-01,
    # ====      -5.3406e-02,  1.3450e-01,  6.2486e-02,  3.2015e-01,  1.0430e-01,
    # ====      -6.0523e-02,  2.3147e-01,  1.2914e-01,  1.9695e-01,  2.3997e-01,
    # ====       1.6453e-02,  1.4818e-01, -2.9654e-02,  1.4968e-02, -2.6127e-01,
    # ====       2.2374e-01,  7.9669e-02, -2.6503e-01,  4.8627e-02,  1.7440e-01,
    # ====       1.6380e-01,  7.8886e-02,  8.2267e-02,  1.9266e-01, -3.4614e-01,
    # ====       1.5399e-02,  2.4605e-01,  7.5943e-02,  5.0941e-02,  2.7304e-01,
    # ====      -3.1354e-01,  1.1718e-02, -2.1115e-01,  8.3167e-02,  1.8771e-01,
    # ====      -3.2826e-01, -2.3783e-01,  1.2360e-01, -5.1753e-03,  9.5240e-02,
    # ====       1.7413e-01,  2.5630e-02, -1.0534e-01, -4.7585e-01,  5.1275e-01,
    # ====      -8.5402e-03,  5.4304e-02,  1.2032e-01,  1.0307e-01, -3.8560e-03,
    # ====      -1.7317e-01,  9.9030e-02, -2.0757e-01,  1.2616e-01, -2.3296e-01,
    # ====      -1.8715e-01,  1.2719e-02, -3.7845e-02,  2.2577e-01,  6.4888e-02,
    # ====       2.2906e-01, -1.7816e-01,  1.6627e-02, -6.9271e-02,  3.9562e-01,
    # ====       3.2091e-02, -1.6152e-01, -1.7693e-01, -7.4656e-03, -3.0512e-01,
    # ====       6.4508e-02, -9.6164e-02, -6.5856e-02,  1.4518e-01,  2.0208e-01,
    # ====       3.8740e-01, -4.4397e-01, -1.0347e-01, -2.5444e-02,  2.5264e-02,
    # ====       7.0283e-03,  2.8192e-01, -1.5903e-01, -1.5281e-01,  2.0362e-01,
    # ====       1.2531e-01,  1.5130e-01,  1.6698e-02,  1.2601e-01,  1.5973e-01,
    # ====      -3.2584e-01,  5.5003e-03, -2.8866e-01, -2.3350e-01, -2.0952e-01,
    # ====      -2.0578e-01,  3.5571e-01,  8.5300e-02, -1.4967e-01,  3.7727e-01,
    # ====      -4.1135e-03, -5.2260e-02,  3.4076e-01, -4.1757e-02,  4.5901e-02,
    # ====      -8.7859e-02, -1.5219e-01,  2.8436e-01,  9.1669e-02, -5.9628e-02,
    # ====      -1.4770e-01,  1.3001e-01,  1.7528e-01, -3.2356e-01, -1.4918e-01,
    # ====      -1.0023e-01,  6.5433e-02, -1.0431e-01, -4.1751e-02,  1.3214e-01,
    # ====       2.1592e-02,  7.9181e-02,  8.8145e-02,  1.2713e-01,  3.7962e-02,
    # ====       3.3972e-01,  3.0160e-01,  3.0284e-02, -4.7681e-01,  5.6716e-02,
    # ====       1.0526e-01,  2.7174e-01,  3.1571e-01, -2.2448e-01, -1.8434e-01,
    # ====      -3.0558e-01,  2.5842e-02,  9.2633e-02, -1.2072e-01,  9.2990e-02,
    # ====       1.6772e-01, -6.5534e-02,  2.8821e-01, -1.8787e-01, -1.0918e-01,
    # ====      -2.5714e-02,  6.7075e-03, -1.3934e-01, -2.3482e-02, -2.2967e-01,
    # ====      -3.7293e-01,  2.8298e-01,  2.2518e-02,  3.0597e-01, -5.1878e-02,
    # ====      -1.8124e-01, -1.2897e-01,  1.5932e-01, -1.4275e-01,  1.2437e-01,
    # ====      -1.8036e-01,  1.0918e-01, -1.6441e-01,  1.8474e-03,  6.3394e-02,
    # ====      -8.4879e-02,  2.6818e-01, -1.7149e-01, -1.9312e-01,  7.4047e-02,
    # ====       1.1044e-01, -9.1325e-02,  2.7694e-01, -2.7626e-01,  1.0049e-01,
    # ====       1.2901e-01, -3.4428e-01, -8.0094e-03,  3.5294e-01,  4.5280e-01,
    # ====       2.3015e-01, -1.8469e-01, -2.1385e-01, -5.5550e-02, -1.5806e-01,
    # ====       1.5012e-01,  2.7594e-01,  1.0215e-01,  2.5086e-01,  5.9249e-02,
    # ====       8.6016e-02, -1.5014e-01,  2.5307e-02, -3.3688e-01,  2.7600e-01,
    # ====      -2.1504e-01,  1.2175e-01, -1.5710e-01, -1.1721e-01, -5.4469e-02,
    # ====      -5.4271e-02,  1.4616e-01,  2.7228e-01,  3.8158e-01,  2.0989e-01,
    # ====      -8.3144e-02, -1.4776e-01, -1.1991e-01, -7.6203e-03, -2.7356e-01,
    # ====      -1.8168e-01, -2.8396e-03, -1.9044e-01,  1.0887e-01, -1.3345e-01,
    # ====      -1.7414e-01, -8.7644e-02, -1.2988e-02,  1.0888e-01,  5.7863e-02,
    # ====      -3.5638e-01,  2.2247e-01,  2.3954e-02,  1.2922e-01,  1.7861e-02,
    # ====       4.3485e-02,  1.1234e-01, -1.3109e-02, -1.2601e-01,  2.8826e-01,
    # ====       2.5117e-01,  2.4645e-01, -2.5668e-01,  2.6439e-02, -3.8188e-01,
    # ====       1.1469e-01, -1.1800e-01,  1.5343e-01, -3.9145e-01,  8.2521e-02,
    # ====      -2.7792e-01, -4.5229e-02,  1.9573e-01, -1.3514e-01, -2.4898e-02,
    # ====      -7.6172e-03, -2.6266e-01,  5.5089e-02, -2.0771e-01,  1.9703e-03,
    # ====       5.0695e-02, -2.7184e-01,  2.8412e-01, -4.5590e-02, -1.8116e-01,
    # ====      -1.8842e-01,  8.5472e-03,  9.9711e-03, -2.8700e-01, -5.1674e-01,
    # ====       1.4067e-02,  1.1652e-01,  2.9589e-01,  8.6066e-02, -4.9153e-02,
    # ====      -3.0358e-01, -1.8295e-02, -7.6782e-02,  7.9164e-02, -3.0543e-01,
    # ====      -9.9874e-02, -4.4190e-02,  2.3373e-02, -2.3091e-02, -4.1636e-02,
    # ====       8.5354e-02,  1.6516e-02,  2.3044e-01, -4.9916e-02,  1.8241e-01,
    # ====      -1.0850e-02,  3.5555e-03,  4.2739e-02]]
    output_raw_cls_embedding_divided_by_norm = divided_by_norm(output_raw_cls_embedding)
    DebuggingHelper.write_line_to_system_console_out(
        f'length(output_raw_cls_embedding_divided_by_norm)={len(output_raw_cls_embedding_divided_by_norm)}')
    DebuggingHelper.write_line_to_system_console_out(
        f'output_raw_cls_embedding_divided_by_norm={output_raw_cls_embedding_divided_by_norm}')
    # ==== output_raw_cls_embedding_divided_by_norm=tensor([[-4.7202e-02, -3.6325e-02,  1.3088e-02,  6.1734e-03, -4.9737e-03,
    # ====      -2.5707e-02,  1.0244e-02,  9.2405e-02, -6.9759e-03, -4.3650e-02,
    # ====       6.2503e-03,  1.2645e-02, -1.5397e-02,  1.4134e-02,  3.7415e-02,
    # ====      -6.5049e-02, -4.9260e-02,  5.7334e-02, -3.1555e-02,  9.4511e-02,
    # ====       5.2630e-02, -1.5849e-02, -2.4915e-02, -3.7424e-02, -1.6554e-02,
    # ====       2.0962e-03, -3.3189e-02,  6.3056e-02,  2.2541e-02, -2.2423e-02,
    # ====      -3.8208e-02,  5.3207e-02,  7.8975e-02,  3.5599e-02, -1.9633e-03,
    # ====       3.3038e-02,  8.4327e-03, -3.6786e-02, -2.0575e-03,  1.0722e-01,
    # ====      -3.3698e-02, -2.0239e-02,  2.3540e-02,  3.1157e-02, -9.8903e-02,
    # ====       6.3576e-02, -1.6900e-02,  9.2368e-02,  5.5116e-02,  1.4237e-02,
    # ====       3.6287e-02, -2.0648e-02,  2.7385e-02,  3.4430e-02,  2.7785e-03,
    # ====      -6.3221e-02,  4.1793e-02, -8.0551e-02,  3.2251e-02, -3.4062e-02,
    # ====       2.3157e-02,  2.1269e-02, -6.8940e-02,  1.7510e-02, -1.6797e-02,
    # ====      -1.8081e-02,  1.9233e-02, -2.0549e-03,  3.9209e-02,  4.4918e-02,
    # ====      -7.4322e-03, -2.9619e-03,  5.9329e-02, -6.6461e-02, -3.3141e-02,
    # ====      -4.9090e-02,  2.4744e-02,  6.0106e-03, -2.8959e-02,  1.2607e-02,
    # ====      -1.8942e-02, -2.1885e-02,  3.5413e-02,  1.1682e-02,  7.6394e-03,
    # ====      -1.1746e-02,  2.4668e-02, -7.4700e-02, -4.1880e-02,  2.0001e-02,
    # ====      -3.4319e-02, -3.0560e-02,  1.1543e-02,  3.7835e-02,  7.8189e-02,
    # ====       2.4347e-02, -4.4379e-04,  7.9585e-03,  2.5092e-02, -2.8702e-02,
    # ====       1.4133e-02,  4.0785e-02,  1.6190e-02,  3.6738e-02,  3.8081e-02,
    # ====       3.6806e-02, -4.9155e-03,  1.3631e-01,  5.8172e-02, -2.8461e-02,
    # ====      -2.5061e-02,  3.1494e-02, -6.9724e-02,  4.3265e-02,  5.2456e-03,
    # ====       1.6087e-02, -5.1399e-02,  1.4788e-02, -5.5855e-02,  1.9319e-02,
    # ====      -4.2545e-02,  1.3943e-02,  5.6274e-02,  2.5469e-02,  2.1580e-02,
    # ====      -2.9432e-02, -1.2993e-02, -3.8940e-02,  6.1355e-02, -7.4093e-03,
    # ====      -4.2543e-02, -9.7051e-03, -2.1271e-02,  1.8319e-02, -5.2679e-02,
    # ====      -5.0432e-02, -4.1814e-02,  1.2851e-02, -5.3013e-02,  1.6739e-02,
    # ====      -3.0091e-02,  4.8156e-02, -1.2229e-02,  1.8874e-02,  4.6562e-02,
    # ====      -3.4087e-04,  1.5094e-02, -1.7884e-02, -7.3722e-02, -1.3879e-02,
    # ====      -4.3460e-03,  6.7667e-02,  3.5833e-03,  3.9830e-02,  2.7831e-02,
    # ====      -3.2511e-03,  1.5249e-02, -3.4268e-02, -6.2183e-02,  4.8936e-02,
    # ====       4.3577e-03, -1.4906e-02,  6.0413e-02, -3.4465e-02,  2.3843e-02,
    # ====      -4.1062e-02, -2.3683e-02, -5.2597e-03,  2.1752e-02, -3.5401e-02,
    # ====      -3.7264e-02, -3.0880e-02,  4.5143e-02,  2.1059e-02,  6.1096e-02,
    # ====       6.5543e-03, -1.2627e-02, -5.7284e-02, -3.4990e-03,  1.1998e-02,
    # ====      -1.0598e-02,  8.9346e-03, -5.8788e-02,  2.0864e-02, -7.5427e-03,
    # ====      -4.7669e-02, -3.5961e-02,  1.2052e-02, -4.0461e-02,  2.1478e-02,
    # ====       1.6976e-02, -1.9508e-02,  1.4661e-02,  4.9264e-02, -2.9648e-02,
    # ====       5.3768e-02, -7.7610e-02,  5.6124e-02, -2.7772e-02, -4.1414e-02,
    # ====      -6.8338e-02, -1.5968e-02,  5.3890e-03,  2.2179e-02,  2.4455e-02,
    # ====       6.5140e-04,  5.9067e-02,  4.3711e-02, -4.9374e-02, -3.5686e-02,
    # ====       1.6289e-02,  2.0690e-02, -1.6877e-02,  4.1071e-02,  4.5524e-02,
    # ====      -3.5235e-03,  1.5201e-02,  2.1278e-02, -2.4044e-03, -3.0762e-02,
    # ====      -5.8661e-02,  2.4153e-02, -4.4489e-02, -2.8274e-02,  2.0142e-02,
    # ====      -3.1877e-02,  8.6357e-02,  2.3765e-03, -2.0831e-02,  3.6575e-02,
    # ====      -7.3849e-02, -1.0309e-02,  5.9275e-02, -8.0099e-03,  2.4328e-02,
    # ====       2.1285e-02, -6.6946e-02, -1.2128e-02, -1.1256e-02,  1.1888e-02,
    # ====      -6.0866e-02,  1.3077e-03, -2.6603e-02,  8.4473e-04, -4.4058e-03,
    # ====      -1.4962e-02, -2.1013e-02,  1.1569e-02, -1.0049e-02, -1.0148e-02,
    # ====       5.9687e-02,  2.3585e-03, -1.5779e-02, -7.5108e-02,  1.6830e-02,
    # ====      -3.3364e-02, -5.8833e-03, -8.6546e-02,  3.0199e-02,  2.4706e-04,
    # ====       3.3106e-02, -4.7726e-02, -1.4203e-02, -5.8085e-02,  2.3490e-03,
    # ====       4.4153e-02,  4.5112e-02, -6.9969e-02, -2.0573e-02, -4.1084e-02,
    # ====       3.3331e-02,  7.6142e-03, -3.2982e-02, -2.3376e-02, -1.7748e-02,
    # ====       3.3616e-02,  1.1273e-02, -7.6713e-02, -1.6636e-04,  7.0603e-02,
    # ====      -1.5492e-02,  1.1814e-02, -3.9993e-05, -3.0343e-02, -8.3955e-04,
    # ====      -3.9485e-02,  5.9952e-02,  1.3566e-03,  2.2502e-03, -2.1657e-02,
    # ====      -2.8660e-02, -8.7615e-02,  1.0351e-02,  2.4364e-02, -6.7796e-02,
    # ====       1.3095e-02,  3.5003e-02, -6.1266e-03, -3.6479e-02,  3.2599e-02,
    # ====       2.0235e-02,  1.4505e-02,  7.2977e-02,  2.5919e-02, -7.0171e-04,
    # ====       5.6222e-03,  2.2940e-03, -4.4418e-02, -6.3815e-02, -1.8184e-02,
    # ====       5.4189e-02, -4.5788e-03, -2.5991e-02,  4.6550e-02, -4.3280e-02,
    # ====      -3.7878e-02,  1.7994e-04,  7.5489e-02,  1.2615e-02,  1.1373e-03,
    # ====      -3.2696e-02, -1.0801e-02, -7.4046e-03,  5.7947e-03,  1.8127e-02,
    # ====      -1.5881e-02,  1.0574e-02, -1.0034e-02,  1.5282e-02,  8.0767e-03,
    # ====       7.4166e-04, -4.8741e-03, -4.6952e-04, -2.7594e-02, -3.2476e-02,
    # ====       3.4372e-02,  7.3257e-02,  3.4226e-02,  3.1064e-02,  2.2001e-02,
    # ====      -4.0330e-02,  1.4209e-02,  3.3633e-02, -4.6866e-02, -3.9618e-02,
    # ====       1.2377e-02, -1.6090e-02, -5.6863e-03, -2.3578e-04, -2.2898e-02,
    # ====       2.4748e-02, -6.1752e-02,  1.3500e-03,  4.0242e-02, -3.9490e-03,
    # ====       4.8742e-02,  4.1894e-02,  1.6519e-03, -6.8365e-02, -3.2033e-02,
    # ====      -2.3937e-02, -1.9775e-02, -7.5697e-03, -4.5466e-02,  4.1398e-02,
    # ====      -7.3720e-03, -4.3611e-02, -4.9126e-02, -5.4855e-02, -5.4531e-02,
    # ====       8.8406e-02, -2.6668e-02,  4.6294e-03, -3.4952e-02, -2.5464e-02,
    # ====      -1.3568e-02, -4.1644e-02,  2.8191e-02, -1.3827e-03,  5.6619e-02,
    # ====       2.4029e-03,  1.0456e-01, -3.3239e-02, -4.8635e-03, -2.5603e-03,
    # ====      -1.1385e-02, -4.5935e-02,  3.9480e-02, -5.9813e-02, -3.0899e-04,
    # ====       2.1141e-02,  1.2798e-02, -1.1000e-02, -1.4158e-02,  2.1723e-02,
    # ====       2.5175e-02,  1.3813e-03,  1.6833e-02,  3.8197e-02, -6.9970e-02,
    # ====       1.5977e-02,  4.5862e-02,  1.3394e-02,  2.7046e-02,  6.1380e-02,
    # ====       6.6299e-03, -7.0248e-02,  1.0379e-02,  4.2366e-02, -2.0762e-02,
    # ====       8.6960e-02, -3.2830e-02, -1.1195e-02, -5.4972e-02, -2.4433e-02,
    # ====       8.6040e-04,  3.3056e-02,  3.6839e-02, -4.7890e-02,  2.5974e-02,
    # ====       1.4316e-02, -2.1061e-02, -1.2069e-02,  5.2918e-03, -3.0927e-02,
    # ====       2.4728e-02,  2.9545e-02, -3.4624e-02,  1.0237e-03,  2.6412e-02,
    # ====       1.6537e-02, -1.2020e-02, -1.9150e-02,  7.2361e-03, -3.0293e-02,
    # ====       6.3373e-03,  4.0866e-03, -5.4692e-03,  1.8560e-02, -2.2467e-02,
    # ====      -3.1294e-02, -2.8098e-05, -7.9357e-02,  1.0575e-02, -4.6302e-03,
    # ====       3.4151e-02, -1.1155e-02,  3.1013e-02, -1.7320e-02,  5.7924e-02,
    # ====      -1.4490e-02,  1.6800e-02, -6.1611e-02,  3.9647e-02, -4.9390e-02,
    # ====      -3.7721e-02,  1.5550e-02, -2.9819e-02, -6.4381e-03,  3.7300e-02,
    # ====       2.4653e-02, -5.1168e-02, -7.9229e-02,  7.9159e-02,  5.4405e-03,
    # ====      -2.2603e-02,  4.0787e-02, -4.7264e-02, -4.5091e-02,  3.2849e-02,
    # ====      -5.3577e-02,  7.5774e-03, -4.6462e-02, -1.1843e-02, -6.7058e-02,
    # ====      -3.1079e-02,  2.0883e-02,  1.4764e-02, -3.7354e-02,  1.2852e-02,
    # ====       1.0032e-02, -5.4937e-02, -1.6008e-02, -1.0214e-02, -2.1891e-02,
    # ====      -9.5268e-03,  2.3994e-02,  1.1147e-02,  5.7109e-02,  1.8605e-02,
    # ====      -1.0796e-02,  4.1290e-02,  2.3037e-02,  3.5133e-02,  4.2807e-02,
    # ====       2.9350e-03,  2.6432e-02, -5.2898e-03,  2.6701e-03, -4.6607e-02,
    # ====       3.9911e-02,  1.4212e-02, -4.7278e-02,  8.6742e-03,  3.1109e-02,
    # ====       2.9220e-02,  1.4072e-02,  1.4675e-02,  3.4368e-02, -6.1746e-02,
    # ====       2.7470e-03,  4.3891e-02,  1.3547e-02,  9.0871e-03,  4.8706e-02,
    # ====      -5.5930e-02,  2.0903e-03, -3.7665e-02,  1.4836e-02,  3.3484e-02,
    # ====      -5.8557e-02, -4.2425e-02,  2.2048e-02, -9.2319e-04,  1.6989e-02,
    # ====       3.1062e-02,  4.5720e-03, -1.8791e-02, -8.4884e-02,  9.1467e-02,
    # ====      -1.5234e-03,  9.6870e-03,  2.1463e-02,  1.8387e-02, -6.8784e-04,
    # ====      -3.0891e-02,  1.7665e-02, -3.7027e-02,  2.2506e-02, -4.1557e-02,
    # ====      -3.3385e-02,  2.2689e-03, -6.7509e-03,  4.0274e-02,  1.1575e-02,
    # ====       4.0861e-02, -3.1782e-02,  2.9660e-03, -1.2357e-02,  7.0572e-02,
    # ====       5.7246e-03, -2.8812e-02, -3.1562e-02, -1.3318e-03, -5.4429e-02,
    # ====       1.1507e-02, -1.7154e-02, -1.1748e-02,  2.5898e-02,  3.6048e-02,
    # ====       6.9105e-02, -7.9197e-02, -1.8457e-02, -4.5388e-03,  4.5066e-03,
    # ====       1.2537e-03,  5.0289e-02, -2.8368e-02, -2.7259e-02,  3.6323e-02,
    # ====       2.2354e-02,  2.6989e-02,  2.9786e-03,  2.2479e-02,  2.8493e-02,
    # ====      -5.8125e-02,  9.8116e-04, -5.1493e-02, -4.1652e-02, -3.7375e-02,
    # ====      -3.6707e-02,  6.3453e-02,  1.5216e-02, -2.6699e-02,  6.7300e-02,
    # ====      -7.3378e-04, -9.3224e-03,  6.0787e-02, -7.4489e-03,  8.1880e-03,
    # ====      -1.5673e-02, -2.7148e-02,  5.0726e-02,  1.6352e-02, -1.0637e-02,
    # ====      -2.6348e-02,  2.3191e-02,  3.1267e-02, -5.7717e-02, -2.6611e-02,
    # ====      -1.7880e-02,  1.1672e-02, -1.8607e-02, -7.4477e-03,  2.3572e-02,
    # ====       3.8516e-03,  1.4125e-02,  1.5724e-02,  2.2678e-02,  6.7718e-03,
    # ====       6.0601e-02,  5.3800e-02,  5.4021e-03, -8.5055e-02,  1.0117e-02,
    # ====       1.8777e-02,  4.8474e-02,  5.6317e-02, -4.0044e-02, -3.2883e-02,
    # ====      -5.4510e-02,  4.6098e-03,  1.6524e-02, -2.1535e-02,  1.6588e-02,
    # ====       2.9919e-02, -1.1690e-02,  5.1413e-02, -3.3514e-02, -1.9477e-02,
    # ====      -4.5870e-03,  1.1965e-03, -2.4857e-02, -4.1889e-03, -4.0970e-02,
    # ====      -6.6525e-02,  5.0480e-02,  4.0169e-03,  5.4580e-02, -9.2543e-03,
    # ====      -3.2330e-02, -2.3005e-02,  2.8420e-02, -2.5464e-02,  2.2186e-02,
    # ====      -3.2174e-02,  1.9475e-02, -2.9328e-02,  3.2956e-04,  1.1309e-02,
    # ====      -1.5141e-02,  4.7839e-02, -3.0591e-02, -3.4449e-02,  1.3209e-02,
    # ====       1.9700e-02, -1.6291e-02,  4.9401e-02, -4.9281e-02,  1.7925e-02,
    # ====       2.3013e-02, -6.1414e-02, -1.4288e-03,  6.2958e-02,  8.0773e-02,
    # ====       4.1055e-02, -3.2947e-02, -3.8148e-02, -9.9092e-03, -2.8195e-02,
    # ====       2.6780e-02,  4.9223e-02,  1.8222e-02,  4.4750e-02,  1.0569e-02,
    # ====       1.5344e-02, -2.6783e-02,  4.5143e-03, -6.0095e-02,  4.9234e-02,
    # ====      -3.8360e-02,  2.1719e-02, -2.8024e-02, -2.0908e-02, -9.7165e-03,
    # ====      -9.6812e-03,  2.6073e-02,  4.8571e-02,  6.8067e-02,  3.7442e-02,
    # ====      -1.4832e-02, -2.6358e-02, -2.1390e-02, -1.3593e-03, -4.8800e-02,
    # ====      -3.2409e-02, -5.0654e-04, -3.3971e-02,  1.9420e-02, -2.3806e-02,
    # ====      -3.1064e-02, -1.5634e-02, -2.3169e-03,  1.9423e-02,  1.0322e-02,
    # ====      -6.3573e-02,  3.9685e-02,  4.2730e-03,  2.3051e-02,  3.1861e-03,
    # ====       7.7571e-03,  2.0039e-02, -2.3385e-03, -2.2478e-02,  5.1421e-02,
    # ====       4.4805e-02,  4.3962e-02, -4.5788e-02,  4.7162e-03, -6.8121e-02,
    # ====       2.0458e-02, -2.1049e-02,  2.7370e-02, -6.9829e-02,  1.4721e-02,
    # ====      -4.9577e-02, -8.0681e-03,  3.4914e-02, -2.4107e-02, -4.4414e-03,
    # ====      -1.3588e-03, -4.6854e-02,  9.8271e-03, -3.7053e-02,  3.5148e-04,
    # ====       9.0431e-03, -4.8492e-02,  5.0683e-02, -8.1326e-03, -3.2315e-02,
    # ====      -3.3611e-02,  1.5247e-03,  1.7787e-03, -5.1197e-02, -9.2179e-02,
    # ====       2.5094e-03,  2.0784e-02,  5.2782e-02,  1.5353e-02, -8.7682e-03,
    # ====      -5.4153e-02, -3.2636e-03, -1.3697e-02,  1.4122e-02, -5.4484e-02,
    # ====      -1.7816e-02, -7.8828e-03,  4.1695e-03, -4.1191e-03, -7.4271e-03,
    # ====       1.5226e-02,  2.9461e-03,  4.1107e-02, -8.9042e-03,  3.2539e-02,
    # ====      -1.9355e-03,  6.3425e-04,  7.6239e-03]]
    DebuggingHelper.write_line_to_system_console_out(
        f'outputs[0][0][1]={outputs[0][0][1]}')
    # ==== outputs[0][0][1]=tensor([-3.7193e-01, -9.3185e-02,  4.2446e-03,  1.0013e-01,  2.2831e-01,
    # ====     -3.0671e-01,  3.1033e-01,  4.3848e-01, -2.8672e-01, -3.9537e-01,
    # ====      4.4487e-01,  1.6668e-01, -2.0127e-01,  3.0817e-01,  1.7328e-01,
    # ====     -2.5477e-01, -4.1770e-01,  4.5319e-01, -1.6300e-01,  6.2588e-01,
    # ====      3.2081e-01, -4.6131e-02, -2.7315e-01, -5.4721e-01, -1.6731e-01,
    # ====      6.2229e-02, -2.4251e-01,  4.4089e-02,  1.3477e-01, -2.5990e-01,
    # ====     -4.5545e-01,  2.8064e-01,  4.2732e-01, -4.2436e-02, -1.2207e-02,
    # ====      3.9085e-01, -1.7344e-01, -2.3699e-01,  1.8786e-01,  8.1706e-01,
    # ====     -1.6654e-01, -2.5980e-01,  9.3432e-02,  2.6376e-01, -6.4960e-01,
    # ====      2.4588e-01, -1.6259e-01,  4.2136e-01,  2.4296e-01,  6.4529e-02,
    # ====      3.5046e-01, -2.1781e-01,  9.0715e-02, -7.0258e-02,  6.7814e-02,
    # ====     -5.8287e-01,  1.2554e-01, -4.1796e-01,  2.0511e-01,  2.3825e-01,
    # ====      1.0025e-01,  4.5073e-02, -5.0667e-01,  1.8078e-01, -9.6354e-02,
    # ====     -2.7322e-01, -4.0982e-02,  9.4870e-02,  3.5468e-02, -5.5626e-02,
    # ====     -1.0973e-01,  3.9187e-02,  4.7552e-01, -4.5856e-01, -3.1217e-01,
    # ====     -3.0159e-01,  3.2826e-01,  4.1734e-02,  7.9242e-02,  1.4378e-01,
    # ====     -3.0816e-01, -2.3101e-01,  1.7541e-01, -1.0961e-02, -2.3588e-01,
    # ====      2.5766e-01, -1.2914e-01, -3.2565e-01, -2.8686e-01,  2.7125e-01,
    # ====      5.0228e-02, -1.8477e-01,  4.0038e-01,  1.9955e-01,  5.2199e-01,
    # ====      2.1103e-01,  1.7987e-01, -2.1143e-02,  3.8757e-01, -2.9331e-01,
    # ====      9.6097e-02,  2.5667e-01,  2.2917e-01,  1.5601e-01,  6.2089e-02,
    # ====      6.7333e-02, -2.4910e-02,  7.7937e-01,  5.8184e-01, -2.5095e-01,
    # ====     -2.2019e-01,  4.4053e-01, -4.3329e-01,  3.5305e-02, -1.8095e-01,
    # ====      1.7823e-02, -7.4056e-01,  1.0348e-01, -7.6629e-01,  2.1541e-01,
    # ====     -3.8783e-01, -1.1478e-01,  2.7876e-01,  1.7305e-02,  4.6600e-02,
    # ====     -2.4213e-01, -2.6302e-02,  1.2984e-01,  5.1427e-01, -6.2814e-02,
    # ====     -5.6764e-01, -3.9911e-02, -2.3173e-01,  1.2630e-01, -3.7725e-01,
    # ====     -1.1291e-01, -2.9966e-01, -8.9026e-03, -5.0515e-01,  2.1357e-01,
    # ====     -4.6931e-01,  2.3719e-01,  7.2509e-02,  1.0847e-01,  3.8065e-01,
    # ====      4.6781e-03,  2.0641e-01, -1.9965e-01, -4.2971e-01,  4.8157e-03,
    # ====      8.3796e-02,  4.5338e-01,  9.9944e-02,  1.5355e-02,  3.2563e-01,
    # ====      1.9966e-01, -5.6466e-02, -1.6122e-01, -2.9605e-01,  2.0542e-01,
    # ====      6.9116e-02, -1.7731e-01,  3.8417e-01, -2.6622e-01, -1.3434e-02,
    # ====     -1.3969e-01, -2.6257e-01,  6.4748e-02, -1.6461e-01, -4.9966e-02,
    # ====     -1.6925e-01, -1.3419e-01, -8.7552e-03,  9.1094e-02,  3.7073e-01,
    # ====      2.1115e-01, -4.3014e-02, -7.2954e-01,  9.5019e-02, -1.7692e-02,
    # ====      5.0636e-03, -3.0789e-02, -7.8143e-01, -6.4530e-02, -3.4279e-02,
    # ====     -1.3646e-01, -1.9838e-01, -4.8561e-02, -1.7372e-01,  6.9377e-02,
    # ====      5.6931e-03, -4.6932e-02,  1.1637e-01,  3.7757e-01, -2.1050e-01,
    # ====      2.2315e-01, -3.1794e-01,  2.9838e-01,  2.6557e-01, -2.1257e-01,
    # ====     -3.8866e-01, -1.1486e-01, -1.5796e-01, -6.7807e-03,  3.4343e-03,
    # ====      5.8303e-02,  2.3927e-01,  2.8008e-01, -1.8093e-01, -2.3306e-01,
    # ====      2.0015e-01,  5.4453e-01, -1.1778e-01,  1.2218e-01,  1.4372e-01,
    # ====      3.0300e-02, -2.5951e-02, -8.1008e-02, -1.3187e-01, -7.0558e-02,
    # ====     -6.9282e-03,  1.3535e-01, -6.9830e-02, -9.0526e-02,  1.5723e-01,
    # ====      5.6326e-03,  2.8221e-01, -7.2663e-02,  7.0756e-02,  1.3230e-01,
    # ====     -7.2743e-01, -5.8354e-03,  3.1653e-01, -6.1914e-02, -2.5677e-02,
    # ====     -6.1683e-03, -4.3176e-01, -5.5030e-01,  7.7653e-02, -2.4057e-01,
    # ====     -2.7197e-01,  5.6018e-02, -8.2218e-02,  1.5891e-03, -9.9888e-02,
    # ====     -1.9101e-01, -5.1813e-02,  1.5051e-01, -1.1422e-01, -1.5719e-01,
    # ====      9.3135e-03,  5.0700e-02,  1.3991e-01, -5.8868e-01,  1.1061e-01,
    # ====     -2.9807e-01, -3.1023e-02, -3.8272e-01,  3.2395e-01, -6.2881e-04,
    # ====     -5.8166e-02, -6.0430e-01, -1.8277e-01, -1.9543e-01,  8.9976e-02,
    # ====      7.8206e-02,  3.9621e-01, -5.2352e-01, -9.8379e-03, -1.7513e-01,
    # ====      2.5713e-01, -1.4464e-01, -1.1562e-02,  1.3245e-01, -4.0498e-01,
    # ====      2.6583e-01, -1.5218e-01, -4.5944e-01,  8.1204e-02,  3.2104e-01,
    # ====     -4.5239e-01,  2.9781e-01,  7.8140e-02, -2.4175e-01, -4.6413e-01,
    # ====     -1.6924e-01,  3.4856e-01, -7.4226e-02, -9.9897e-02, -4.0929e-02,
    # ====     -2.2496e-01, -2.1553e-01,  1.3783e-01,  2.8473e-01, -7.2850e-01,
    # ====     -6.7241e-03,  6.8873e-02,  2.0177e-01, -2.9954e-02,  3.7986e-01,
    # ====      1.9299e-01,  1.6530e-02,  5.8428e-01,  2.1947e-01,  4.6380e-02,
    # ====      3.3188e-01,  9.3636e-03,  1.8417e-01, -4.2937e-01, -1.1164e-01,
    # ====      2.9045e-01, -2.9617e-02, -1.4962e-02,  3.9125e-01, -7.3990e-01,
    # ====     -2.2598e-01,  1.7715e-01,  5.2284e-01,  2.3677e-01,  6.4561e-02,
    # ====     -1.2966e-01, -7.6014e-02, -6.7677e-02, -5.2821e-02,  2.6651e-01,
    # ====     -6.0342e-02,  1.4356e-01,  8.8239e-03, -8.4615e-02, -6.0833e-02,
    # ====      9.5744e-02, -6.2390e-02, -1.7538e-01, -5.5634e-03, -3.0507e-02,
    # ====      2.7097e-01,  3.1375e-01,  2.8142e-01,  2.5929e-01,  2.0311e-01,
    # ====     -1.8536e-01,  2.8165e-01, -1.9339e-02, -3.0562e-01, -8.4753e-02,
    # ====      3.5339e-01, -4.5541e-02, -6.5845e-02, -9.0232e-02, -1.6677e-01,
    # ====      1.1404e-01, -2.7625e-01, -1.2326e-01,  2.1320e-01, -1.5472e-02,
    # ====      5.5287e-02,  3.8459e-01, -1.7429e-01, -4.3945e-01, -2.6413e-01,
    # ====     -1.5033e-01, -8.3650e-02, -8.4676e-02, -2.5015e-01,  2.9152e-02,
    # ====     -3.5068e-01, -2.0295e-01, -5.6240e-01, -4.4399e-01, -4.6991e-01,
    # ====      8.3430e-01, -2.0666e-01,  1.8637e-01, -4.5667e-02, -1.5797e-01,
    # ====     -2.2145e-02, -3.4771e-01,  4.8943e-02, -1.7327e-01,  1.9950e-02,
    # ====     -5.6119e-02,  5.8606e-01, -5.9160e-02,  3.0153e-02,  1.3945e-01,
    # ====     -1.9444e-01, -3.0026e-01,  2.4640e-01, -2.6306e-01, -8.6165e-02,
    # ====      2.2922e-01,  1.1493e-02, -1.6362e-01, -7.4753e-01,  2.1304e-01,
    # ====     -5.3545e-04,  7.6038e-02,  1.7636e-01,  2.0954e-01, -4.6420e-01,
    # ====      1.6297e-01,  2.8747e-01,  2.7495e-02,  3.8991e-01,  5.3346e-01,
    # ====      2.3612e-01, -3.4876e-01, -4.8435e-02,  3.2947e-01, -1.1006e-01,
    # ====      4.0464e-01, -3.0571e-01,  4.2979e-01, -2.7916e-01, -1.9326e-01,
    # ====     -3.3875e-01,  5.1136e-02,  1.5982e-01, -4.5734e-01,  5.9473e-02,
    # ====      1.1721e-01,  1.5398e-01, -1.1869e-01,  1.5412e-02, -5.7006e-02,
    # ====      2.3513e-01,  7.0681e-01, -3.1010e-01, -3.1715e-01,  3.1076e-02,
    # ====      1.9277e-01,  3.2957e-02,  2.1582e-01,  4.5637e-02, -4.0845e-01,
    # ====      6.3320e-02,  5.1357e-04,  1.5465e-01,  5.9844e-03,  1.0675e-01,
    # ====      7.0807e-02, -1.1730e-01, -4.6472e-01, -2.3530e-01,  1.1472e-02,
    # ====      1.2768e-01, -2.7757e-01,  2.8712e-01, -4.2123e-01,  2.1554e-01,
    # ====      9.9253e-02,  3.5996e-01, -4.3245e-01,  3.7534e-01, -2.1682e-01,
    # ====     -4.9836e-02,  1.0105e-02, -5.9588e-02,  8.8983e-02,  5.5000e-02,
    # ====      8.4251e-02, -1.8962e-01, -7.5124e-01,  5.6188e-01, -5.0664e-02,
    # ====      2.4436e-02,  3.1284e-01, -1.9425e-01, -1.7404e-02,  2.6150e-01,
    # ====     -3.8295e-01, -9.5369e-02, -3.0530e-01,  9.9583e-02, -2.0554e-01,
    # ====     -2.7479e-01,  2.9742e-01,  1.2818e-01, -2.2136e-01, -5.7288e-02,
    # ====     -9.7707e-02, -2.9640e-01,  7.8893e-03,  1.8832e-01, -8.8958e-02,
    # ====      9.6299e-02,  3.2694e-01,  1.6378e-01,  2.2816e-01, -7.7776e-02,
    # ====     -1.7179e-01,  4.7089e-01,  2.3317e-01,  4.3603e-01,  1.6476e-01,
    # ====      2.4345e-02,  1.3546e-01, -4.5339e-01,  8.9334e-03, -1.5340e-01,
    # ====      2.4016e-01, -5.3145e-02, -7.4203e-03, -9.4932e-02,  2.1200e-01,
    # ====      2.4201e-01,  2.8409e-01, -3.0382e-02,  3.9753e-01, -1.1098e-01,
    # ====      2.3847e-01,  3.5173e-01,  1.5080e-01,  1.3374e-01,  1.0480e-01,
    # ====      9.9530e-02,  2.8454e-01, -2.1807e-01,  3.1295e-01,  7.1731e-02,
    # ====     -1.1971e-01, -8.1111e-02,  1.3489e-01, -1.4599e-01,  1.6596e-01,
    # ====      1.1426e-01,  3.1391e-01, -2.2063e-01, -5.6768e-01,  4.5967e-01,
    # ====      1.3896e-01,  5.3762e-01,  1.3161e-01,  8.9416e-02, -8.2074e-02,
    # ====     -2.1772e-02,  3.2070e-02, -1.7423e-01,  2.7371e-01, -3.8199e-01,
    # ====     -2.0102e-01,  2.7414e-01,  8.4888e-02,  2.6028e-01,  1.4912e-01,
    # ====      3.9036e-01, -1.2046e-01, -9.7922e-02, -3.7429e-02,  4.5031e-01,
    # ====      1.4046e-01, -3.2315e-01,  3.8542e-01, -3.0622e-03, -4.4881e-01,
    # ====     -4.1761e-02, -9.0124e-02, -7.4736e-02,  3.7526e-01,  9.6530e-02,
    # ====      3.3015e-01, -4.1980e-01, -2.8413e-01, -1.1973e-01, -1.5882e-01,
    # ====     -6.8554e-01,  3.0959e-01, -1.1563e-01, -2.7627e-03,  2.3979e-01,
    # ====      1.1980e-01,  1.3169e-01, -1.2661e-01,  1.2936e-01,  4.3455e-03,
    # ====     -2.7929e-01,  2.3124e-01, -2.3685e-01, -5.5398e-03, -4.7104e-02,
    # ====     -4.9808e-03,  3.7350e-01, -3.2791e-02, -6.6117e-01,  3.7698e-01,
    # ====     -2.3104e-01,  3.0632e-01,  4.3048e-01, -1.2847e-01, -4.9041e-02,
    # ====      1.0978e-01, -1.7861e-01,  2.9125e-01,  1.9754e-01,  7.0468e-02,
    # ====     -4.0631e-02,  1.1671e-01, -3.6504e-02,  5.8280e-02, -2.4342e-01,
    # ====      1.6915e-01, -8.3847e-02,  2.4941e-01, -1.6314e-01,  2.2512e-02,
    # ====     -4.1171e-02,  3.2018e-01,  7.1378e-02,  2.1325e-01, -2.0939e-02,
    # ====      2.1973e-01,  2.9444e-01,  1.3969e-01, -4.4239e-01,  3.7464e-01,
    # ====      1.7567e-01,  4.9832e-01,  2.6983e-01, -1.8677e-01, -1.9206e-01,
    # ====     -3.5512e-01, -1.4682e-02,  2.5722e-01, -9.3709e-02, -1.5454e-01,
    # ====      2.1238e-01, -1.0359e-01,  5.7144e-01, -1.5177e-01, -1.3436e-01,
    # ====     -2.4204e-02, -4.5461e-01, -3.1163e-01, -4.4276e-02, -3.1983e-01,
    # ====     -2.4460e-01,  2.4123e-01, -3.3110e-02, -2.1819e-02, -6.3811e-02,
    # ====     -1.8287e-01, -3.5391e-01,  3.6844e-01, -2.6734e-01, -1.9439e-03,
    # ====     -2.6397e-01,  1.7664e-01, -3.4624e-01,  3.8310e-02,  1.0083e-01,
    # ====      1.5129e-01,  5.8206e-01,  1.9658e-01, -4.5622e-01,  1.7478e-01,
    # ====      2.1588e-01, -1.5146e-01,  1.5383e-01, -2.0327e-01,  7.0073e-02,
    # ====      2.0910e-01, -3.9435e-01,  6.1086e-01,  3.5977e-01,  6.7693e-01,
    # ====      1.1565e-01,  2.4593e-01, -2.1931e-01, -2.4728e-01, -2.4199e-01,
    # ====      3.2451e-01,  1.4944e-01, -9.9883e-03,  1.9717e-01,  1.4817e-03,
    # ====      2.3484e-02, -2.0733e-01, -9.5882e-02, -3.4408e-01,  3.1140e-01,
    # ====     -1.4322e-01, -1.1899e-02, -5.0668e-02, -1.7173e-01,  1.1785e-02,
    # ====      1.3847e-02,  3.4291e-01,  2.3850e-01,  3.3350e-01,  1.6080e-01,
    # ====     -2.3552e-02, -1.5835e-01, -6.1145e-02, -2.0813e-01, -3.0239e-01,
    # ====     -3.4605e-01,  1.9877e-01, -8.3019e-02,  5.6182e-02,  9.9121e-02,
    # ====     -1.5528e-01,  3.0567e-02, -7.8040e-03,  4.1713e-01, -1.8567e-01,
    # ====     -2.5150e-01,  3.1062e-01,  1.1797e-01,  9.1393e-02, -5.4641e-02,
    # ====      3.9678e-02, -2.0983e-02, -2.1805e-02, -2.2749e-01,  5.0171e-01,
    # ====      5.4977e-01,  2.6518e-01, -1.6931e-01, -2.6791e-02, -3.9876e-01,
    # ====      1.4739e-01, -3.1497e-01, -3.6837e-02, -4.1853e-01,  1.4145e-01,
    # ====     -3.9508e-01, -2.0239e-01, -1.4278e-02, -9.8426e-02, -2.0106e-01,
    # ====      1.5252e-01, -3.2904e-01,  2.6882e-01, -4.0147e-01, -1.9194e-02,
    # ====      3.5895e-02, -2.3732e-01,  4.9856e-01, -1.2967e-01,  1.8962e-01,
    # ====     -6.2348e-01,  8.1097e-02, -1.5541e-01,  7.5382e-03, -7.7058e-01,
    # ====     -3.6479e-01,  5.9798e-02,  7.1310e-01,  3.4689e-01,  1.2511e-01,
    # ====     -3.7308e-01, -1.1540e-01,  3.6077e-01,  7.0945e-02, -1.9163e-01,
    # ====     -1.9885e-01,  3.0302e-01,  3.5816e-02, -8.0543e-02, -1.5537e-01,
    # ====      1.7031e-01,  2.1551e-01,  2.2636e-01, -2.7598e-01,  1.4445e-01,
    # ====      5.7451e-02,  1.6933e-01, -1.6599e-02]
    # ------------------------------------------------------------------------
    output_raw_1_embedding = outputs[0][:, 1, :]
    # ---- NOTE ---- for single utterance, the above is the same as: output_raw_1_embedding = outputs[0][0][1]
    DebuggingHelper.write_line_to_system_console_out(
        f'length(output_raw_1_embedding)={len(output_raw_1_embedding)}')
    DebuggingHelper.write_line_to_system_console_out(
        f'output_raw_1_embedding={output_raw_1_embedding}')
    output_raw_1_embedding_divided_by_norm = divided_by_norm(output_raw_1_embedding)
    DebuggingHelper.write_line_to_system_console_out(
        f'length(output_raw_1_embedding_divided_by_norm)={len(output_raw_1_embedding_divided_by_norm)}')
    DebuggingHelper.write_line_to_system_console_out(
        f'output_raw_1_embedding_divided_by_norm={output_raw_1_embedding_divided_by_norm}')
    # ------------------------------------------------------------------------
    DebuggingHelper.write_line_to_system_console_out(
        f'DTE model={model}',
        utf8_conversion=False)
    # ------------------------------------------------------------------------
    # ---- NOTE-FOR-REFERENCE ---- Exception has occurred: RuntimeError       (note: full exception trace is shown but execution is paused at: _run_module_as_main)
    # ---- NOTE-FOR-REFERENCE ---- Encountering a dict at the output of the tracer might cause the trace to be incorrect, this is only valid if the container structure does not change based on the module's inputs. Consider using a constant container instead (e.g. for `list`, use a `tuple` instead. for `dict`, use a `NamedTuple` instead). If you absolutely need this and know the side effects, pass strict=False to trace() to allow this behavior.
    # ---- NOTE-FOR-REFERENCE ----   File "D:\Anaconda3\envs\python37_dnn_pytorch150\Lib\site-packages\torch\jit\_trace.py", line 959, in trace_module
    # ---- NOTE-FOR-REFERENCE ----     argument_names,
    # ---- NOTE-FOR-REFERENCE ----   File "D:\Anaconda3\envs\python37_dnn_pytorch150\Lib\site-packages\torch\jit\_trace.py", line 744, in trace
    # ---- NOTE-FOR-REFERENCE ----     _module_class,
    # ---- NOTE-FOR-REFERENCE ----   File "D:\git\EmbedML\private\hunyang\project\LanguageUnderstandingOpenSource\src\python\model\language_understanding\helper\app_pytorch_language_understanding_transformers_helper.py", line 1003, in main_test_model_tokenizer_bert_base_new
    # ---- NOTE-FOR-REFERENCE ----     traced_model = torch.jit.trace(model, input_ids)
    # ---- NOTE-FOR-REFERENCE ----   File "D:\git\EmbedML\private\hunyang\project\LanguageUnderstandingOpenSource\src\python\model\language_understanding\helper\app_pytorch_language_understanding_transformers_helper.py", line 2764, in <module>
    # ---- NOTE-FOR-REFERENCE ----     main_test_model_tokenizer_bert_base_new()
    # ---- NOTE-FOR-REFERENCE ----   File "D:\Anaconda3\envs\python37_dnn_pytorch150\Lib\runpy.py", line 85, in _run_code
    # ---- NOTE-FOR-REFERENCE ----     exec(code, run_globals)
    # ---- NOTE-FOR-REFERENCE ----   File "D:\Anaconda3\envs\python37_dnn_pytorch150\Lib\runpy.py", line 96, in _run_module_code
    # ---- NOTE-FOR-REFERENCE ----     mod_name, mod_spec, pkg_name, script_name)
    # ---- NOTE-FOR-REFERENCE ----   File "D:\Anaconda3\envs\python37_dnn_pytorch150\Lib\runpy.py", line 263, in run_path
    # ---- NOTE-FOR-REFERENCE ----     pkg_name=pkg_name, script_name=fname)
    # ---- NOTE-FOR-REFERENCE ----   File "D:\Anaconda3\envs\python37_dnn_pytorch150\Lib\runpy.py", line 85, in _run_code
    # ---- NOTE-FOR-REFERENCE ----     exec(code, run_globals)
    # ---- NOTE-FOR-REFERENCE ----   File "D:\Anaconda3\envs\python37_dnn_pytorch150\Lib\runpy.py", line 193, in _run_module_as_main (Current frame)
    # ---- NOTE-FOR-REFERENCE ----     "__main__", mod_spec)

    traced_model = torch.jit.trace(model, input_ids, strict=False)
    DebuggingHelper.write_line_to_system_console_out(
        f'DTE traced_model={traced_model}',
        utf8_conversion=False)
    DebuggingHelper.write_line_to_system_console_out(
        f'DTE traced_model.graph={traced_model.graph}',
        utf8_conversion=False)
    DebuggingHelper.write_line_to_system_console_out(
        f'DTE traced_model.code={traced_model.code}',
        utf8_conversion=False)
    # ------------------------------------------------------------------------
    traced_model.save(pytorch_transformers_model_traced_output_file)
    traced_model_loaded = torch.jit.load(pytorch_transformers_model_traced_output_file)
    traced_model_loaded.eval()
    DebuggingHelper.write_line_to_system_console_out(
        f'DTE traced_model_loaded={traced_model_loaded}',
        utf8_conversion=False)
    DebuggingHelper.write_line_to_system_console_out(
        f'DTE traced_model_loaded.graph={traced_model_loaded.graph}',
        utf8_conversion=False)
    DebuggingHelper.write_line_to_system_console_out(
        f'DTE traced_model_loaded.code={traced_model_loaded.code}',
        utf8_conversion=False)
    # ------------------------------------------------------------------------

def main_test_tokenizer_bert_base_uncased():
    """
    The main_test_tokenizer_bert_base_uncased() function can quickly test
    PytorchLanguageUnderstandingTransformersPretainedModelHelper functions.
    """
    # ------------------------------------------------------------------------
    # ---- NOTE-PYLINT ---- R0915: Too many statements
    # pylint: disable=R0915
    # ---- NOTE-PYLINT ---- C0301: Line too long
    # pylint: disable=C0301
    pytorch_transformers_pretrained_model_keys = \
        PytorchLanguageUnderstandingTransformersPretainedModelHelper.get_pretrained_model_keys()
    DebuggingHelper.write_line_to_system_console_out(
        f'Pytorch Transformers pretrained model keys={str(pytorch_transformers_pretrained_model_keys)}')
    parser = argparse.ArgumentParser()
    parser.add_argument(
        '--pytorch_transformers_pretrained_model_key',
        default='bert-base-uncased', # ---- default='bert-base-uncased', # ---- default='bert-large-uncased', # ---- default='bert-base-cased', # ---- default='bert-large-cased', # ---- default='xlnet-large-cased'
        type=str,
        required=False,
        help=f'pytorch_transformers pre-trained model keys: '
             f'{str(pytorch_transformers_pretrained_model_keys)}.')
    parser.add_argument(
        '--pytorch_transformers_model_cache_dir_learner',
        default=os.path.join(
            ConfigurationHelper.MODEL_CACHE_DIRECTORY_STAMPED,
            'pytorch_transformers_cache'),
        type=str,
        required=False,
        help='pytorch_transformers model cache directory.')
    parser.add_argument(
        '--pytorch_transformers_model_saved_dir_learner',
        default=os.path.join(
            ConfigurationHelper.MODEL_CACHE_DIRECTORY_STAMPED,
            'pytorch_transformers'),
        type=str,
        required=False,
        help='pytorch_transformers model cache directory.')
    parser.add_argument(
        '--pytorch_transformers_model_tokenizer',
        default='bert-base-uncased', # ---- default='bert-base-uncased', # ---- default='bert-large-uncased', # ---- default='bert-base-cased', # ---- default='bert-large-cased', # ---- default='xlnet-large-cased'
        type=str,
        required=False,
        help=f'Pytorch Transformers pre-trained model tokenizer vocabulary files: '
             f'{str(PytorchLanguageUnderstandingTransformersPretainedModelHelper.get_pretrained_vocabulary_file_keys())}.')
    parser.add_argument(
        '--pytorch_transformers_model_cache_dir_tokenizer',
        default=os.path.join(
            ConfigurationHelper.MODEL_TOKENIZER_CACHE_DIRECTORY_STAMPED,
            'pytorch_transformers_cache'),
        type=str,
        required=False,
        help='Pytorch Transformers model tokenizer cache directory.')
    parser.add_argument(
        '--pytorch_transformers_model_saved_dir_tokenizer',
        default=os.path.join(
            ConfigurationHelper.MODEL_TOKENIZER_CACHE_DIRECTORY_STAMPED,
            'pytorch_transformers'),
        type=str,
        required=False,
        help='Pytorch Transformers model tokenizer cache directory.')
    args: argparse.Namespace = parser.parse_args()
    # ------------------------------------------------------------------------
    model_key = 'bert-base-cased'
    config = PytorchLanguageUnderstandingTransformersPretainedModelHelper.config_from_pretrained(
        pretrained_model_name_or_path=model_key)
    DebuggingHelper.write_line_to_system_console_out(
        f'config={str(config)}')
    tokenizer = PytorchLanguageUnderstandingTransformersPretainedModelHelper.tokenizer_from_pretrained( \
        pretrained_model_name_or_path=model_key,
        cache_dir=args.pytorch_transformers_model_cache_dir_tokenizer)
    DebuggingHelper.write_line_to_system_console_out(
        f'tokenizer={str(tokenizer)}')
    DebuggingHelper.write_line_to_system_console_out(
        f'tokenizer.vocab_size={tokenizer.vocab_size}')
    # ------------------------------------------------------------------------
    utterance_road_not_taken: str = UTTERANCE_ROAD_NOT_TAKEN
    # ------------------------------------------------------------------------
    utterance: str = \
        utterance_road_not_taken
    output_tokens: List[str] = \
        tokenizer.tokenize(utterance)
    DebuggingHelper.write_line_to_system_console_out(
        f'output_tokens-A={output_tokens}')
    DebuggingHelper.write_line_to_system_console_out(
        f'output_tokens-A={len(output_tokens)}')
    # ---- NOTE-OUTPUT ---- b"output_tokens-A=['Two', 'roads', 'diver', '##ged', 'in', 'a', 'wood', ',', 'and', 'I', '-', '-', 'I', 'took', 'the', 'one', 'less', 'traveled', 'by', ',', 'And', 'that', 'has', 'made', 'all', 'the', 'difference', '.']"
    # ---- NOTE-OUTPUT ---- b'output_tokens-A=28'
    utterance = \
        'wake me up at four fifteen am'
    output_tokens = \
        tokenizer.tokenize(utterance)
    DebuggingHelper.write_line_to_system_console_out(
        f'output_tokens-B={output_tokens}')
    DebuggingHelper.write_line_to_system_console_out(
        f'output_tokens-B={len(output_tokens)}')
    # ---- NOTE-OUTPUT ---- b"output_tokens-B=['wake', 'me', 'up', 'at', 'four', 'fifteen', 'am']"
    # ---- NOTE-OUTPUT ---- b'output_tokens-B=7'
    utterance = \
        'wake me up at four forty-five am'
    output_tokens = \
        tokenizer.tokenize(utterance)
    DebuggingHelper.write_line_to_system_console_out(
        f'output_tokens-C={output_tokens}')
    DebuggingHelper.write_line_to_system_console_out(
        f'output_tokens-C={len(output_tokens)}')
    # ---- NOTE-OUTPUT ---- b"output_tokens-C=['wake', 'me', 'up', 'at', 'four', 'forty', '-', 'five', 'am']"
    # ---- NOTE-OUTPUT ---- b'output_tokens-C=9'
    utterance = \
        'cancel alarm'
    output_tokens = \
        tokenizer.tokenize(utterance)
    DebuggingHelper.write_line_to_system_console_out(
        f'output_tokens-D={output_tokens}')
    DebuggingHelper.write_line_to_system_console_out(
        f'output_tokens-D={len(output_tokens)}')
    # ---- NOTE-OUTPUT ---- b"output_tokens-D=['cancel', 'alarm']"
    # ---- NOTE-OUTPUT ---- b'output_tokens-D=2'
    utterance = \
        utterance_road_not_taken
    output_tokens: List[str] = \
        tokenizer.tokenize(utterance)
    DebuggingHelper.write_line_to_system_console_out(
        f'output_tokens-E={output_tokens}')
    DebuggingHelper.write_line_to_system_console_out(
        f'output_tokens-E={len(output_tokens)}')
    token_id_lists: List[List[int]] = \
        PytorchLanguageUnderstandingTransformersPretainedModelHelper.convert_token_lists_to_ids(output_tokens, tokenizer)
    DebuggingHelper.write_line_to_system_console_out(
        f'output_token_ids-E={token_id_lists}')
    # ---- NOTE-PYLINT ---- C0301: Line too long
    # pylint: disable=C0301
    # ---- NOTE-OUTPUT ---- b"output_tokens-E=['Two', 'roads', 'diver', '##ged', 'in', 'a', 'wood', ',', 'and', 'I', '-', '-', 'I', 'took', 'the', 'one', 'less', 'traveled', 'by', ',', 'And', 'that', 'has', 'made', 'all', 'the', 'difference', '.']"
    # ---- NOTE-OUTPUT ---- b'output_tokens-E=28'
    utterance = \
        utterance_road_not_taken
    output_tokens: List[str] = \
        PytorchLanguageUnderstandingTransformersPretainedModelHelper.tokenize_with_punctuations(utterance, tokenizer)
    DebuggingHelper.write_line_to_system_console_out(
        f'output_tokens-F={output_tokens}')
    DebuggingHelper.write_line_to_system_console_out(
        f'output_tokens-F={len(output_tokens)}')
    token_id_lists: List[List[int]] = \
        PytorchLanguageUnderstandingTransformersPretainedModelHelper.convert_token_lists_to_ids(output_tokens, tokenizer)
    DebuggingHelper.write_line_to_system_console_out(
        f'output_token_ids-F={token_id_lists}')
    # ---- NOTE-PYLINT ---- C0301: Line too long
    # pylint: disable=C0301
    # ---- NOTE-OUTPUT ---- b"output_tokens-F=[['Two'], ['roads'], ['diver', '##ged'], ['in'], ['a'], ['wood'], [','], ['and'], ['I'], ['-'], ['-'], ['I'], ['took'], ['the'], ['one'], ['less'], ['traveled'], ['by'], [','], ['And'], ['that'], ['has'], ['made'], ['all'], ['the'], ['difference'], ['.']]"
    # ---- NOTE-OUTPUT ---- b"segmented_pieces=['Two', 'roads', 'diverged', 'in', 'a', 'wood', ',', 'and', 'I', '-', '-', 'I', 'took', 'the', 'one', 'less', 'traveled', 'by', ',', 'And', 'that', 'has', 'made', 'all', 'the', 'difference', '.']"
    # ---- NOTE-OUTPUT ---- b'output_tokens-F=27'
    utterance = \
        utterance_road_not_taken
    # utterance = \
    #     "it's morning"
    output_tokens: List[str] = \
        PytorchLanguageUnderstandingTransformersPretainedModelHelper.tokenize(utterance, tokenizer)
    DebuggingHelper.write_line_to_system_console_out(
        f'output_tokens-G={output_tokens}')
    DebuggingHelper.write_line_to_system_console_out(
        f'output_tokens-G={len(output_tokens)}')
    token_id_lists: List[List[int]] = \
        PytorchLanguageUnderstandingTransformersPretainedModelHelper.convert_token_lists_to_ids(output_tokens, tokenizer)
    DebuggingHelper.write_line_to_system_console_out(
        f'output_token_ids-G={token_id_lists}')
    # ---- NOTE-PYLINT ---- C0301: Line too long
    # pylint: disable=C0301
    # ---- NOTE-OUTPUT ---- b"output_tokens-G=[['Two'], ['roads'], ['diver', '##ged'], ['in'], ['a'], ['wood'], [','], ['and'], ['I'], ['-', '-'], ['I'], ['took'], ['the'], ['one'], ['less'], ['traveled'], ['by'], [','], ['And'], ['that'], ['has'], ['made'], ['all'], ['the'], ['difference'], ['.']]"
    # ---- NOTE-OUTPUT ---- b"segmented_pieces=[' Two', ' roads', ' diverged', ' in', ' a', ' wood', ',', ' and', ' I', ' --', ' I', ' took', ' the', ' one', ' less', ' traveled', ' by', ',', ' And', ' that', ' has', ' made', ' all', ' the', ' difference', '.']"
    # ---- NOTE-OUTPUT ---- b'output_tokens-G=26'
    # ------------------------------------------------------------------------

def main_test_model_tokenizer_roberta_base_new():
    """
    The main_test_model_tokenizer_roberta_base_new() function can quickly test
    PytorchLanguageUnderstandingTransformersPretainedModelHelper functions.
    """
    # ------------------------------------------------------------------------
    # ---- NOTE-PYLINT ---- R0915: Too many statements
    # pylint: disable=R0915
    # ---- NOTE-PYLINT ---- C0301: Line too long
    # pylint: disable=C0301
    # ---- NOTE-PYLINT ---- R0914: Too many local variables (19/15) (too-many-locals)
    # pylint: disable=R0914
    parser = argparse.ArgumentParser()
    parser.add_argument(
        '--pytorch_transformers_vocab_file',
        default='vocab.json',
        type=str,
        required=False,
        help=f'pytorch transformers vocab file')
    parser.add_argument(
        '--pytorch_transformers_merges_file',
        default='merges.txt',
        type=str,
        required=False,
        help=f'pytorch transformers merges file')
    parser.add_argument(
        '--pytorch_transformers_model_dir_tokenizer',
        default='',
        type=str,
        required=False,
        help='Pytorch Transformers model tokenizer directory.')
    parser.add_argument(
        '--pytorch_transformers_model_file',
        default='pytorch_model.bin',
        type=str,
        required=False,
        help=f'pytorch transformers model file')
    parser.add_argument(
        '--pytorch_transformers_model_dir_learner',
        default='',
        type=str,
        required=False,
        help='Pytorch Transformers model learner directory.')
    parser.add_argument(
        '--pytorch_transformers_model_config_file',
        default='config.json',
        type=str,
        required=False,
        help='pytorch_transformers model configuration file.')
    args: argparse.Namespace = parser.parse_args()
    # ------------------------------------------------------------------------
    pytorch_transformers_model_dir_tokenizer: str = \
        args.pytorch_transformers_model_dir_tokenizer
    pytorch_transformers_model_dir_tokenizer = \
        r"/model_dte_proprietary/collected_DTE_pytorch/dte_tnlr-v3_12l_bing_cortana_mixture/tokenizer"
    pytorch_transformers_model_dir_learner: str = \
        args.pytorch_transformers_model_dir_learner
    pytorch_transformers_model_dir_learner = \
        r"/model_dte_proprietary/collected_DTE_pytorch/dte_tnlr-v3_12l_bing_cortana_mixture/model"
    vocab_file: str = \
        os.path.join(pytorch_transformers_model_dir_tokenizer, args.pytorch_transformers_vocab_file)
    merges_file: str = \
        os.path.join(pytorch_transformers_model_dir_tokenizer, args.pytorch_transformers_merges_file)
    config_file: str = \
        os.path.join(pytorch_transformers_model_dir_learner, args.pytorch_transformers_model_config_file)
    model_file: str = \
        os.path.join(pytorch_transformers_model_dir_learner, args.pytorch_transformers_model_file)
    # ------------------------------------------------------------------------
    tokenizer = PytorchLanguageUnderstandingTransformersPretainedModelHelper.roberta_tokenizer_new( \
        vocab_file, \
        merges_file)
    DebuggingHelper.write_line_to_system_console_out(
        f'tokenizer={str(tokenizer)}')
    DebuggingHelper.write_line_to_system_console_out(
        f'tokenizer.vocab_size={tokenizer.vocab_size}')
    # ---- NOTE-OUTPUT ---- b'tokenizer.vocab_size=50265'
    # ------------------------------------------------------------------------
    with open(config_file) as config_file_handle:
        config_json = json.load(config_file_handle)
    config_default = PytorchLanguageUnderstandingTransformersPretainedModelHelper.roberta_config_new_from_dictionary( \
        {})
    DebuggingHelper.write_line_to_system_console_out(
        f'config_default={str(config_default)}')
    config = PytorchLanguageUnderstandingTransformersPretainedModelHelper.roberta_config_new_from_dictionary( \
        config_json)
    DebuggingHelper.write_line_to_system_console_out(
        f'config_json={str(config_json)}')
    DebuggingHelper.write_line_to_system_console_out(
        f'config={str(config)}')
    # ------------------------------------------------------------------------
    model = PytorchLanguageUnderstandingTransformersPretainedModelHelper.roberta_model_from_pretrained(model_file, config=config)
    model.eval()
    # ------------------------------------------------------------------------
    utterance_road_not_taken: str = UTTERANCE_ROAD_NOT_TAKEN
    utterance_road_not_taken = ' ' + utterance_road_not_taken
    # ------------------------------------------------------------------------
    token_id_lists: List[int] = tokenizer.encode(utterance_road_not_taken)
    DebuggingHelper.write_line_to_system_console_out(
        f'token_id_lists={str(token_id_lists)}')
    DebuggingHelper.write_line_to_system_console_out(
        f'len(token_id_lists)={len(token_id_lists)}')
    # ---- NOTE-PYLINT ---- C0301: Line too long
    # pylint: disable=C0301
    # ---- NOTE-OUTPUT ---- b'token_id_lists=[0, 1596, 3197, 13105, 4462, 11, 10, 5627, 6, 8, 38, 480, 38, 362, 5, 65, 540, 8468, 30, 6, 178, 14, 34, 156, 70, 5, 2249, 4, 2]'
    # ---- NOTE-OUTPUT ---- b'len(token_id_lists)=29'
    encoded = tokenizer.encode_plus(utterance_road_not_taken)
    DebuggingHelper.write_line_to_system_console_out(
        f'encoded={str(encoded)}')
    DebuggingHelper.write_line_to_system_console_out(
        f'len(encoded)={len(encoded)}')
    # ---- NOTE-PYLINT ---- C0301: Line too long
    # pylint: disable=C0301
    # ---- NOTE-OUTPUT ---- b"encoded={'input_ids': [0, 1596, 3197, 13105, 4462, 11, 10, 5627, 6, 8, 38, 480, 38, 362, 5, 65, 540, 8468, 30, 6, 178, 14, 34, 156, 70, 5, 2249, 4, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
    # ---- NOTE-OUTPUT ---- b'len(encoded)=2'
    # ------------------------------------------------------------------------
    # ---- NOTE-PYLINT ---- E1102: torch.tensor is not callable (not-callable)
    # pylint: disable=E1102
    input_ids = torch.tensor(tokenizer.encode(utterance_road_not_taken, add_special_tokens=True)).unsqueeze(0)  # ---- Batch size 1
    outputs = model(input_ids)
    # ==== outputs = model(input_ids, masked_lm_labels=input_ids)
    DebuggingHelper.write_line_to_system_console_out(
        f'input_ids={str(input_ids)}')
    DebuggingHelper.write_line_to_system_console_out(
        f'len(outputs)={len(outputs)}')
    DebuggingHelper.write_line_to_system_console_out(
        f'len(outputs[0][0][0])={len(outputs[0][0][0])}')
    DebuggingHelper.write_line_to_system_console_out(
        f'len(outputs[0][0][1])={len(outputs[0][0][1])}')
    DebuggingHelper.write_line_to_system_console_out(
        f'len(outputs[0][0])={len(outputs[0][0])}')
    DebuggingHelper.write_line_to_system_console_out(
        f'len(outputs[0])={len(outputs[0])}')
    # ==== DebuggingHelper.write_line_to_system_console_out(
    # ====     f'outputs[0].get_shape()={outputs[0].get_shape()}')
    DebuggingHelper.write_line_to_system_console_out(
        f'len(outputs[1])={len(outputs[1])}')
    # ==== DebuggingHelper.write_line_to_system_console_out(
    # ====     f'outputs[1].get_shape()={outputs[1].get_shape()}')
    DebuggingHelper.write_line_to_system_console_out(
        f'outputs[0][0][0]={outputs[0][0][0]}')
    # ==== outputs[0][0][0]=tensor([-2.2953e-01, -8.6760e-01, -1.3049e-01,  7.0627e-02,  1.5719e+00,
    # ====     -4.1814e-01, -1.8130e-01,  2.3001e-01, -3.6832e-01, -2.9287e-01,
    # ====      6.8390e-01,  9.4028e-01, -5.1265e-01,  3.3825e-01,  4.3878e-01,
    # ====     -6.3531e-01, -5.7002e-01, -3.6665e-01, -7.2602e-02, -3.1195e-01,
    # ====      1.4893e-01,  2.7354e-01, -1.9985e-01, -4.5694e-01,  3.4209e-01,
    # ====     -5.4518e-01,  8.3613e-02,  8.9714e-01,  1.6408e-01, -1.6356e-01,
    # ====      4.3922e-01,  1.2769e-01,  4.4166e-01,  3.9255e-01, -2.7356e-01,
    # ====     -3.4042e-01,  3.4515e-01,  9.0092e-01,  2.1167e-01, -6.1869e-01,
    # ====      4.5103e-01, -6.2294e-01,  2.1635e-01, -2.9099e-01,  4.7289e-01,
    # ====     -4.6438e-01, -6.1920e-01,  2.7967e-01,  2.7988e-01,  7.9668e-02,
    # ====     -7.3220e-01, -1.3592e+00, -2.5205e-01, -7.1272e-02,  3.2840e-01,
    # ====      2.6663e-02,  3.5145e-01,  6.6855e-01, -8.7702e-01, -3.5177e-01,
    # ====      1.0122e-01,  1.2090e+00, -5.3579e-01,  6.0265e-01,  4.1572e-01,
    # ====     -3.1127e-01, -2.3993e-01, -2.7573e-01, -3.8006e-01, -1.1776e-01,
    # ====     -3.4973e-02, -1.4507e+00,  2.9421e-01, -2.2439e-01,  9.3015e-01,
    # ====     -6.4782e-01, -4.2426e-01, -1.2093e+00, -4.1123e-01,  5.6787e-01,
    # ====      9.6994e-01, -7.8499e-02, -7.9094e-02, -1.4121e-01, -1.5368e-01,
    # ====     -2.4383e-03, -1.2912e-01, -9.1224e-03, -1.4498e-02,  1.9919e-01,
    # ====      6.3102e-01,  7.1355e-01, -4.5219e-01,  4.2484e-01, -8.4884e-01,
    # ====      5.6303e-01, -2.9709e-01, -9.8335e-02,  3.5200e-01, -3.7125e-01,
    # ====      5.1189e-01,  8.5332e-01,  2.2199e-01, -1.4990e-01,  6.1179e-01,
    # ====      6.9694e-01, -3.1468e-01, -6.2164e-02,  4.4499e-01,  3.7134e-01,
    # ====      3.6648e-01, -4.2214e-01, -9.2279e-02, -7.0366e-01,  4.0881e-01,
    # ====      7.9729e-01, -4.1028e-01, -2.4970e-01, -1.5655e-01,  2.2119e-01,
    # ====      1.0122e+00, -3.6576e-01, -1.1669e-01,  6.4286e-01, -1.1257e-01,
    # ====      3.1613e-01,  5.1157e-01,  6.8319e-01,  7.1066e-02,  2.7381e-01,
    # ====     -2.5654e-01, -6.8546e-01,  1.7770e-01, -2.0877e-01, -6.2847e-01,
    # ====     -5.0020e-01,  8.6962e-02, -5.7430e-01, -5.0058e-02, -5.8598e-01,
    # ====     -6.9405e-01,  5.0685e-01, -1.7404e-01,  9.6407e-01, -6.9958e-01,
    # ====      2.0752e-01,  6.9485e-01,  6.3427e-01, -6.9615e-01, -1.4770e+00,
    # ====     -2.1723e-01,  4.5171e-01, -5.3088e-01, -7.7952e-01,  3.5559e-02,
    # ====     -2.5114e-01,  3.1670e-01,  1.4811e-01, -4.0270e-01, -1.8298e-01,
    # ====     -2.3860e-02,  4.3110e-01,  9.1325e-02, -2.2308e-01, -2.1516e-01,
    # ====     -4.3047e-01,  2.2054e-01, -8.2146e-02, -6.3024e-01,  3.0281e-01,
    # ====      9.1241e-01,  6.5680e-02,  4.3308e-01, -3.4477e-01,  1.5414e-01,
    # ====      4.7870e-01, -1.1110e-01, -3.0940e-01, -1.0986e+00, -4.3471e-01,
    # ====     -5.0140e-01,  1.1502e+00, -4.1961e-01,  2.5785e-01,  1.8244e-01,
    # ====      4.9665e-02,  2.4134e-02, -5.2579e-02,  2.3437e-01, -4.6741e-02,
    # ====     -1.3894e-01,  1.6305e-01,  5.6792e-01,  6.1620e-01,  2.0009e-01,
    # ====      1.7482e-01,  4.0985e-01, -8.6657e-02,  3.2150e-01, -7.1366e-01,
    # ====     -2.5309e-01, -3.2119e-01, -7.3321e-01, -3.6545e-02,  5.2119e-01,
    # ====     -3.2616e-01,  2.5381e-01,  1.8135e-01, -2.1077e-01, -8.9504e-01,
    # ====      5.6062e-01,  4.4240e-01,  2.3921e-01, -3.0082e-01, -8.2083e-01,
    # ====     -3.0440e-01, -6.2794e-01, -5.0982e-01, -5.4538e-01, -2.3486e-01,
    # ====      8.1553e-01,  6.1983e-01, -6.1801e-01, -9.0246e-01,  2.6575e-01,
    # ====      9.5010e-01, -1.9644e-01,  6.1539e-01, -1.2382e+00, -5.4568e-01,
    # ====     -3.9367e-01, -1.3789e-01, -3.3693e-01, -6.8109e-01,  7.2747e-01,
    # ====     -7.8778e-01, -1.6502e-01,  1.9066e-01,  9.2523e-01,  3.8349e-01,
    # ====     -4.0785e-01,  2.8958e-01,  4.8273e-01,  7.1619e-01,  1.3545e+00,
    # ====     -2.5772e-02,  5.1138e-01, -5.5553e-01, -2.7793e-01,  6.8891e-01,
    # ====      8.5027e-01, -4.9946e-01, -2.8922e-01,  4.9907e-03, -6.8728e-01,
    # ====     -8.4059e-02,  2.4997e-01,  6.9423e-01,  5.3837e-01,  1.0323e+00,
    # ====     -2.3652e-01, -1.6422e-01,  4.6097e-02,  7.1231e-01,  7.0829e-01,
    # ====     -2.6971e-01,  3.5881e-01, -3.5143e-01,  8.0121e-01, -4.9715e-01,
    # ====     -3.6787e-01,  8.2318e-02, -1.2707e-01,  7.1057e-01,  1.0473e-02,
    # ====      3.7413e-01, -3.2358e-01,  4.7024e-01, -7.5415e-01,  5.2866e-01,
    # ====      4.1064e-01, -5.5260e-01,  7.2641e-02,  8.6484e-01, -1.1028e+00,
    # ====     -4.1383e-01,  5.1138e-01,  1.5166e-01, -1.3555e-01,  4.3559e-01,
    # ====      5.7033e-02,  4.0379e-01,  9.9668e-01, -1.6978e-01, -7.5852e-01,
    # ====     -4.6958e-02, -9.0424e-01,  2.8138e-01, -7.7636e-01,  7.5649e-02,
    # ====      8.8112e-02,  5.1189e-01, -6.3547e-02,  1.1034e+00, -1.2319e-01,
    # ====      3.6806e-01,  4.4567e-01,  7.2561e-01, -6.8182e-02,  2.5854e-01,
    # ====     -9.7536e-01, -3.0658e-01, -4.6387e-01, -6.0338e-01, -2.5698e-01,
    # ====     -7.6732e-01,  2.2901e-01,  1.0069e-03,  1.5012e-01, -4.4019e-02,
    # ====     -5.5995e-01, -8.5648e-01,  2.1225e-01, -7.3009e-01, -1.0668e-01,
    # ====      2.5335e-01,  4.3914e-02,  3.5417e-01, -7.4031e-01,  2.4670e-01,
    # ====     -3.6075e-02, -2.1014e-01, -9.2342e-01,  2.7557e-01, -1.4204e-01,
    # ====     -3.0165e-01,  2.5293e-01,  7.1008e-01, -2.4551e-01, -2.9577e-01,
    # ====      4.0115e-01,  1.0585e+00,  7.7638e-02, -7.6701e-02,  1.1228e-01,
    # ====     -2.8153e-01,  4.5497e-01, -6.6325e-01, -3.0979e-01,  2.8731e-02,
    # ====     -8.3592e-01, -3.0698e-01, -6.7801e-01,  5.6885e-01,  3.3813e-01,
    # ====     -1.8087e-01, -6.5308e-01,  1.0028e+00, -7.8068e-01, -2.2339e-01,
    # ====     -1.2512e+00,  6.4227e-01,  1.0094e+00, -3.4460e-01,  1.2577e+00,
    # ====      3.0521e-01,  3.1242e-01,  1.0342e-01, -1.1024e+00,  7.0224e-01,
    # ====      1.3149e-01, -3.8529e-01,  1.6033e-01,  2.7528e-01,  6.8722e-03,
    # ====      4.5798e-01, -7.6508e-01, -1.4631e-01,  1.0218e-01, -1.9360e-01,
    # ====      1.7714e-01,  9.6633e-02,  3.9704e-01,  4.3766e-01, -1.9633e-01,
    # ====     -2.9393e-01,  3.8651e-01,  5.0546e-01,  6.4958e-01, -1.3981e-01,
    # ====     -3.1557e-01, -9.0167e-01, -1.2136e+00, -3.1435e-01,  2.3004e-01,
    # ====     -7.9397e-01, -4.8289e-01,  1.4971e-01, -1.5661e-01, -1.2209e-01,
    # ====     -4.8225e-01,  8.4445e-01, -2.1028e-02, -3.3423e-01, -2.9407e-02,
    # ====      3.9345e-01, -1.2975e+00, -2.2230e-01, -1.2278e-01, -4.9076e-01,
    # ====     -2.3392e-01,  4.3872e-01,  5.2702e-01, -8.9103e-01,  6.7218e-01,
    # ====      9.5914e-01,  1.4646e+00, -7.9985e-01, -2.1620e-01, -7.1701e-03,
    # ====     -1.9765e-01,  8.0427e-03,  3.8204e-01,  3.4201e-01,  4.0925e-01,
    # ====      1.5644e+00,  9.7818e-02, -6.1878e-01, -2.6290e-01,  3.8272e-01,
    # ====      3.0639e-01,  1.1895e+00, -1.2016e-01,  1.4507e+00,  7.1300e-02,
    # ====     -7.1535e-01,  2.3382e-01,  3.6722e-01, -1.5823e-01, -1.8911e-01,
    # ====      1.1031e-01, -7.6146e-01, -2.6592e-01, -9.1633e-01, -2.2831e-01,
    # ====      2.5828e-01, -3.7441e-01,  1.0928e-01, -7.0773e-01,  1.2323e-01,
    # ====     -4.0083e-01, -2.6965e-01,  5.2345e-03, -1.6798e+00,  3.7390e-01,
    # ====     -2.7422e-02,  1.6628e-01, -9.6804e-02, -2.4569e-01,  6.3234e-01,
    # ====     -4.5364e-02, -1.7463e-01, -3.0381e-01,  5.7870e-02,  6.6258e-02,
    # ====     -6.1522e-01,  1.6253e+00, -8.5183e-01,  1.8421e-01,  9.2061e-01,
    # ====     -1.9038e-01,  1.6766e-01,  1.5719e-01,  8.3064e-01, -9.0677e-01,
    # ====      1.9351e-01, -6.8531e-02, -1.1838e-01, -6.9683e-01,  3.0156e-02,
    # ====      7.5686e-01, -1.8245e-01,  1.2828e+00,  1.0916e+00,  8.4193e-01,
    # ====     -2.2576e-01, -1.7079e-01,  1.1124e+00,  6.7811e-01, -3.5217e-01,
    # ====     -3.2904e-01, -3.2054e-01, -2.7681e-01,  1.9604e-01, -1.3298e-01,
    # ====      1.8102e-01, -2.8041e-02, -1.7534e-01, -2.3138e-01,  3.5186e-01,
    # ====     -1.3179e-02, -6.4613e-01,  5.6201e-02, -3.5716e-01, -1.1904e-01,
    # ====     -1.1320e+00,  1.5355e-01, -4.1323e-01,  1.3839e-01, -6.3496e-01,
    # ====      2.5123e-01, -4.6122e-01,  6.2725e-02,  3.7547e-01,  2.1205e-01,
    # ====     -2.3409e-01, -2.2077e-02, -6.4720e-02,  2.1202e-01,  5.1035e-01,
    # ====     -8.6036e-01, -9.5883e-01, -8.9149e-01,  2.9647e-01, -2.0549e-01,
    # ====      6.7078e-02, -4.7877e-02, -1.6631e-01, -2.4674e-01, -6.1233e-01,
    # ====     -3.6139e-01,  3.5207e-01, -4.8000e-01,  1.9030e-01,  2.2862e-01,
    # ====     -3.1750e-01, -2.7904e-01,  2.6620e-02,  6.8168e-01, -4.3245e-01,
    # ====      6.0179e-01, -8.6882e-02,  5.1420e-01,  6.6846e-01, -4.8942e-01,
    # ====      4.3511e-01,  8.1610e-01, -8.2472e-02, -1.1015e-01,  5.1431e-01,
    # ====     -2.7198e-01, -7.0739e-01, -9.3136e-02,  2.3598e-01,  7.1992e-02,
    # ====      1.1988e+00,  4.2225e-01,  3.7996e-01,  5.9742e-01,  2.6951e-01,
    # ====     -5.6812e-02, -3.2226e-01,  8.4166e-01, -1.0874e+00, -8.5332e-01,
    # ====      1.5286e+00,  1.4312e-01, -4.0836e-01, -1.0307e+00, -4.2534e-01,
    # ====     -3.9668e-01,  5.3186e-01, -6.9091e-02,  4.5460e-01,  5.2273e-01,
    # ====     -6.7396e-01,  1.2093e-01, -1.7408e-01, -7.7528e-01, -1.0028e-01,
    # ====      7.5790e-01,  6.7572e-01, -8.1781e-02, -5.1498e-01,  6.4830e-01,
    # ====      5.8146e-01,  4.5339e-02,  6.7972e-02,  1.7040e+00, -4.1715e-01,
    # ====      1.0898e+00,  7.2827e-02,  2.5151e-01, -6.5060e-01, -3.9874e-01,
    # ====      3.6537e-01,  5.9199e-01, -3.1409e-01,  9.2529e-02,  4.4464e-01,
    # ====      6.9215e-01, -1.8158e-02, -3.3650e-02, -1.6194e-01, -4.2368e-01,
    # ====     -4.7358e-01, -6.3526e-01,  6.1680e-01, -8.2417e-02,  2.8897e-01,
    # ====      2.5602e-01, -3.6649e-01,  6.4816e-01,  7.4561e-01,  3.0648e-01,
    # ====     -1.0590e+00, -3.7811e-01,  4.2708e-02,  2.4554e-01, -2.0004e-01,
    # ====     -9.6091e-02,  5.4886e-01,  9.1263e-01,  6.3757e-01, -9.6766e-02,
    # ====     -4.8184e-01, -6.5513e-01,  2.4212e-01, -1.9056e-01,  1.6843e-01,
    # ====      1.1000e+00, -5.9101e-01,  2.9173e-01, -9.7736e-01, -4.3031e-02,
    # ====     -5.0432e-01,  7.8905e-01, -6.3812e-01,  1.5468e-01,  4.0650e-02,
    # ====      8.1161e-01, -4.0711e-01, -3.6903e-01, -2.7736e-01,  2.6063e-01,
    # ====      4.0480e-02, -2.7442e-01, -2.5354e-01, -3.5137e-01, -1.6969e-01,
    # ====     -5.9199e-01, -3.2228e-01,  5.3613e-02, -4.2127e-01,  1.3796e-01,
    # ====     -3.8280e-01, -1.1346e+00,  3.3629e-02,  8.3283e-01,  6.0415e-02,
    # ====      1.0422e+00,  6.0512e-02,  3.6505e-01, -3.4007e-02,  3.6446e-01,
    # ====     -5.6517e-01,  1.8085e-01, -1.1663e+00, -8.8643e-01,  5.7756e-01,
    # ====      6.1341e-01, -3.9390e-01, -2.7217e-01, -5.1765e-01, -6.7767e-01,
    # ====     -9.7806e-02,  2.8944e-01, -2.1863e-01, -7.6392e-01,  3.4577e-01,
    # ====      8.3321e-01,  1.0017e+00,  5.0588e-01, -1.3383e-01,  2.9811e-01,
    # ====     -3.4087e-01, -1.1054e+00,  2.1275e-01, -4.1603e-01, -3.6250e-01,
    # ====     -4.8938e-01,  1.7790e-01,  4.1119e-01, -1.0893e-01,  3.1788e-01,
    # ====      3.6394e-01, -2.6408e-01, -4.4063e-01, -1.6084e-01, -8.0249e-01,
    # ====      5.5410e-01,  1.7320e-01,  3.6055e-01,  1.1530e-01, -4.5572e-02,
    # ====     -5.4730e-01,  1.0238e-01,  5.4579e-02,  8.0333e-01, -1.4088e-01,
    # ====      2.9023e-01,  2.2133e-01, -4.2637e-01,  3.6794e-02,  7.6550e-01,
    # ====      2.6639e-01, -5.5603e-01, -4.7308e-01, -3.4949e-01, -3.3157e-01,
    # ====     -4.8555e-01, -3.8333e-02,  6.7319e-01,  5.6911e-01,  4.3587e-01,
    # ====     -1.4045e-01, -1.4260e-02,  9.8556e-01,  5.2827e-01, -3.9959e-01,
    # ====      7.1524e-01, -2.8006e-01, -1.2251e+00, -1.4394e-02,  8.7717e-01,
    # ====      5.6043e-01, -6.4992e-01, -4.7382e-01, -5.8564e-02, -1.1188e+00,
    # ====     -6.2022e-01, -6.3917e-01,  2.8031e-01, -8.7641e-01, -1.5188e-01,
    # ====      3.7945e-02,  9.2608e-01, -4.5989e-01,  2.7226e-01, -9.0524e-02,
    # ====      1.0967e-01,  4.1305e-02, -6.0938e-01, -2.9479e-01, -3.5042e-01,
    # ====     -2.6737e-01,  9.4940e-01, -2.4925e-01,  9.2110e-01, -9.5887e-02,
    # ====     -9.6115e-01,  2.3952e-02,  6.6885e-01,  1.1559e+00,  2.5682e-01,
    # ====      7.5284e-01,  1.8328e-01,  1.2768e-01]
    DebuggingHelper.write_line_to_system_console_out(
        f'outputs[0][0][1]={outputs[0][0][1]}')
    DebuggingHelper.write_line_to_system_console_out(
        f'outputs[0][0]={outputs[0][0]}')
    DebuggingHelper.write_line_to_system_console_out(
        f'outputs[0]={outputs[0]}')
    DebuggingHelper.write_line_to_system_console_out(
        f'outputs[1]={outputs[1]}')
    # ==== outputs[1]=tensor([[ 2.3780e-01, -3.0412e-01,  2.1434e-02,  4.6586e-01, -1.5202e-01,
    # ====       8.3916e-02,  1.1665e-02, -3.5813e-01, -1.6795e-01, -2.3258e-02,
    # ====       2.7812e-01,  9.2162e-02,  5.3046e-03, -2.4961e-02,  5.6152e-01,
    # ====      -9.9189e-02,  3.3320e-01,  9.5694e-02, -9.8245e-02,  3.1566e-01,
    # ====       1.3260e-01,  6.1052e-02, -9.1882e-02, -3.3286e-01, -3.3075e-01,
    # ====      -3.2643e-01, -1.2561e-01,  1.6043e-02,  9.7025e-02, -2.2732e-01,
    # ====      -2.5221e-01,  1.5976e-01,  2.6602e-01,  4.2906e-01,  3.6056e-02,
    # ====      -1.6573e-01,  8.2801e-02, -3.2699e-02,  1.8848e-01,  9.0040e-02,
    # ====       6.2523e-02, -3.0378e-01,  5.9705e-03,  1.8965e-01,  8.1363e-02,
    # ====       4.0084e-01, -4.9026e-01, -4.3784e-02,  2.4574e-01,  2.2810e-02,
    # ====      -9.3012e-02,  2.5101e-01,  1.2863e-01,  2.1125e-01, -2.8817e-01,
    # ====       8.1864e-02,  1.5216e-01,  5.3342e-01, -2.6435e-01, -9.2744e-02,
    # ====       1.5647e-01, -2.5684e-01, -3.0837e-01,  6.8884e-02,  1.4958e-01,
    # ====       4.9443e-01,  1.7811e-02, -5.0137e-01,  5.3307e-01,  1.7083e-01,
    # ====      -1.7848e-01, -1.1237e-02,  1.2467e-01, -2.0705e-01,  3.0263e-02,
    # ====       4.5501e-02,  9.7000e-02,  1.0449e-02,  3.2800e-02,  5.7993e-01,
    # ====      -3.5108e-01,  2.2748e-01, -1.0889e-01,  3.8787e-01, -1.1419e-01,
    # ====      -2.5409e-01,  5.4762e-01, -2.2241e-01,  4.6504e-01,  1.9327e-01,
    # ====      -1.7696e-01, -1.6979e-01, -6.3501e-01,  1.1569e-01,  1.6185e-01,
    # ====      -1.1586e-01,  3.2062e-02,  3.7934e-01, -3.3139e-01, -4.0511e-01,
    # ====      -2.3025e-01,  2.3855e-01, -8.2766e-02, -2.5105e-02, -7.2548e-04,
    # ====      -3.2419e-01,  1.2291e-01,  4.5731e-02,  2.9236e-02, -7.7629e-02,
    # ====      -8.8676e-02,  8.9778e-03, -1.8077e-01,  7.9343e-02, -2.0499e-01,
    # ====       3.3195e-01, -2.4846e-03,  1.0176e-01,  8.5223e-03,  3.3093e-01,
    # ====      -1.4817e-02,  2.5116e-01,  3.1102e-01, -3.6625e-01,  3.3248e-01,
    # ====      -8.8569e-02,  3.3736e-01,  2.5086e-01, -1.4605e-01,  2.0655e-01,
    # ====       5.6329e-01,  1.9339e-01,  1.3518e-01,  5.7426e-02,  9.7485e-02,
    # ====      -4.4186e-01, -2.7214e-01, -4.8588e-01, -4.4229e-01,  3.6608e-03,
    # ====       6.1908e-01,  6.0506e-01, -6.4587e-02,  4.8148e-01,  2.8363e-01,
    # ====       2.0545e-01,  2.5718e-01,  6.9513e-02, -5.3518e-01,  3.3161e-01,
    # ====      -1.0837e-02, -4.4960e-01, -6.7696e-02, -1.3024e-01,  7.1689e-02,
    # ====       3.0727e-02,  2.2947e-01,  1.1912e-01,  2.8748e-01,  3.3741e-01,
    # ====       6.0833e-03, -8.4919e-02,  3.5552e-01,  7.1345e-01, -8.1756e-02,
    # ====       1.6312e-01, -4.6786e-02,  4.2761e-01,  1.3845e-01,  1.1672e-01,
    # ====      -2.0439e-01,  3.6028e-01, -1.7743e-01, -8.4144e-02, -3.9610e-01,
    # ====      -1.2362e-02,  1.9296e-01,  1.0337e-01, -2.2005e-01, -4.6356e-01,
    # ====       1.9362e-01,  2.7986e-02,  1.3090e-02, -5.9553e-02, -9.5183e-02,
    # ====      -1.9229e-01,  4.9903e-02, -1.5689e-01,  4.1229e-01, -1.1880e-01,
    # ====      -1.0099e-01,  1.7553e-01, -9.8408e-02, -8.9157e-02, -3.4104e-02,
    # ====      -9.1213e-02, -2.0511e-01,  1.8192e-01, -9.5908e-04,  5.0613e-01,
    # ====      -1.7195e-01, -4.7063e-01, -4.7560e-01,  2.0503e-02,  1.4620e-01,
    # ====      -5.4693e-01,  1.8622e-01,  3.5844e-01, -5.7130e-01,  1.1834e-01,
    # ====      -2.0507e-02,  2.6918e-01, -3.9478e-02,  3.7778e-01,  1.4518e-01,
    # ====      -2.2945e-01, -3.5283e-01, -3.3451e-01,  4.2335e-01, -1.1177e-02,
    # ====       1.2679e-01,  2.8887e-02, -2.2525e-01,  3.3221e-01,  4.5358e-02,
    # ====      -3.7157e-01, -5.3361e-01,  1.0115e-01, -1.0933e-01, -4.3997e-02,
    # ====       1.4876e-02,  1.1554e-02, -1.8838e-01,  3.2342e-01, -1.6856e-01,
    # ====      -2.1819e-01,  6.7125e-02,  5.5605e-01, -1.4550e-01, -1.7827e-01,
    # ====      -2.2823e-01, -1.4951e-01,  2.0492e-01,  1.3434e-01,  2.7216e-01,
    # ====       2.0237e-01, -1.7931e-01,  1.9177e-01, -3.7086e-02, -1.8619e-01,
    # ====      -2.3198e-01, -3.6374e-01,  2.3099e-01, -8.1393e-02, -6.0735e-02,
    # ====      -5.0450e-01,  2.7193e-01,  4.8250e-01, -1.4152e-01, -2.3374e-01,
    # ====      -4.9315e-02, -4.9204e-01,  1.8754e-02,  3.7671e-01, -2.9621e-01,
    # ====       1.3268e-01,  3.3580e-01,  1.1954e-01,  5.7725e-01,  8.6994e-02,
    # ====      -1.5417e-01,  5.2908e-01,  2.8992e-01, -3.4209e-03, -3.3726e-01,
    # ====       3.4494e-02, -1.1771e-01,  2.4373e-01,  3.8713e-01, -4.7876e-02,
    # ====      -9.6475e-02,  1.1621e-01, -2.9653e-01, -4.7479e-01, -5.0333e-01,
    # ====      -2.7750e-01,  5.2046e-01,  4.2127e-01, -5.7056e-02, -1.4724e-02,
    # ====       4.5199e-01,  6.2763e-02,  1.6690e-01, -1.1642e-01, -3.5289e-01,
    # ====       1.3076e-01,  4.1174e-02, -2.3433e-01, -3.3328e-01, -1.8152e-01,
    # ====      -2.3211e-02, -2.3328e-01, -2.9519e-01, -2.7662e-01,  4.8027e-02,
    # ====      -3.9433e-01,  2.3392e-01, -1.6616e-01, -3.4071e-01, -1.1190e-01,
    # ====       1.5159e-01,  6.3510e-01, -1.3919e-01, -3.9872e-02, -2.1690e-01,
    # ====       3.5884e-01, -4.6205e-01, -4.0713e-02,  2.4853e-02,  1.3465e-01,
    # ====      -4.9082e-02, -4.8522e-01, -9.0743e-02,  3.7919e-01, -9.1492e-02,
    # ====       5.4264e-01,  1.6631e-01,  5.0900e-01, -4.3632e-01, -4.0903e-01,
    # ====       1.6561e-01,  2.3155e-02,  2.2308e-01, -2.1635e-01, -3.4732e-01,
    # ====      -3.4822e-01, -9.0326e-02,  2.2806e-01,  3.0292e-01,  1.1719e-01,
    # ====       1.1580e-01,  8.4161e-02,  3.7484e-01,  3.0164e-01,  1.0607e-01,
    # ====      -1.5614e-01, -1.9234e-02,  3.6364e-01, -3.8896e-01,  6.1473e-02,
    # ====      -2.1823e-01, -5.9192e-01, -3.4082e-01,  7.7884e-01, -4.7009e-01,
    # ====      -9.7357e-02, -3.6644e-01, -3.1020e-01, -2.1177e-01, -3.0862e-01,
    # ====      -1.2727e-02, -5.5071e-01, -3.3797e-01,  1.5332e-01, -3.9191e-02,
    # ====       4.4467e-01, -1.6136e-01, -4.0260e-02,  1.9253e-01, -1.8932e-01,
    # ====       3.4227e-01,  2.9484e-01, -2.2381e-02, -8.5312e-02, -4.7261e-01,
    # ====      -2.4688e-01, -2.9265e-02, -1.2857e-01, -1.2010e-01, -5.5141e-01,
    # ====       1.8774e-01, -8.5120e-02, -1.1832e-02, -2.8129e-01,  1.1203e-01,
    # ====       5.3720e-01,  2.0978e-02, -4.8697e-01,  2.4877e-01,  3.0123e-01,
    # ====      -2.3920e-01,  8.8930e-02,  1.8018e-01, -1.2195e-01, -3.8816e-01,
    # ====       2.0634e-01, -6.8948e-02, -3.0773e-01,  3.7455e-01,  2.4319e-01,
    # ====       7.9170e-02, -1.4413e-01, -6.3524e-01,  3.0155e-01, -1.2498e-01,
    # ====       2.0869e-01, -8.1734e-02, -3.2209e-01,  1.0643e-01,  2.7017e-01,
    # ====      -3.5604e-01,  2.7912e-01, -1.9392e-01,  2.2909e-01, -2.7790e-01,
    # ====       9.5893e-02,  6.0638e-02,  2.6273e-01, -1.9099e-01, -4.0730e-01,
    # ====       3.1354e-01, -6.1580e-03,  1.1691e-01, -5.9344e-01,  1.0943e-01,
    # ====       1.3382e-02,  1.8860e-01, -2.6014e-01, -6.8855e-02,  3.4798e-01,
    # ====      -2.8273e-01, -2.6888e-01,  3.6216e-01,  3.1555e-01,  1.4458e-03,
    # ====      -2.6266e-01,  2.5052e-01, -3.1858e-02, -2.0559e-01,  2.6269e-01,
    # ====      -3.5579e-02, -4.4207e-01,  1.4761e-01, -3.9638e-01,  1.3879e-01,
    # ====       1.5591e-03, -4.1445e-01, -8.3563e-02, -1.8405e-02,  5.5354e-01,
    # ====      -1.5747e-01, -1.4277e-01, -2.7008e-01,  3.0383e-01,  5.9605e-02,
    # ====      -2.1890e-01,  1.5490e-01, -4.6248e-01, -6.7927e-02, -2.2992e-01,
    # ====      -4.5512e-02,  3.5417e-02,  1.2965e-01, -9.4797e-02,  3.2385e-01,
    # ====       1.0585e-01,  2.2676e-02, -4.5759e-01, -1.1433e-01, -2.2322e-01,
    # ====       4.2435e-01, -1.4394e-01, -1.8609e-01,  1.4453e-01,  3.1181e-01,
    # ====       3.8477e-01, -6.6822e-02, -3.2845e-01,  9.9376e-02,  1.7627e-01,
    # ====       1.7622e-01,  4.3003e-01, -3.2448e-01, -1.0589e-01, -1.3905e-03,
    # ====       2.2980e-01,  4.8857e-02,  3.7255e-01, -2.9642e-01, -2.2850e-01,
    # ====       1.5686e-01, -1.9804e-01, -3.4687e-01, -3.6935e-01, -5.0734e-01,
    # ====       1.2659e-01, -3.7837e-01, -3.4302e-01,  2.2192e-01,  2.7288e-01,
    # ====       5.1789e-02,  1.4855e-01,  6.4918e-03,  2.6890e-01, -6.2759e-01,
    # ====       2.6867e-02,  5.1731e-01,  1.5980e-01, -1.4625e-01, -2.5057e-01,
    # ====      -3.8850e-01,  1.2728e-01, -4.1117e-01, -2.4163e-01, -7.5919e-01,
    # ====      -1.9045e-01, -5.9689e-02, -4.6500e-02, -3.5979e-01, -1.8016e-01,
    # ====      -1.7824e-01, -1.4912e-01,  2.1047e-01,  2.6250e-01,  2.9412e-01,
    # ====       4.3531e-01,  1.2086e-01,  5.1748e-02, -1.5907e-01,  2.5995e-01,
    # ====       3.9850e-01,  1.9113e-01,  8.0188e-02,  6.5042e-02, -3.8616e-01,
    # ====       1.1499e-01,  7.5104e-02,  1.4072e-01,  1.6647e-01, -5.9247e-01,
    # ====      -2.4919e-01,  6.0248e-01,  3.4477e-01, -1.1399e-01,  2.3357e-01,
    # ====      -4.3054e-01, -8.8788e-02,  3.1526e-01, -2.3757e-01, -1.5234e-01,
    # ====      -2.0346e-01,  1.4840e-02,  5.0695e-04, -4.4320e-02, -2.8615e-01,
    # ====       3.1071e-01, -6.5392e-02,  5.9157e-01,  5.5422e-02,  4.9663e-02,
    # ====      -1.1100e-01,  3.1299e-02,  1.9786e-01,  2.6635e-01,  1.0164e-01,
    # ====      -4.0769e-01,  4.7230e-01,  2.0708e-01, -5.2004e-01, -1.3385e-01,
    # ====      -4.4136e-01, -4.1674e-01,  1.2295e-01,  2.9599e-01, -1.0969e-01,
    # ====       1.0851e-01, -8.1897e-02, -5.2952e-02,  2.0528e-01,  2.2207e-01,
    # ====       1.7611e-01,  4.8696e-02,  5.9069e-02,  3.1482e-01,  3.7416e-01,
    # ====      -4.6839e-01,  2.8974e-01,  3.1362e-01, -1.3305e-01, -3.0732e-01,
    # ====       2.7957e-01, -2.9051e-01,  1.9062e-01, -1.0565e-01,  1.2126e-01,
    # ====      -5.6594e-02,  1.6733e-01,  1.6751e-01, -3.0051e-02, -1.1953e-01,
    # ====      -2.2486e-01, -5.3992e-01, -1.1471e-01, -1.2497e-01, -2.8014e-02,
    # ====       1.4409e-01, -2.1638e-01,  1.8163e-01,  1.1842e-01,  1.8299e-01,
    # ====      -1.3933e-01, -2.6227e-01, -8.8089e-02,  1.5501e-01, -2.0001e-01,
    # ====       4.3114e-01,  4.2216e-03, -7.3895e-02, -3.2218e-02, -1.3161e-01,
    # ====       8.7186e-02,  4.8758e-01, -4.7787e-01, -2.3293e-03, -1.8843e-01,
    # ====       9.0899e-02,  5.5842e-03, -1.5472e-01, -1.5352e-01, -3.0749e-01,
    # ====      -1.3128e-01,  1.5400e-01,  3.3279e-01, -4.1167e-01,  2.3562e-02,
    # ====       4.1736e-02,  4.1760e-01,  3.9871e-01,  2.8686e-01,  1.5711e-01,
    # ====      -1.0066e-01, -1.0401e-01, -3.4003e-01,  1.5611e-01,  4.9604e-01,
    # ====       6.0076e-01,  4.7104e-03,  3.0971e-03,  1.3557e-01, -1.9529e-01,
    # ====      -4.6413e-04,  5.2717e-02,  2.8311e-02, -1.7081e-01, -4.6279e-01,
    # ====      -1.2347e-01, -1.2045e-01,  4.6546e-02,  7.9773e-02, -6.8882e-02,
    # ====      -2.0658e-01, -1.0026e-02, -5.4267e-02, -4.7557e-01, -5.9061e-01,
    # ====      -2.0970e-01, -1.4897e-01,  2.3656e-01,  3.2671e-03,  2.0515e-02,
    # ====      -1.0183e-01,  4.4150e-01, -1.3795e-01,  5.5995e-01, -2.8540e-01,
    # ====      -1.6918e-01,  5.6013e-01, -6.9926e-02,  2.3487e-01, -1.0522e-02,
    # ====       7.8855e-02, -1.7848e-01,  2.6850e-02, -3.2743e-01,  2.7247e-01,
    # ====      -3.5785e-01,  9.0117e-02,  1.6608e-02, -3.4669e-02,  1.7980e-01,
    # ====      -3.7446e-01,  5.9471e-02,  1.9760e-01,  5.6336e-01,  1.1110e-02,
    # ====       4.0582e-02,  1.2305e-01,  4.6287e-01,  6.0728e-02,  2.4244e-01,
    # ====      -1.5113e-01,  4.8183e-01,  3.3511e-01,  2.6499e-01, -2.1960e-01,
    # ====       6.2498e-02,  2.4787e-01, -4.6327e-02, -4.6746e-01,  1.7495e-01,
    # ====      -2.1039e-01,  1.8388e-01, -9.1023e-03,  6.4192e-01,  3.4280e-02,
    # ====      -2.5009e-01, -5.2623e-01, -2.2941e-01, -4.6043e-02,  3.7181e-02,
    # ====      -4.8806e-01, -3.3069e-01, -8.0986e-02, -1.1226e-01,  1.2337e-01,
    # ====       4.7090e-01, -3.4802e-01, -1.0356e-02,  1.1840e-01,  4.3500e-01,
    # ====      -3.4090e-03, -3.4131e-01, -2.0622e-01, -2.1046e-01, -2.1881e-02,
    # ====       3.4332e-01,  5.1663e-02,  1.2433e-01,  1.8753e-01,  2.1154e-01,
    # ====      -6.2950e-02,  1.6252e-02, -2.4862e-02,  2.3363e-01, -1.3406e-02,
    # ====      -4.8624e-02, -4.7759e-02, -1.6040e-01,  3.1085e-01, -4.2711e-03,
    # ====      -3.6530e-01, -6.1111e-02,  4.7598e-02, -5.2017e-01,  1.0172e-01,
    # ====      -4.6136e-01, -6.9154e-02, -6.7321e-02,  1.7360e-01,  1.7526e-01,
    # ====      -3.4261e-01, -6.0863e-01,  1.2022e-01, -3.6651e-01,  1.7278e-01,
    # ====      -1.6491e-01, -4.3071e-01, -2.5613e-01]]
    DebuggingHelper.write_line_to_system_console_out(
        f'outputs[1][0]={outputs[1][0]}')
    DebuggingHelper.write_line_to_system_console_out(
        f'outputs={outputs}')
    # ------------------------------------------------------------------------
    output_raw_cls_embedding = outputs[0][:, 0, :]
    # ---- NOTE ---- for single utterance, the above is the same as: output_raw_cls_embedding = outputs[0][0][0]
    DebuggingHelper.write_line_to_system_console_out(
        f'length(output_raw_cls_embedding)={len(output_raw_cls_embedding)}')
    DebuggingHelper.write_line_to_system_console_out(
        f'output_raw_cls_embedding={output_raw_cls_embedding}')
    # ==== output_raw_cls_embedding=tensor([[-2.2953e-01, -8.6760e-01, -1.3049e-01,  7.0627e-02,  1.5719e+00,
    # ====      -4.1814e-01, -1.8130e-01,  2.3001e-01, -3.6832e-01, -2.9287e-01,
    # ====       6.8390e-01,  9.4028e-01, -5.1265e-01,  3.3825e-01,  4.3878e-01,
    # ====      -6.3531e-01, -5.7002e-01, -3.6665e-01, -7.2602e-02, -3.1195e-01,
    # ====       1.4893e-01,  2.7354e-01, -1.9985e-01, -4.5694e-01,  3.4209e-01,
    # ====      -5.4518e-01,  8.3613e-02,  8.9714e-01,  1.6408e-01, -1.6356e-01,
    # ====       4.3922e-01,  1.2769e-01,  4.4166e-01,  3.9255e-01, -2.7356e-01,
    # ====      -3.4042e-01,  3.4515e-01,  9.0092e-01,  2.1167e-01, -6.1869e-01,
    # ====       4.5103e-01, -6.2294e-01,  2.1635e-01, -2.9099e-01,  4.7289e-01,
    # ====      -4.6438e-01, -6.1920e-01,  2.7967e-01,  2.7988e-01,  7.9668e-02,
    # ====      -7.3220e-01, -1.3592e+00, -2.5205e-01, -7.1272e-02,  3.2840e-01,
    # ====       2.6663e-02,  3.5145e-01,  6.6855e-01, -8.7702e-01, -3.5177e-01,
    # ====       1.0122e-01,  1.2090e+00, -5.3579e-01,  6.0265e-01,  4.1572e-01,
    # ====      -3.1127e-01, -2.3993e-01, -2.7573e-01, -3.8006e-01, -1.1776e-01,
    # ====      -3.4973e-02, -1.4507e+00,  2.9421e-01, -2.2439e-01,  9.3015e-01,
    # ====      -6.4782e-01, -4.2426e-01, -1.2093e+00, -4.1123e-01,  5.6787e-01,
    # ====       9.6994e-01, -7.8499e-02, -7.9094e-02, -1.4121e-01, -1.5368e-01,
    # ====      -2.4383e-03, -1.2912e-01, -9.1224e-03, -1.4498e-02,  1.9919e-01,
    # ====       6.3102e-01,  7.1355e-01, -4.5219e-01,  4.2484e-01, -8.4884e-01,
    # ====       5.6303e-01, -2.9709e-01, -9.8335e-02,  3.5200e-01, -3.7125e-01,
    # ====       5.1189e-01,  8.5332e-01,  2.2199e-01, -1.4990e-01,  6.1179e-01,
    # ====       6.9694e-01, -3.1468e-01, -6.2164e-02,  4.4499e-01,  3.7134e-01,
    # ====       3.6648e-01, -4.2214e-01, -9.2279e-02, -7.0366e-01,  4.0881e-01,
    # ====       7.9729e-01, -4.1028e-01, -2.4970e-01, -1.5655e-01,  2.2119e-01,
    # ====       1.0122e+00, -3.6576e-01, -1.1669e-01,  6.4286e-01, -1.1257e-01,
    # ====       3.1613e-01,  5.1157e-01,  6.8319e-01,  7.1066e-02,  2.7381e-01,
    # ====      -2.5654e-01, -6.8546e-01,  1.7770e-01, -2.0877e-01, -6.2847e-01,
    # ====      -5.0020e-01,  8.6962e-02, -5.7430e-01, -5.0058e-02, -5.8598e-01,
    # ====      -6.9405e-01,  5.0685e-01, -1.7404e-01,  9.6407e-01, -6.9958e-01,
    # ====       2.0752e-01,  6.9485e-01,  6.3427e-01, -6.9615e-01, -1.4770e+00,
    # ====      -2.1723e-01,  4.5171e-01, -5.3088e-01, -7.7952e-01,  3.5559e-02,
    # ====      -2.5114e-01,  3.1670e-01,  1.4811e-01, -4.0270e-01, -1.8298e-01,
    # ====      -2.3860e-02,  4.3110e-01,  9.1325e-02, -2.2308e-01, -2.1516e-01,
    # ====      -4.3047e-01,  2.2054e-01, -8.2146e-02, -6.3024e-01,  3.0281e-01,
    # ====       9.1241e-01,  6.5680e-02,  4.3308e-01, -3.4477e-01,  1.5414e-01,
    # ====       4.7870e-01, -1.1110e-01, -3.0940e-01, -1.0986e+00, -4.3471e-01,
    # ====      -5.0140e-01,  1.1502e+00, -4.1961e-01,  2.5785e-01,  1.8244e-01,
    # ====       4.9665e-02,  2.4134e-02, -5.2579e-02,  2.3437e-01, -4.6741e-02,
    # ====      -1.3894e-01,  1.6305e-01,  5.6792e-01,  6.1620e-01,  2.0009e-01,
    # ====       1.7482e-01,  4.0985e-01, -8.6657e-02,  3.2150e-01, -7.1366e-01,
    # ====      -2.5309e-01, -3.2119e-01, -7.3321e-01, -3.6545e-02,  5.2119e-01,
    # ====      -3.2616e-01,  2.5381e-01,  1.8135e-01, -2.1077e-01, -8.9504e-01,
    # ====       5.6062e-01,  4.4240e-01,  2.3921e-01, -3.0082e-01, -8.2083e-01,
    # ====      -3.0440e-01, -6.2794e-01, -5.0982e-01, -5.4538e-01, -2.3486e-01,
    # ====       8.1553e-01,  6.1983e-01, -6.1801e-01, -9.0246e-01,  2.6575e-01,
    # ====       9.5010e-01, -1.9644e-01,  6.1539e-01, -1.2382e+00, -5.4568e-01,
    # ====      -3.9367e-01, -1.3789e-01, -3.3693e-01, -6.8109e-01,  7.2747e-01,
    # ====      -7.8778e-01, -1.6502e-01,  1.9066e-01,  9.2523e-01,  3.8349e-01,
    # ====      -4.0785e-01,  2.8958e-01,  4.8273e-01,  7.1619e-01,  1.3545e+00,
    # ====      -2.5772e-02,  5.1138e-01, -5.5553e-01, -2.7793e-01,  6.8891e-01,
    # ====       8.5027e-01, -4.9946e-01, -2.8922e-01,  4.9907e-03, -6.8728e-01,
    # ====      -8.4059e-02,  2.4997e-01,  6.9423e-01,  5.3837e-01,  1.0323e+00,
    # ====      -2.3652e-01, -1.6422e-01,  4.6097e-02,  7.1231e-01,  7.0829e-01,
    # ====      -2.6971e-01,  3.5881e-01, -3.5143e-01,  8.0121e-01, -4.9715e-01,
    # ====      -3.6787e-01,  8.2318e-02, -1.2707e-01,  7.1057e-01,  1.0473e-02,
    # ====       3.7413e-01, -3.2358e-01,  4.7024e-01, -7.5415e-01,  5.2866e-01,
    # ====       4.1064e-01, -5.5260e-01,  7.2641e-02,  8.6484e-01, -1.1028e+00,
    # ====      -4.1383e-01,  5.1138e-01,  1.5166e-01, -1.3555e-01,  4.3559e-01,
    # ====       5.7033e-02,  4.0379e-01,  9.9668e-01, -1.6978e-01, -7.5852e-01,
    # ====      -4.6958e-02, -9.0424e-01,  2.8138e-01, -7.7636e-01,  7.5649e-02,
    # ====       8.8112e-02,  5.1189e-01, -6.3547e-02,  1.1034e+00, -1.2319e-01,
    # ====       3.6806e-01,  4.4567e-01,  7.2561e-01, -6.8182e-02,  2.5854e-01,
    # ====      -9.7536e-01, -3.0658e-01, -4.6387e-01, -6.0338e-01, -2.5698e-01,
    # ====      -7.6732e-01,  2.2901e-01,  1.0069e-03,  1.5012e-01, -4.4019e-02,
    # ====      -5.5995e-01, -8.5648e-01,  2.1225e-01, -7.3009e-01, -1.0668e-01,
    # ====       2.5335e-01,  4.3914e-02,  3.5417e-01, -7.4031e-01,  2.4670e-01,
    # ====      -3.6075e-02, -2.1014e-01, -9.2342e-01,  2.7557e-01, -1.4204e-01,
    # ====      -3.0165e-01,  2.5293e-01,  7.1008e-01, -2.4551e-01, -2.9577e-01,
    # ====       4.0115e-01,  1.0585e+00,  7.7638e-02, -7.6701e-02,  1.1228e-01,
    # ====      -2.8153e-01,  4.5497e-01, -6.6325e-01, -3.0979e-01,  2.8731e-02,
    # ====      -8.3592e-01, -3.0698e-01, -6.7801e-01,  5.6885e-01,  3.3813e-01,
    # ====      -1.8087e-01, -6.5308e-01,  1.0028e+00, -7.8068e-01, -2.2339e-01,
    # ====      -1.2512e+00,  6.4227e-01,  1.0094e+00, -3.4460e-01,  1.2577e+00,
    # ====       3.0521e-01,  3.1242e-01,  1.0342e-01, -1.1024e+00,  7.0224e-01,
    # ====       1.3149e-01, -3.8529e-01,  1.6033e-01,  2.7528e-01,  6.8722e-03,
    # ====       4.5798e-01, -7.6508e-01, -1.4631e-01,  1.0218e-01, -1.9360e-01,
    # ====       1.7714e-01,  9.6633e-02,  3.9704e-01,  4.3766e-01, -1.9633e-01,
    # ====      -2.9393e-01,  3.8651e-01,  5.0546e-01,  6.4958e-01, -1.3981e-01,
    # ====      -3.1557e-01, -9.0167e-01, -1.2136e+00, -3.1435e-01,  2.3004e-01,
    # ====      -7.9397e-01, -4.8289e-01,  1.4971e-01, -1.5661e-01, -1.2209e-01,
    # ====      -4.8225e-01,  8.4445e-01, -2.1028e-02, -3.3423e-01, -2.9407e-02,
    # ====       3.9345e-01, -1.2975e+00, -2.2230e-01, -1.2278e-01, -4.9076e-01,
    # ====      -2.3392e-01,  4.3872e-01,  5.2702e-01, -8.9103e-01,  6.7218e-01,
    # ====       9.5914e-01,  1.4646e+00, -7.9985e-01, -2.1620e-01, -7.1701e-03,
    # ====      -1.9765e-01,  8.0427e-03,  3.8204e-01,  3.4201e-01,  4.0925e-01,
    # ====       1.5644e+00,  9.7818e-02, -6.1878e-01, -2.6290e-01,  3.8272e-01,
    # ====       3.0639e-01,  1.1895e+00, -1.2016e-01,  1.4507e+00,  7.1300e-02,
    # ====      -7.1535e-01,  2.3382e-01,  3.6722e-01, -1.5823e-01, -1.8911e-01,
    # ====       1.1031e-01, -7.6146e-01, -2.6592e-01, -9.1633e-01, -2.2831e-01,
    # ====       2.5828e-01, -3.7441e-01,  1.0928e-01, -7.0773e-01,  1.2323e-01,
    # ====      -4.0083e-01, -2.6965e-01,  5.2345e-03, -1.6798e+00,  3.7390e-01,
    # ====      -2.7422e-02,  1.6628e-01, -9.6804e-02, -2.4569e-01,  6.3234e-01,
    # ====      -4.5364e-02, -1.7463e-01, -3.0381e-01,  5.7870e-02,  6.6258e-02,
    # ====      -6.1522e-01,  1.6253e+00, -8.5183e-01,  1.8421e-01,  9.2061e-01,
    # ====      -1.9038e-01,  1.6766e-01,  1.5719e-01,  8.3064e-01, -9.0677e-01,
    # ====       1.9351e-01, -6.8531e-02, -1.1838e-01, -6.9683e-01,  3.0156e-02,
    # ====       7.5686e-01, -1.8245e-01,  1.2828e+00,  1.0916e+00,  8.4193e-01,
    # ====      -2.2576e-01, -1.7079e-01,  1.1124e+00,  6.7811e-01, -3.5217e-01,
    # ====      -3.2904e-01, -3.2054e-01, -2.7681e-01,  1.9604e-01, -1.3298e-01,
    # ====       1.8102e-01, -2.8041e-02, -1.7534e-01, -2.3138e-01,  3.5186e-01,
    # ====      -1.3179e-02, -6.4613e-01,  5.6201e-02, -3.5716e-01, -1.1904e-01,
    # ====      -1.1320e+00,  1.5355e-01, -4.1323e-01,  1.3839e-01, -6.3496e-01,
    # ====       2.5123e-01, -4.6122e-01,  6.2725e-02,  3.7547e-01,  2.1205e-01,
    # ====      -2.3409e-01, -2.2077e-02, -6.4720e-02,  2.1202e-01,  5.1035e-01,
    # ====      -8.6036e-01, -9.5883e-01, -8.9149e-01,  2.9647e-01, -2.0549e-01,
    # ====       6.7078e-02, -4.7877e-02, -1.6631e-01, -2.4674e-01, -6.1233e-01,
    # ====      -3.6139e-01,  3.5207e-01, -4.8000e-01,  1.9030e-01,  2.2862e-01,
    # ====      -3.1750e-01, -2.7904e-01,  2.6620e-02,  6.8168e-01, -4.3245e-01,
    # ====       6.0179e-01, -8.6882e-02,  5.1420e-01,  6.6846e-01, -4.8942e-01,
    # ====       4.3511e-01,  8.1610e-01, -8.2472e-02, -1.1015e-01,  5.1431e-01,
    # ====      -2.7198e-01, -7.0739e-01, -9.3136e-02,  2.3598e-01,  7.1992e-02,
    # ====       1.1988e+00,  4.2225e-01,  3.7996e-01,  5.9742e-01,  2.6951e-01,
    # ====      -5.6812e-02, -3.2226e-01,  8.4166e-01, -1.0874e+00, -8.5332e-01,
    # ====       1.5286e+00,  1.4312e-01, -4.0836e-01, -1.0307e+00, -4.2534e-01,
    # ====      -3.9668e-01,  5.3186e-01, -6.9091e-02,  4.5460e-01,  5.2273e-01,
    # ====      -6.7396e-01,  1.2093e-01, -1.7408e-01, -7.7528e-01, -1.0028e-01,
    # ====       7.5790e-01,  6.7572e-01, -8.1781e-02, -5.1498e-01,  6.4830e-01,
    # ====       5.8146e-01,  4.5339e-02,  6.7972e-02,  1.7040e+00, -4.1715e-01,
    # ====       1.0898e+00,  7.2827e-02,  2.5151e-01, -6.5060e-01, -3.9874e-01,
    # ====       3.6537e-01,  5.9199e-01, -3.1409e-01,  9.2529e-02,  4.4464e-01,
    # ====       6.9215e-01, -1.8158e-02, -3.3650e-02, -1.6194e-01, -4.2368e-01,
    # ====      -4.7358e-01, -6.3526e-01,  6.1680e-01, -8.2417e-02,  2.8897e-01,
    # ====       2.5602e-01, -3.6649e-01,  6.4816e-01,  7.4561e-01,  3.0648e-01,
    # ====      -1.0590e+00, -3.7811e-01,  4.2708e-02,  2.4554e-01, -2.0004e-01,
    # ====      -9.6091e-02,  5.4886e-01,  9.1263e-01,  6.3757e-01, -9.6766e-02,
    # ====      -4.8184e-01, -6.5513e-01,  2.4212e-01, -1.9056e-01,  1.6843e-01,
    # ====       1.1000e+00, -5.9101e-01,  2.9173e-01, -9.7736e-01, -4.3031e-02,
    # ====      -5.0432e-01,  7.8905e-01, -6.3812e-01,  1.5468e-01,  4.0650e-02,
    # ====       8.1161e-01, -4.0711e-01, -3.6903e-01, -2.7736e-01,  2.6063e-01,
    # ====       4.0480e-02, -2.7442e-01, -2.5354e-01, -3.5137e-01, -1.6969e-01,
    # ====      -5.9199e-01, -3.2228e-01,  5.3613e-02, -4.2127e-01,  1.3796e-01,
    # ====      -3.8280e-01, -1.1346e+00,  3.3629e-02,  8.3283e-01,  6.0415e-02,
    # ====       1.0422e+00,  6.0512e-02,  3.6505e-01, -3.4007e-02,  3.6446e-01,
    # ====      -5.6517e-01,  1.8085e-01, -1.1663e+00, -8.8643e-01,  5.7756e-01,
    # ====       6.1341e-01, -3.9390e-01, -2.7217e-01, -5.1765e-01, -6.7767e-01,
    # ====      -9.7806e-02,  2.8944e-01, -2.1863e-01, -7.6392e-01,  3.4577e-01,
    # ====       8.3321e-01,  1.0017e+00,  5.0588e-01, -1.3383e-01,  2.9811e-01,
    # ====      -3.4087e-01, -1.1054e+00,  2.1275e-01, -4.1603e-01, -3.6250e-01,
    # ====      -4.8938e-01,  1.7790e-01,  4.1119e-01, -1.0893e-01,  3.1788e-01,
    # ====       3.6394e-01, -2.6408e-01, -4.4063e-01, -1.6084e-01, -8.0249e-01,
    # ====       5.5410e-01,  1.7320e-01,  3.6055e-01,  1.1530e-01, -4.5572e-02,
    # ====      -5.4730e-01,  1.0238e-01,  5.4579e-02,  8.0333e-01, -1.4088e-01,
    # ====       2.9023e-01,  2.2133e-01, -4.2637e-01,  3.6794e-02,  7.6550e-01,
    # ====       2.6639e-01, -5.5603e-01, -4.7308e-01, -3.4949e-01, -3.3157e-01,
    # ====      -4.8555e-01, -3.8333e-02,  6.7319e-01,  5.6911e-01,  4.3587e-01,
    # ====      -1.4045e-01, -1.4260e-02,  9.8556e-01,  5.2827e-01, -3.9959e-01,
    # ====       7.1524e-01, -2.8006e-01, -1.2251e+00, -1.4394e-02,  8.7717e-01,
    # ====       5.6043e-01, -6.4992e-01, -4.7382e-01, -5.8564e-02, -1.1188e+00,
    # ====      -6.2022e-01, -6.3917e-01,  2.8031e-01, -8.7641e-01, -1.5188e-01,
    # ====       3.7945e-02,  9.2608e-01, -4.5989e-01,  2.7226e-01, -9.0524e-02,
    # ====       1.0967e-01,  4.1305e-02, -6.0938e-01, -2.9479e-01, -3.5042e-01,
    # ====      -2.6737e-01,  9.4940e-01, -2.4925e-01,  9.2110e-01, -9.5887e-02,
    # ====      -9.6115e-01,  2.3952e-02,  6.6885e-01,  1.1559e+00,  2.5682e-01,
    # ====       7.5284e-01,  1.8328e-01,  1.2768e-01]]
    output_raw_cls_embedding_divided_by_norm = divided_by_norm(output_raw_cls_embedding)
    DebuggingHelper.write_line_to_system_console_out(
        f'length(output_raw_cls_embedding_divided_by_norm)={len(output_raw_cls_embedding_divided_by_norm)}')
    DebuggingHelper.write_line_to_system_console_out(
        f'output_raw_cls_embedding_divided_by_norm={output_raw_cls_embedding_divided_by_norm}')
    # ==== output_raw_cls_embedding_divided_by_norm=tensor([[-1.5409e-02, -5.8243e-02, -8.7598e-03,  4.7413e-03,  1.0553e-01,
    # ====      -2.8070e-02, -1.2171e-02,  1.5441e-02, -2.4726e-02, -1.9661e-02,
    # ====       4.5911e-02,  6.3123e-02, -3.4415e-02,  2.2707e-02,  2.9456e-02,
    # ====      -4.2650e-02, -3.8266e-02, -2.4614e-02, -4.8739e-03, -2.0942e-02,
    # ====       9.9977e-03,  1.8363e-02, -1.3416e-02, -3.0675e-02,  2.2965e-02,
    # ====      -3.6599e-02,  5.6131e-03,  6.0226e-02,  1.1015e-02, -1.0980e-02,
    # ====       2.9486e-02,  8.5722e-03,  2.9649e-02,  2.6353e-02, -1.8365e-02,
    # ====      -2.2853e-02,  2.3170e-02,  6.0481e-02,  1.4210e-02, -4.1534e-02,
    # ====       3.0278e-02, -4.1819e-02,  1.4524e-02, -1.9534e-02,  3.1746e-02,
    # ====      -3.1175e-02, -4.1568e-02,  1.8775e-02,  1.8789e-02,  5.3482e-03,
    # ====      -4.9154e-02, -9.1245e-02, -1.6921e-02, -4.7846e-03,  2.2046e-02,
    # ====       1.7900e-03,  2.3593e-02,  4.4881e-02, -5.8876e-02, -2.3615e-02,
    # ====       6.7954e-03,  8.1165e-02, -3.5968e-02,  4.0457e-02,  2.7908e-02,
    # ====      -2.0896e-02, -1.6107e-02, -1.8510e-02, -2.5514e-02, -7.9057e-03,
    # ====      -2.3478e-03, -9.7390e-02,  1.9751e-02, -1.5064e-02,  6.2442e-02,
    # ====      -4.3489e-02, -2.8481e-02, -8.1181e-02, -2.7606e-02,  3.8122e-02,
    # ====       6.5114e-02, -5.2698e-03, -5.3097e-03, -9.4794e-03, -1.0317e-02,
    # ====      -1.6368e-04, -8.6681e-03, -6.1240e-04, -9.7326e-04,  1.3372e-02,
    # ====       4.2361e-02,  4.7902e-02, -3.0356e-02,  2.8520e-02, -5.6984e-02,
    # ====       3.7797e-02, -1.9944e-02, -6.6014e-03,  2.3631e-02, -2.4923e-02,
    # ====       3.4364e-02,  5.7285e-02,  1.4903e-02, -1.0063e-02,  4.1070e-02,
    # ====       4.6786e-02, -2.1125e-02, -4.1731e-03,  2.9873e-02,  2.4929e-02,
    # ====       2.4603e-02, -2.8339e-02, -6.1948e-03, -4.7238e-02,  2.7444e-02,
    # ====       5.3524e-02, -2.7543e-02, -1.6763e-02, -1.0510e-02,  1.4849e-02,
    # ====       6.7953e-02, -2.4554e-02, -7.8337e-03,  4.3156e-02, -7.5569e-03,
    # ====       2.1223e-02,  3.4342e-02,  4.5864e-02,  4.7708e-03,  1.8382e-02,
    # ====      -1.7222e-02, -4.6016e-02,  1.1930e-02, -1.4015e-02, -4.2190e-02,
    # ====      -3.3579e-02,  5.8379e-03, -3.8554e-02, -3.3605e-03, -3.9338e-02,
    # ====      -4.6592e-02,  3.4025e-02, -1.1684e-02,  6.4719e-02, -4.6964e-02,
    # ====       1.3932e-02,  4.6646e-02,  4.2580e-02, -4.6734e-02, -9.9152e-02,
    # ====      -1.4583e-02,  3.0324e-02, -3.5639e-02, -5.2330e-02,  2.3871e-03,
    # ====      -1.6860e-02,  2.1261e-02,  9.9427e-03, -2.7034e-02, -1.2284e-02,
    # ====      -1.6018e-03,  2.8940e-02,  6.1308e-03, -1.4976e-02, -1.4444e-02,
    # ====      -2.8898e-02,  1.4805e-02, -5.5146e-03, -4.2309e-02,  2.0328e-02,
    # ====       6.1252e-02,  4.4092e-03,  2.9073e-02, -2.3145e-02,  1.0348e-02,
    # ====       3.2136e-02, -7.4581e-03, -2.0771e-02, -7.3753e-02, -2.9183e-02,
    # ====      -3.3660e-02,  7.7212e-02, -2.8169e-02,  1.7310e-02,  1.2248e-02,
    # ====       3.3341e-03,  1.6202e-03, -3.5297e-03,  1.5734e-02, -3.1378e-03,
    # ====      -9.3271e-03,  1.0946e-02,  3.8125e-02,  4.1366e-02,  1.3433e-02,
    # ====       1.1736e-02,  2.7514e-02, -5.8175e-03,  2.1583e-02, -4.7909e-02,
    # ====      -1.6990e-02, -2.1562e-02, -4.9222e-02, -2.4533e-03,  3.4989e-02,
    # ====      -2.1896e-02,  1.7039e-02,  1.2174e-02, -1.4150e-02, -6.0086e-02,
    # ====       3.7636e-02,  2.9699e-02,  1.6058e-02, -2.0194e-02, -5.5104e-02,
    # ====      -2.0435e-02, -4.2155e-02, -3.4225e-02, -3.6612e-02, -1.5767e-02,
    # ====       5.4748e-02,  4.1610e-02, -4.1488e-02, -6.0584e-02,  1.7840e-02,
    # ====       6.3782e-02, -1.3187e-02,  4.1312e-02, -8.3119e-02, -3.6633e-02,
    # ====      -2.6428e-02, -9.2569e-03, -2.2619e-02, -4.5723e-02,  4.8836e-02,
    # ====      -5.2885e-02, -1.1078e-02,  1.2799e-02,  6.2112e-02,  2.5744e-02,
    # ====      -2.7379e-02,  1.9440e-02,  3.2407e-02,  4.8079e-02,  9.0931e-02,
    # ====      -1.7301e-03,  3.4330e-02, -3.7294e-02, -1.8658e-02,  4.6248e-02,
    # ====       5.7080e-02, -3.3530e-02, -1.9416e-02,  3.3503e-04, -4.6138e-02,
    # ====      -5.6430e-03,  1.6781e-02,  4.6605e-02,  3.6142e-02,  6.9299e-02,
    # ====      -1.5878e-02, -1.1024e-02,  3.0945e-03,  4.7819e-02,  4.7549e-02,
    # ====      -1.8106e-02,  2.4088e-02, -2.3592e-02,  5.3787e-02, -3.3374e-02,
    # ====      -2.4696e-02,  5.5262e-03, -8.5304e-03,  4.7701e-02,  7.0305e-04,
    # ====       2.5116e-02, -2.1723e-02,  3.1568e-02, -5.0627e-02,  3.5490e-02,
    # ====       2.7567e-02, -3.7097e-02,  4.8765e-03,  5.8058e-02, -7.4036e-02,
    # ====      -2.7781e-02,  3.4330e-02,  1.0181e-02, -9.1000e-03,  2.9242e-02,
    # ====       3.8287e-03,  2.7107e-02,  6.6909e-02, -1.1398e-02, -5.0921e-02,
    # ====      -3.1524e-03, -6.0703e-02,  1.8890e-02, -5.2118e-02,  5.0784e-03,
    # ====       5.9151e-03,  3.4364e-02, -4.2660e-03,  7.4076e-02, -8.2699e-03,
    # ====       2.4708e-02,  2.9919e-02,  4.8711e-02, -4.5771e-03,  1.7356e-02,
    # ====      -6.5478e-02, -2.0581e-02, -3.1141e-02, -4.0506e-02, -1.7251e-02,
    # ====      -5.1511e-02,  1.5374e-02,  6.7596e-05,  1.0078e-02, -2.9550e-03,
    # ====      -3.7590e-02, -5.7497e-02,  1.4249e-02, -4.9012e-02, -7.1614e-03,
    # ====       1.7008e-02,  2.9480e-03,  2.3776e-02, -4.9698e-02,  1.6561e-02,
    # ====      -2.4217e-03, -1.4107e-02, -6.1991e-02,  1.8499e-02, -9.5356e-03,
    # ====      -2.0250e-02,  1.6980e-02,  4.7669e-02, -1.6482e-02, -1.9856e-02,
    # ====       2.6930e-02,  7.1060e-02,  5.2120e-03, -5.1490e-03,  7.5378e-03,
    # ====      -1.8899e-02,  3.0543e-02, -4.4525e-02, -2.0797e-02,  1.9287e-03,
    # ====      -5.6117e-02, -2.0608e-02, -4.5516e-02,  3.8188e-02,  2.2699e-02,
    # ====      -1.2142e-02, -4.3842e-02,  6.7317e-02, -5.2408e-02, -1.4996e-02,
    # ====      -8.3994e-02,  4.3116e-02,  6.7763e-02, -2.3133e-02,  8.4429e-02,
    # ====       2.0489e-02,  2.0973e-02,  6.9427e-03, -7.4009e-02,  4.7143e-02,
    # ====       8.8268e-03, -2.5865e-02,  1.0763e-02,  1.8480e-02,  4.6134e-04,
    # ====       3.0745e-02, -5.1361e-02, -9.8222e-03,  6.8594e-03, -1.2996e-02,
    # ====       1.1892e-02,  6.4871e-03,  2.6654e-02,  2.9381e-02, -1.3180e-02,
    # ====      -1.9732e-02,  2.5947e-02,  3.3932e-02,  4.3608e-02, -9.3856e-03,
    # ====      -2.1185e-02, -6.0531e-02, -8.1474e-02, -2.1103e-02,  1.5443e-02,
    # ====      -5.3301e-02, -3.2417e-02,  1.0050e-02, -1.0514e-02, -8.1963e-03,
    # ====      -3.2374e-02,  5.6689e-02, -1.4116e-03, -2.2437e-02, -1.9741e-03,
    # ====       2.6413e-02, -8.7104e-02, -1.4923e-02, -8.2422e-03, -3.2946e-02,
    # ====      -1.5703e-02,  2.9452e-02,  3.5380e-02, -5.9816e-02,  4.5125e-02,
    # ====       6.4389e-02,  9.8321e-02, -5.3695e-02, -1.4514e-02, -4.8134e-04,
    # ====      -1.3269e-02,  5.3992e-04,  2.5647e-02,  2.2960e-02,  2.7474e-02,
    # ====       1.0502e-01,  6.5667e-03, -4.1540e-02, -1.7649e-02,  2.5693e-02,
    # ====       2.0568e-02,  7.9855e-02, -8.0662e-03,  9.7387e-02,  4.7865e-03,
    # ====      -4.8023e-02,  1.5697e-02,  2.4652e-02, -1.0622e-02, -1.2695e-02,
    # ====       7.4052e-03, -5.1118e-02, -1.7852e-02, -6.1515e-02, -1.5327e-02,
    # ====       1.7339e-02, -2.5135e-02,  7.3362e-03, -4.7511e-02,  8.2727e-03,
    # ====      -2.6909e-02, -1.8102e-02,  3.5140e-04, -1.1277e-01,  2.5100e-02,
    # ====      -1.8409e-03,  1.1162e-02, -6.4986e-03, -1.6493e-02,  4.2450e-02,
    # ====      -3.0453e-03, -1.1723e-02, -2.0395e-02,  3.8849e-03,  4.4480e-03,
    # ====      -4.1301e-02,  1.0911e-01, -5.7185e-02,  1.2366e-02,  6.1802e-02,
    # ====      -1.2781e-02,  1.1255e-02,  1.0553e-02,  5.5762e-02, -6.0873e-02,
    # ====       1.2990e-02, -4.6006e-03, -7.9469e-03, -4.6779e-02,  2.0244e-03,
    # ====       5.0809e-02, -1.2248e-02,  8.6114e-02,  7.3282e-02,  5.6520e-02,
    # ====      -1.5155e-02, -1.1465e-02,  7.4677e-02,  4.5523e-02, -2.3642e-02,
    # ====      -2.2089e-02, -2.1518e-02, -1.8582e-02,  1.3161e-02, -8.9273e-03,
    # ====       1.2152e-02, -1.8824e-03, -1.1771e-02, -1.5533e-02,  2.3621e-02,
    # ====      -8.8476e-04, -4.3376e-02,  3.7728e-03, -2.3977e-02, -7.9914e-03,
    # ====      -7.5993e-02,  1.0308e-02, -2.7741e-02,  9.2901e-03, -4.2626e-02,
    # ====       1.6866e-02, -3.0963e-02,  4.2108e-03,  2.5206e-02,  1.4235e-02,
    # ====      -1.5715e-02, -1.4821e-03, -4.3447e-03,  1.4234e-02,  3.4261e-02,
    # ====      -5.7757e-02, -6.4368e-02, -5.9847e-02,  1.9902e-02, -1.3795e-02,
    # ====       4.5031e-03, -3.2141e-03, -1.1165e-02, -1.6564e-02, -4.1107e-02,
    # ====      -2.4261e-02,  2.3635e-02, -3.2223e-02,  1.2775e-02,  1.5348e-02,
    # ====      -2.1314e-02, -1.8732e-02,  1.7871e-03,  4.5763e-02, -2.9031e-02,
    # ====       4.0399e-02, -5.8326e-03,  3.4519e-02,  4.4875e-02, -3.2856e-02,
    # ====       2.9210e-02,  5.4786e-02, -5.5365e-03, -7.3945e-03,  3.4527e-02,
    # ====      -1.8259e-02, -4.7488e-02, -6.2524e-03,  1.5842e-02,  4.8329e-03,
    # ====       8.0478e-02,  2.8346e-02,  2.5508e-02,  4.0106e-02,  1.8093e-02,
    # ====      -3.8139e-03, -2.1634e-02,  5.6502e-02, -7.2997e-02, -5.7285e-02,
    # ====       1.0262e-01,  9.6076e-03, -2.7414e-02, -6.9194e-02, -2.8554e-02,
    # ====      -2.6630e-02,  3.5705e-02, -4.6382e-03,  3.0518e-02,  3.5092e-02,
    # ====      -4.5244e-02,  8.1180e-03, -1.1686e-02, -5.2046e-02, -6.7317e-03,
    # ====       5.0879e-02,  4.5362e-02, -5.4901e-03, -3.4571e-02,  4.3521e-02,
    # ====       3.9034e-02,  3.0437e-03,  4.5631e-03,  1.1439e-01, -2.8004e-02,
    # ====       7.3162e-02,  4.8890e-03,  1.6884e-02, -4.3676e-02, -2.6768e-02,
    # ====       2.4528e-02,  3.9741e-02, -2.1086e-02,  6.2116e-03,  2.9849e-02,
    # ====       4.6465e-02, -1.2190e-03, -2.2590e-03, -1.0871e-02, -2.8442e-02,
    # ====      -3.1792e-02, -4.2646e-02,  4.1407e-02, -5.5328e-03,  1.9399e-02,
    # ====       1.7187e-02, -2.4603e-02,  4.3512e-02,  5.0054e-02,  2.0574e-02,
    # ====      -7.1093e-02, -2.5383e-02,  2.8671e-03,  1.6484e-02, -1.3429e-02,
    # ====      -6.4507e-03,  3.6846e-02,  6.1266e-02,  4.2801e-02, -6.4961e-03,
    # ====      -3.2347e-02, -4.3980e-02,  1.6254e-02, -1.2793e-02,  1.1307e-02,
    # ====       7.3842e-02, -3.9676e-02,  1.9584e-02, -6.5612e-02, -2.8888e-03,
    # ====      -3.3856e-02,  5.2971e-02, -4.2838e-02,  1.0384e-02,  2.7289e-03,
    # ====       5.4485e-02, -2.7330e-02, -2.4773e-02, -1.8620e-02,  1.7496e-02,
    # ====       2.7175e-03, -1.8422e-02, -1.7021e-02, -2.3588e-02, -1.1392e-02,
    # ====      -3.9741e-02, -2.1635e-02,  3.5991e-03, -2.8280e-02,  9.2612e-03,
    # ====      -2.5698e-02, -7.6170e-02,  2.2575e-03,  5.5909e-02,  4.0558e-03,
    # ====       6.9963e-02,  4.0623e-03,  2.4506e-02, -2.2829e-03,  2.4467e-02,
    # ====      -3.7941e-02,  1.2141e-02, -7.8296e-02, -5.9508e-02,  3.8773e-02,
    # ====       4.1179e-02, -2.6443e-02, -1.8272e-02, -3.4751e-02, -4.5493e-02,
    # ====      -6.5659e-03,  1.9430e-02, -1.4677e-02, -5.1283e-02,  2.3212e-02,
    # ====       5.5935e-02,  6.7244e-02,  3.3961e-02, -8.9842e-03,  2.0012e-02,
    # ====      -2.2883e-02, -7.4210e-02,  1.4283e-02, -2.7929e-02, -2.4335e-02,
    # ====      -3.2853e-02,  1.1943e-02,  2.7604e-02, -7.3128e-03,  2.1340e-02,
    # ====       2.4432e-02, -1.7728e-02, -2.9580e-02, -1.0797e-02, -5.3872e-02,
    # ====       3.7198e-02,  1.1627e-02,  2.4205e-02,  7.7405e-03, -3.0593e-03,
    # ====      -3.6741e-02,  6.8726e-03,  3.6640e-03,  5.3929e-02, -9.4573e-03,
    # ====       1.9484e-02,  1.4859e-02, -2.8623e-02,  2.4700e-03,  5.1390e-02,
    # ====       1.7883e-02, -3.7327e-02, -3.1759e-02, -2.3462e-02, -2.2259e-02,
    # ====      -3.2596e-02, -2.5733e-03,  4.5192e-02,  3.8206e-02,  2.9261e-02,
    # ====      -9.4286e-03, -9.5729e-04,  6.6162e-02,  3.5463e-02, -2.6825e-02,
    # ====       4.8015e-02, -1.8801e-02, -8.2243e-02, -9.6631e-04,  5.8886e-02,
    # ====       3.7622e-02, -4.3630e-02, -3.1808e-02, -3.9315e-03, -7.5105e-02,
    # ====      -4.1636e-02, -4.2909e-02,  1.8818e-02, -5.8835e-02, -1.0196e-02,
    # ====       2.5473e-03,  6.2170e-02, -3.0873e-02,  1.8277e-02, -6.0771e-03,
    # ====       7.3622e-03,  2.7729e-03, -4.0909e-02, -1.9789e-02, -2.3524e-02,
    # ====      -1.7949e-02,  6.3735e-02, -1.6732e-02,  6.1835e-02, -6.4371e-03,
    # ====      -6.4524e-02,  1.6079e-03,  4.4901e-02,  7.7600e-02,  1.7241e-02,
    # ====       5.0539e-02,  1.2304e-02,  8.5712e-03]]
    # ------------------------------------------------------------------------
    DebuggingHelper.write_line_to_system_console_out(
        f'DTE model={model}',
        utf8_conversion=False)
    # ------------------------------------------------------------------------

def main_test_tokenizer_roberta_base_new():
    """
    The main_test_tokenizer_roberta_base_new() function can quickly test
    PytorchLanguageUnderstandingTransformersPretainedModelHelper functions.
    """
    # ------------------------------------------------------------------------
    # ---- NOTE-PYLINT ---- R0915: Too many statements
    # pylint: disable=R0915
    # ---- NOTE-PYLINT ---- C0301: Line too long
    # pylint: disable=C0301
    parser = argparse.ArgumentParser()
    parser.add_argument(
        '--pytorch_transformers_vocab_file',
        default='vocab.json',
        type=str,
        required=False,
        help=f'pytorch transformers vocab file')
    parser.add_argument(
        '--pytorch_transformers_merges_file',
        default='merges.txt',
        type=str,
        required=False,
        help=f'pytorch transformers merges file')
    parser.add_argument(
        '--pytorch_transformers_model_dir_tokenizer',
        default='',
        type=str,
        required=False,
        help='Pytorch Transformers model tokenizer directory.')
    parser.add_argument(
        '--pytorch_transformers_model_dir_learner',
        default='',
        type=str,
        required=False,
        help='Pytorch Transformers model learner directory.')
    args: argparse.Namespace = parser.parse_args()
    # ------------------------------------------------------------------------
    pytorch_transformers_model_dir_tokenizer: str = \
        args.pytorch_transformers_model_dir_tokenizer
    pytorch_transformers_model_dir_tokenizer = \
        r"/model_dte_proprietary/collected_DTE_pytorch/dte_tnlr-v3_12l_bing_cortana_mixture/tokenizer"
    vocab_file: str = \
        os.path.join(pytorch_transformers_model_dir_tokenizer, args.pytorch_transformers_vocab_file)
    merges_file: str = \
        os.path.join(pytorch_transformers_model_dir_tokenizer, args.pytorch_transformers_merges_file)
    tokenizer = PytorchLanguageUnderstandingTransformersPretainedModelHelper.roberta_tokenizer_new( \
        vocab_file, \
        merges_file)
    DebuggingHelper.write_line_to_system_console_out(
        f'tokenizer={str(tokenizer)}')
    DebuggingHelper.write_line_to_system_console_out(
        f'tokenizer.vocab_size={tokenizer.vocab_size}')
    # ---- NOTE-OUTPUT ---- b'tokenizer.vocab_size=50265'
    # ------------------------------------------------------------------------
    utterance_road_not_taken: str = \
        'Two roads diverged in a wood, and I -- I took the one less traveled by, And that has made all the difference.'
    utterance_road_not_taken = ' ' + utterance_road_not_taken
    # ------------------------------------------------------------------------
    utterance: str = \
        utterance_road_not_taken
    output_tokens: List[str] = \
        tokenizer.tokenize(utterance)
    DebuggingHelper.write_line_to_system_console_out(
        f'output_tokens-A={output_tokens}')
    DebuggingHelper.write_line_to_system_console_out(
        f'output_tokens-A={len(output_tokens)}')
    token_id_lists: List[List[int]] = \
        PytorchLanguageUnderstandingTransformersPretainedModelHelper.convert_token_lists_to_ids(output_tokens, tokenizer)
    DebuggingHelper.write_line_to_system_console_out(
        f'output_token_ids-A={token_id_lists}')
    # ---- NOTE-PYLINT ---- C0301: Line too long
    # pylint: disable=C0301
    # ---- NOTE-OUTPUT ---- b"output_tokens-A=['\xc4\xa0Two', '\xc4\xa0roads', '\xc4\xa0diver', 'ged', '\xc4\xa0in', '\xc4\xa0a', '\xc4\xa0wood', ',', '\xc4\xa0and', '\xc4\xa0I', '\xc4\xa0--', '\xc4\xa0I', '\xc4\xa0took', '\xc4\xa0the', '\xc4\xa0one', '\xc4\xa0less', '\xc4\xa0traveled', '\xc4\xa0by', ',', '\xc4\xa0And', '\xc4\xa0that', '\xc4\xa0has', '\xc4\xa0made', '\xc4\xa0all', '\xc4\xa0the', '\xc4\xa0difference', '.']"
    # ---- NOTE-OUTPUT ---- b'output_tokens-A=27'
    # ---- NOTE-OUTPUT ---- b'output_token_ids-A=[1596, 3197, 13105, 4462, 11, 10, 5627, 6, 8, 38, 480, 38, 362, 5, 65, 540, 8468, 30, 6, 178, 14, 34, 156, 70, 5, 2249, 4]'
    utterance: str = \
        utterance_road_not_taken
    # ==== output_tokens: List[str] = \
    # ====     tokenizer.tokenize(utterance)
    # DebuggingHelper.write_line_to_system_console_out(
    #     f'output_tokens-B={output_tokens}')
    # ==== DebuggingHelper.write_line_to_system_console_out(
    # ====     f'output_tokens-B={len(output_tokens)}')
    token_id_lists: List[int] = \
        tokenizer.encode(utterance)
    DebuggingHelper.write_line_to_system_console_out(
        f'output_token_ids-B={token_id_lists}')
    # ---- NOTE-PYLINT ---- C0301: Line too long
    # pylint: disable=C0301
    # ---- NOTE-OUTPUT ---- b"output_tokens-A=['\xc4\xa0Two', '\xc4\xa0roads', '\xc4\xa0diver', 'ged', '\xc4\xa0in', '\xc4\xa0a', '\xc4\xa0wood', ',', '\xc4\xa0and', '\xc4\xa0I', '\xc4\xa0--', '\xc4\xa0I', '\xc4\xa0took', '\xc4\xa0the', '\xc4\xa0one', '\xc4\xa0less', '\xc4\xa0traveled', '\xc4\xa0by', ',', '\xc4\xa0And', '\xc4\xa0that', '\xc4\xa0has', '\xc4\xa0made', '\xc4\xa0all', '\xc4\xa0the', '\xc4\xa0difference', '.']"
    # ---- NOTE-OUTPUT ---- b'output_tokens-A=27'
    # ---- NOTE-OUTPUT ---- b'output_token_ids-B=[0, 1596, 3197, 13105, 4462, 11, 10, 5627, 6, 8, 38, 480, 38, 362, 5, 65, 540, 8468, 30, 6, 178, 14, 34, 156, 70, 5, 2249, 4, 2]'
    utterance = \
        utterance_road_not_taken
    output_tokens: List[str] = \
        PytorchLanguageUnderstandingTransformersPretainedModelHelper.tokenize(utterance, tokenizer)
    DebuggingHelper.write_line_to_system_console_out(
        f'output_tokens-G={output_tokens}')
    DebuggingHelper.write_line_to_system_console_out(
        f'output_tokens-G={len(output_tokens)}')
    token_id_lists: List[List[int]] = \
        PytorchLanguageUnderstandingTransformersPretainedModelHelper.convert_token_lists_to_ids(output_tokens, tokenizer)
    DebuggingHelper.write_line_to_system_console_out(
        f'output_token_ids-G={token_id_lists}')
    # ---- NOTE-PYLINT ---- C0301: Line too long
    # pylint: disable=C0301
    # ---- NOTE-OUTPUT ---- b"segmented_pieces=[' Two', ' roads', ' diverged', ' in', ' a', ' wood', ',', ' and', ' I', ' --', ' I', ' took', ' the', ' one', ' less', ' traveled', ' by', ',', ' And', ' that', ' has', ' made', ' all', ' the', ' difference', '.']"
    # ---- NOTE ---- add_space_prefix=False
    # ---- NOTE-OUTPUT ---- b"output_tokens-b"output_tokens-G=[['\xc4\xa0Two'], ['\xc4\xa0roads'], ['\xc4\xa0diver', 'ged'], ['\xc4\xa0in'], ['\xc4\xa0a'], ['\xc4\xa0wood'], [','], ['\xc4\xa0and'], ['\xc4\xa0I'], ['\xc4\xa0--'], ['\xc4\xa0I'], ['\xc4\xa0took'], ['\xc4\xa0the'], ['\xc4\xa0one'], ['\xc4\xa0less'], ['\xc4\xa0traveled'], ['\xc4\xa0by'], [','], ['\xc4\xa0And'], ['\xc4\xa0that'], ['\xc4\xa0has'], ['\xc4\xa0made'], ['\xc4\xa0all'], ['\xc4\xa0the'], ['\xc4\xa0difference'], ['.']]"
    # ---- NOTE-OUTPUT ---- b'output_tokens-G=26'
    # ---- NOTE-OUTPUT ---- b'output_token_ids-G=[[1596], [3197], [13105, 4462], [11], [10], [5627], [6], [8], [38], [480], [38], [362], [5], [65], [540], [8468], [30], [6], [178], [14], [34], [156], [70], [5], [2249], [4]]'
    # ---- NOTE ---- add_space_prefix=True
    # ---- NOTE-OUTPUT ---- b"output_tokens-G=[['\xc4\xa0Two'], ['\xc4\xa0roads'], ['\xc4\xa0diver', 'ged'], ['\xc4\xa0in'], ['\xc4\xa0a'], ['\xc4\xa0wood'], ['\xc4\xa0,'], ['\xc4\xa0and'], ['\xc4\xa0I'], ['\xc4\xa0--'], ['\xc4\xa0I'], ['\xc4\xa0took'], ['\xc4\xa0the'], ['\xc4\xa0one'], ['\xc4\xa0less'], ['\xc4\xa0traveled'], ['\xc4\xa0by'], ['\xc4\xa0,'], ['\xc4\xa0And'], ['\xc4\xa0that'], ['\xc4\xa0has'], ['\xc4\xa0made'], ['\xc4\xa0all'], ['\xc4\xa0the'], ['\xc4\xa0difference'], ['\xc4\xa0.']]"
    # ---- NOTE-OUTPUT ---- b'output_tokens-G=26'
    # ---- NOTE-OUTPUT ---- b'output_token_ids-G=[[1596], [3197], [13105, 4462], [11], [10], [5627], [2156], [8], [38], [480], [38], [362], [5], [65], [540], [8468], [30], [2156], [178], [14], [34], [156], [70], [5], [2249], [479]]'
    # ---- NOTE ---- strip space from each word before tokenization
    # ---- NOTE-OUTPUT ---- b"output_tokens-G=[['Two'], ['roads'], ['d', 'iver', 'ged'], ['in'], ['a'], ['wood'], [','], ['and'], ['I'], ['--'], ['I'], ['t', 'ook'], ['the'], ['one'], ['less'], ['travel', 'ed'], ['by'], [','], ['And'], ['that'], ['has'], ['made'], ['all'], ['the'], ['diff', 'erence'], ['.']]"
    # ---- NOTE-OUTPUT ---- b'output_tokens-G=26'
    # ---- NOTE-OUTPUT ---- b'output_token_ids-G=[[9058], [14923], [417, 8538, 4462], [179], [102], [1845], [6], [463], [100], [5579], [100], [90, 6576], [627], [1264], [1672], [28881, 196], [1409], [6], [2409], [6025], [7333], [7078], [1250], [627], [32278, 24935], [4]]'
    # ------------------------------------------------------------------------
    utterance_road_not_taken = '<s>' + utterance_road_not_taken + '</s>'
    # ------------------------------------------------------------------------
    utterance: str = \
        utterance_road_not_taken
    output_tokens: List[str] = \
        tokenizer.tokenize(utterance)
    DebuggingHelper.write_line_to_system_console_out(
        f'output_tokens-<s>-A={output_tokens}')
    DebuggingHelper.write_line_to_system_console_out(
        f'output_tokens-<s>-A={len(output_tokens)}')
    token_id_lists: List[List[int]] = \
        PytorchLanguageUnderstandingTransformersPretainedModelHelper.convert_token_lists_to_ids(output_tokens, tokenizer)
    DebuggingHelper.write_line_to_system_console_out(
        f'output_token_ids-<s>-A={token_id_lists}')
    # ---- NOTE-PYLINT ---- C0301: Line too long
    # pylint: disable=C0301
    # ---- NOTE-OUTPUT ---- b"output_tokens-<s>-A=['<', 's', '>', '\xc4\xa0Two', '\xc4\xa0roads', '\xc4\xa0diver', 'ged', '\xc4\xa0in', '\xc4\xa0a', '\xc4\xa0wood', ',', '\xc4\xa0and', '\xc4\xa0I', '\xc4\xa0--', '\xc4\xa0I', '\xc4\xa0took', '\xc4\xa0the', '\xc4\xa0one', '\xc4\xa0less', '\xc4\xa0traveled', '\xc4\xa0by', ',', '\xc4\xa0And', '\xc4\xa0that', '\xc4\xa0has', '\xc4\xa0made', '\xc4\xa0all', '\xc4\xa0the', '\xc4\xa0difference', '.</', 's', '>']"
    # ---- NOTE-OUTPUT ---- b'output_tokens-<s>-A=32'
    # ---- NOTE-OUTPUT ---- b'output_token_ids-<s>-A=[41552, 29, 15698, 1596, 3197, 13105, 4462, 11, 10, 5627, 6, 8, 38, 480, 38, 362, 5, 65, 540, 8468, 30, 6, 178, 14, 34, 156, 70, 5, 2249, 49803, 29, 15698]'
    utterance = \
        utterance_road_not_taken
    output_tokens: List[str] = \
        PytorchLanguageUnderstandingTransformersPretainedModelHelper.tokenize(utterance, tokenizer)
    DebuggingHelper.write_line_to_system_console_out(
        f'output_tokens-<s>-G={output_tokens}')
    DebuggingHelper.write_line_to_system_console_out(
        f'output_tokens-<s>-G={len(output_tokens)}')
    token_id_lists: List[List[int]] = \
        PytorchLanguageUnderstandingTransformersPretainedModelHelper.convert_token_lists_to_ids(output_tokens, tokenizer)
    DebuggingHelper.write_line_to_system_console_out(
        f'output_token_ids-<s>-G={token_id_lists}')
    # ---- NOTE-PYLINT ---- C0301: Line too long
    # pylint: disable=C0301
    # ---- NOTE-OUTPUT ---- b"segmented_pieces=['<', 's', '>', ' Two', ' roads', ' diverged', ' in', ' a', ' wood', ',', ' and', ' I', ' --', ' I', ' took', ' the', ' one', ' less', ' traveled', ' by', ',', ' And', ' that', ' has', ' made', ' all', ' the', ' difference', '.</', 's', '>']"
    # ---- NOTE-OUTPUT ---- b"output_tokens-<s>-G=[['<'], ['s'], ['>'], ['\xc4\xa0Two'], ['\xc4\xa0roads'], ['\xc4\xa0diver', 'ged'], ['\xc4\xa0in'], ['\xc4\xa0a'], ['\xc4\xa0wood'], [','], ['\xc4\xa0and'], ['\xc4\xa0I'], ['\xc4\xa0--'], ['\xc4\xa0I'], ['\xc4\xa0took'], ['\xc4\xa0the'], ['\xc4\xa0one'], ['\xc4\xa0less'], ['\xc4\xa0traveled'], ['\xc4\xa0by'], [','], ['\xc4\xa0And'], ['\xc4\xa0that'], ['\xc4\xa0has'], ['\xc4\xa0made'], ['\xc4\xa0all'], ['\xc4\xa0the'], ['\xc4\xa0difference'], ['.</'], ['s'], ['>']]"
    # ---- NOTE-OUTPUT ---- b'output_tokens-<s>-G=31'
    # ---- NOTE-OUTPUT ---- b'output_token_ids-<s>-G=[[41552], [29], [15698], [1596], [3197], [13105, 4462], [11], [10], [5627], [6], [8], [38], [480], [38], [362], [5], [65], [540], [8468], [30], [6], [178], [14], [34], [156], [70], [5], [2249], [49803], [29], [15698]]'
    # ------------------------------------------------------------------------

def main_test_tokenizer_roberta_base():
    """
    The main_test_tokenizer_roberta_base() function can quickly test
    PytorchLanguageUnderstandingTransformersPretainedModelHelper functions.
    """
    # ------------------------------------------------------------------------
    # ---- NOTE-PYLINT ---- R0915: Too many statements
    # pylint: disable=R0915
    # ---- NOTE-PYLINT ---- C0301: Line too long
    # pylint: disable=C0301
    pytorch_transformers_pretrained_model_keys = \
        PytorchLanguageUnderstandingTransformersPretainedModelHelper.get_pretrained_model_keys()
    DebuggingHelper.write_line_to_system_console_out(
        f'Pytorch Transformers pretrained model keys={str(pytorch_transformers_pretrained_model_keys)}')
    parser = argparse.ArgumentParser()
    parser.add_argument(
        '--pytorch_transformers_pretrained_model_key',
        default='roberta-base', # ---- default='roberta-base', # ---- default='roberta-large'
        type=str,
        required=False,
        help=f'pytorch_transformers pre-trained model keys: '
             f'{str(pytorch_transformers_pretrained_model_keys)}.')
    parser.add_argument(
        '--pytorch_transformers_model_cache_dir_learner',
        default=os.path.join(
            ConfigurationHelper.MODEL_CACHE_DIRECTORY_STAMPED,
            'pytorch_transformers_cache'),
        type=str,
        required=False,
        help='pytorch_transformers model cache directory.')
    parser.add_argument(
        '--pytorch_transformers_model_saved_dir_learner',
        default=os.path.join(
            ConfigurationHelper.MODEL_CACHE_DIRECTORY_STAMPED,
            'pytorch_transformers'),
        type=str,
        required=False,
        help='pytorch_transformers model cache directory.')
    parser.add_argument(
        '--pytorch_transformers_model_tokenizer',
        default='roberta-base', # ---- default='roberta-base', # ---- default='roberta-large'
        type=str,
        required=False,
        help=f'Pytorch Transformers pre-trained model tokenizer vocabulary files: '
             f'{str(PytorchLanguageUnderstandingTransformersPretainedModelHelper.get_pretrained_vocabulary_file_keys())}.')
    parser.add_argument(
        '--pytorch_transformers_model_cache_dir_tokenizer',
        default=os.path.join(
            ConfigurationHelper.MODEL_TOKENIZER_CACHE_DIRECTORY_STAMPED,
            'pytorch_transformers_cache'),
        type=str,
        required=False,
        help='Pytorch Transformers model tokenizer cache directory.')
    parser.add_argument(
        '--pytorch_transformers_model_saved_dir_tokenizer',
        default=os.path.join(
            ConfigurationHelper.MODEL_TOKENIZER_CACHE_DIRECTORY_STAMPED,
            'pytorch_transformers'),
        type=str,
        required=False,
        help='Pytorch Transformers model tokenizer cache directory.')
    args: argparse.Namespace = parser.parse_args()
    # ------------------------------------------------------------------------
    model_key = 'roberta-base'
    config = PytorchLanguageUnderstandingTransformersPretainedModelHelper.config_from_pretrained(
        pretrained_model_name_or_path=model_key)
    DebuggingHelper.write_line_to_system_console_out(
        f'config={str(config)}')
    # ---- NOTE-OUTPUT ---- b'config=RobertaConfig {\n  "_num_labels": 2,\n  "architectures": [\n    "RobertaForMaskedLM"\n  ],\n  "attention_probs_dropout_prob": 0.1,\n  "bad_words_ids": null,\n  "bos_token_id": 0,\n  "decoder_start_token_id": null,\n  "do_sample": false,\n  "early_stopping": false,\n  "eos_token_id": 2,\n  "finetuning_task": null,\n  "hidden_act": "gelu",\n  "hidden_dropout_prob": 0.1,\n  "hidden_size": 768,\n  "id2label": {\n    "0": "LABEL_0",\n    "1": "LABEL_1"\n  },\n  "initializer_range": 0.02,\n  "intermediate_size": 3072,\n  "is_decoder": false,\n  "is_encoder_decoder": false,\n  "label2id": {\n    "LABEL_0": 0,\n    "LABEL_1": 1\n  },\n  "layer_norm_eps": 1e-05,\n  "length_penalty": 1.0,\n  "max_length": 20,\n  "max_position_embeddings": 514,\n  "min_length": 0,\n  "model_type": "roberta",\n  "no_repeat_ngram_size": 0,\n  "num_attention_heads": 12,\n  "num_beams": 1,\n  "num_hidden_layers": 12,\n  "num_return_sequences": 1,\n  "output_attentions": false,\n  "output_hidden_states": false,\n  "output_past": true,\n  "pad_token_id": 1,\n  "prefix": null,\n  "pruned_heads": {},\n  "repetition_penalty": 1.0,\n  "task_specific_params": null,\n  "temperature": 1.0,\n  "top_k": 50,\n  "top_p": 1.0,\n  "torchscript": false,\n  "type_vocab_size": 1,\n  "use_bfloat16": false,\n  "vocab_size": 50265\n}\n'
    tokenizer = PytorchLanguageUnderstandingTransformersPretainedModelHelper.tokenizer_from_pretrained( \
        pretrained_model_name_or_path=model_key,
        cache_dir=args.pytorch_transformers_model_cache_dir_tokenizer)
    DebuggingHelper.write_line_to_system_console_out(
        f'tokenizer={str(tokenizer)}')
    DebuggingHelper.write_line_to_system_console_out(
        f'tokenizer.vocab_size={tokenizer.vocab_size}')
    # ---- NOTE-OUTPUT ---- b'tokenizer.vocab_size=50265'
    # ------------------------------------------------------------------------
    utterance_road_not_taken: str = \
        'Two roads diverged in a wood, and I -- I took the one less traveled by, And that has made all the difference.'
    # ------------------------------------------------------------------------
    utterance: str = \
        utterance_road_not_taken
    output_tokens: List[str] = \
        tokenizer.tokenize(utterance)
    DebuggingHelper.write_line_to_system_console_out(
        f'output_tokens-A={output_tokens}')
    DebuggingHelper.write_line_to_system_console_out(
        f'output_tokens-A={len(output_tokens)}')
    token_id_lists: List[List[int]] = \
        PytorchLanguageUnderstandingTransformersPretainedModelHelper.convert_token_lists_to_ids(output_tokens, tokenizer)
    DebuggingHelper.write_line_to_system_console_out(
        f'output_token_ids-E={token_id_lists}')
    # ---- NOTE-OUTPUT ---- b"output_tokens-A=['Two', '\xc4\xa0roads', '\xc4\xa0diver', 'ged', '\xc4\xa0in', '\xc4\xa0a', '\xc4\xa0wood', ',', '\xc4\xa0and', '\xc4\xa0I', '\xc4\xa0--', '\xc4\xa0I', '\xc4\xa0took', '\xc4\xa0the', '\xc4\xa0one', '\xc4\xa0less', '\xc4\xa0traveled', '\xc4\xa0by', ',', '\xc4\xa0And', '\xc4\xa0that', '\xc4\xa0has', '\xc4\xa0made', '\xc4\xa0all', '\xc4\xa0the', '\xc4\xa0difference', '.']"
    # ---- NOTE-OUTPUT ---- b'output_tokens-A=27'
    utterance = \
        'wake me up at four fifteen am'
    output_tokens = \
        tokenizer.tokenize(utterance)
    DebuggingHelper.write_line_to_system_console_out(
        f'output_tokens-B={output_tokens}')
    DebuggingHelper.write_line_to_system_console_out(
        f'output_tokens-B={len(output_tokens)}')
    # ---- NOTE-OUTPUT ---- b"output_tokens-B=['wake', '\xc4\xa0me', '\xc4\xa0up', '\xc4\xa0at', '\xc4\xa0four', '\xc4\xa0fifteen', '\xc4\xa0am']"
    # ---- NOTE-OUTPUT ---- b'output_tokens-B=7'
    utterance = \
        'wake me up at four forty-five am'
    output_tokens = \
        tokenizer.tokenize(utterance)
    DebuggingHelper.write_line_to_system_console_out(
        f'output_tokens-C={output_tokens}')
    DebuggingHelper.write_line_to_system_console_out(
        f'output_tokens-C={len(output_tokens)}')
    # ---- NOTE-OUTPUT ---- b"output_tokens-C=['wake', '\xc4\xa0me', '\xc4\xa0up', '\xc4\xa0at', '\xc4\xa0four', '\xc4\xa0forty', '-', 'five', '\xc4\xa0am']"
    # ---- NOTE-OUTPUT ---- b'output_tokens-C=9'
    utterance = \
        'cancel alarm'
    output_tokens = \
        tokenizer.tokenize(utterance)
    DebuggingHelper.write_line_to_system_console_out(
        f'output_tokens-D={output_tokens}')
    DebuggingHelper.write_line_to_system_console_out(
        f'output_tokens-D={len(output_tokens)}')
    # ---- NOTE-OUTPUT ---- b"output_tokens-D=['c', 'ancel', '\xc4\xa0alarm']"
    # ---- NOTE-OUTPUT ---- b'output_tokens-D=3'
    utterance = \
        utterance_road_not_taken
    output_tokens: List[str] = \
        tokenizer.tokenize(utterance)
    DebuggingHelper.write_line_to_system_console_out(
        f'output_tokens-E={output_tokens}')
    DebuggingHelper.write_line_to_system_console_out(
        f'output_tokens-E={len(output_tokens)}')
    token_id_lists: List[List[int]] = \
        PytorchLanguageUnderstandingTransformersPretainedModelHelper.convert_token_lists_to_ids(output_tokens, tokenizer)
    DebuggingHelper.write_line_to_system_console_out(
        f'output_token_ids-E={token_id_lists}')
    # ---- NOTE-PYLINT ---- C0301: Line too long
    # pylint: disable=C0301
    # ---- NOTE-OUTPUT ---- b"output_tokens-E=['Two', '\xc4\xa0roads', '\xc4\xa0diver', 'ged', '\xc4\xa0in', '\xc4\xa0a', '\xc4\xa0wood', ',', '\xc4\xa0and', '\xc4\xa0I', '\xc4\xa0--', '\xc4\xa0I', '\xc4\xa0took', '\xc4\xa0the', '\xc4\xa0one', '\xc4\xa0less', '\xc4\xa0traveled', '\xc4\xa0by', ',', '\xc4\xa0And', '\xc4\xa0that', '\xc4\xa0has', '\xc4\xa0made', '\xc4\xa0all', '\xc4\xa0the', '\xc4\xa0difference', '.']"
    # ---- NOTE-OUTPUT ---- b'output_tokens-E=27'
    # ---- NOTE-OUTPUT ---- b'output_token_ids-E=[9058, 3197, 13105, 4462, 11, 10, 5627, 6, 8, 38, 480, 38, 362, 5, 65, 540, 8468, 30, 6, 178, 14, 34, 156, 70, 5, 2249, 4]'
    utterance = \
        utterance_road_not_taken
    output_tokens: List[str] = \
        PytorchLanguageUnderstandingTransformersPretainedModelHelper.tokenize_with_punctuations(utterance, tokenizer)
    DebuggingHelper.write_line_to_system_console_out(
        f'output_tokens-F={output_tokens}')
    DebuggingHelper.write_line_to_system_console_out(
        f'output_tokens-F={len(output_tokens)}')
    token_id_lists: List[List[int]] = \
        PytorchLanguageUnderstandingTransformersPretainedModelHelper.convert_token_lists_to_ids(output_tokens, tokenizer)
    DebuggingHelper.write_line_to_system_console_out(
        f'output_token_ids-F={token_id_lists}')
    # ---- NOTE-PYLINT ---- C0301: Line too long
    # pylint: disable=C0301
    # ---- NOTE-OUTPUT ---- b"segmented_pieces=['Two', 'roads', 'diverged', 'in', 'a', 'wood', ',', 'and', 'I', '-', '-', 'I', 'took', 'the', 'one', 'less', 'traveled', 'by', ',', 'And', 'that', 'has', 'made', 'all', 'the', 'difference', '.']"
    # ---- NOTE-OUTPUT ---- b"output_tokens-F=[['\xc4\xa0Two'], ['\xc4\xa0roads'], ['\xc4\xa0diver', 'ged'], ['\xc4\xa0in'], ['\xc4\xa0a'], ['\xc4\xa0wood'], ['\xc4\xa0,'], ['\xc4\xa0and'], ['\xc4\xa0I'], ['\xc4\xa0-'], ['\xc4\xa0-'], ['\xc4\xa0I'], ['\xc4\xa0took'], ['\xc4\xa0the'], ['\xc4\xa0one'], ['\xc4\xa0less'], ['\xc4\xa0traveled'], ['\xc4\xa0by'], ['\xc4\xa0,'], ['\xc4\xa0And'], ['\xc4\xa0that'], ['\xc4\xa0has'], ['\xc4\xa0made'], ['\xc4\xa0all'], ['\xc4\xa0the'], ['\xc4\xa0difference'], ['\xc4\xa0.']]"
    # ---- NOTE-OUTPUT ---- b'output_tokens-F=27'
    # ---- NOTE-OUTPUT ---- b'output_token_ids-F=[[1596], [3197], [13105, 4462], [11], [10], [5627], [2156], [8], [38], [111], [111], [38], [362], [5], [65], [540], [8468], [30], [2156], [178], [14], [34], [156], [70], [5], [2249], [479]]'
    utterance = \
        utterance_road_not_taken
    output_tokens: List[str] = \
        PytorchLanguageUnderstandingTransformersPretainedModelHelper.tokenize(utterance, tokenizer)
    DebuggingHelper.write_line_to_system_console_out(
        f'output_tokens-G={output_tokens}')
    DebuggingHelper.write_line_to_system_console_out(
        f'output_tokens-G={len(output_tokens)}')
    token_id_lists: List[List[int]] = \
        PytorchLanguageUnderstandingTransformersPretainedModelHelper.convert_token_lists_to_ids(output_tokens, tokenizer)
    DebuggingHelper.write_line_to_system_console_out(
        f'output_token_ids-G={token_id_lists}')
    # ---- NOTE-PYLINT ---- C0301: Line too long
    # pylint: disable=C0301
    # ---- NOTE-OUTPUT ---- b"segmented_pieces=['Two', ' roads', ' diverged', ' in', ' a', ' wood', ',', ' and', ' I', ' --', ' I', ' took', ' the', ' one', ' less', ' traveled', ' by', ',', ' And', ' that', ' has', ' made', ' all', ' the', ' difference', '.']"
    # ---- NOTE-OUTPUT ---- b"output_tokens-G=[['\xc4\xa0Two'], ['\xc4\xa0roads'], ['\xc4\xa0diver', 'ged'], ['\xc4\xa0in'], ['\xc4\xa0a'], ['\xc4\xa0wood'], ['\xc4\xa0,'], ['\xc4\xa0and'], ['\xc4\xa0I'], ['\xc4\xa0--'], ['\xc4\xa0I'], ['\xc4\xa0took'], ['\xc4\xa0the'], ['\xc4\xa0one'], ['\xc4\xa0less'], ['\xc4\xa0traveled'], ['\xc4\xa0by'], ['\xc4\xa0,'], ['\xc4\xa0And'], ['\xc4\xa0that'], ['\xc4\xa0has'], ['\xc4\xa0made'], ['\xc4\xa0all'], ['\xc4\xa0the'], ['\xc4\xa0difference'], ['\xc4\xa0.']]"
    # ---- NOTE-OUTPUT ---- b'output_tokens-G=26'
    # ---- NOTE-OUTPUT ---- b'output_token_ids-G=[[1596], [3197], [13105, 4462], [11], [10], [5627], [2156], [8], [38], [480], [38], [362], [5], [65], [540], [8468], [30], [2156], [178], [14], [34], [156], [70], [5], [2249], [479]]'
    # ------------------------------------------------------------------------

def main_test_get_model_named_parameters_bert():
    """
    main_test_get_model_named_parameters_bert()
    """
    pytorch_transformers_model_saved_dir: str = \
        os.path.join(os.path.os.sep, 'pyludispatch', '_models', 'pytorch_transformers', 'bert-base-uncased', 'sequence_classification')
        # ---- '/pyludispatch/_models/pytorch_transformers/bert-base-uncased/sequence_classification'
    model = PytorchLanguageUnderstandingTransformersPretainedModelHelper.pretrained_model_from_pretrained( \
        pretrained_model_name_or_path=pytorch_transformers_model_saved_dir,
        model_type=PytorchLanguageUnderstandingTransformersPretainedModelHelper.MODEL_TYPE_BERT)
    model_named_parameters = \
        PytorchLanguageUnderstandingTransformersPretainedModelHelper.get_model_named_parameters(model)
    DebuggingHelper.write_line_to_system_console_out(
        f'type(model_named_parameters).__name__={type(model_named_parameters).__name__}')
    DebuggingHelper.write_line_to_system_console_out(
        f'model_named_parameters={[x for x in model_named_parameters]}')

def main_test_bert():
    """
    The main_test_bert() function can quickly test PytorchLanguageUnderstandingTransformersPretainedModelHelper functions.
    """
    # ---- NOTE-PYLINT ---- R0915: Too many statements
    # pylint: disable=R0915
    # ---- NOTE-PYLINT ---- C0301: Line too long
    # pylint: disable=C0301
    pytorch_transformers_pretrained_model_keys = \
        PytorchLanguageUnderstandingTransformersPretainedModelHelper.get_pretrained_model_keys()
    DebuggingHelper.write_line_to_system_console_out(
        f'Pytorch Transformers pretrained model keys={str(pytorch_transformers_pretrained_model_keys)}')
    parser = argparse.ArgumentParser()
    parser.add_argument(
        '--pytorch_transformers_pretrained_model_key',
        default='bert-base-uncased', # ---- default='bert-base-uncased', # ---- default='bert-large-uncased', # ---- default='bert-base-cased', # ---- default='bert-large-cased', # ---- default='xlnet-large-cased'
        type=str,
        required=False,
        help=f'pytorch_transformers pre-trained model keys: '
             f'{str(pytorch_transformers_pretrained_model_keys)}.')
    parser.add_argument(
        '--pytorch_transformers_model_cache_dir_learner',
        default=os.path.join(
            ConfigurationHelper.MODEL_CACHE_DIRECTORY_STAMPED,
            'pytorch_transformers_cache'),
        type=str,
        required=False,
        help='pytorch_transformers model cache directory.')
    parser.add_argument(
        '--pytorch_transformers_model_saved_dir_learner',
        default=os.path.join(
            ConfigurationHelper.MODEL_CACHE_DIRECTORY_STAMPED,
            'pytorch_transformers'),
        type=str,
        required=False,
        help='pytorch_transformers model cache directory.')
    parser.add_argument(
        '--pytorch_transformers_model_tokenizer',
        default='bert-base-uncased', # ---- default='bert-base-uncased', # ---- default='bert-large-uncased', # ---- default='bert-base-cased', # ---- default='bert-large-cased', # ---- default='xlnet-large-cased'
        type=str,
        required=False,
        help=f'Pytorch Transformers pre-trained model tokenizer vocabulary files: '
             f'{str(PytorchLanguageUnderstandingTransformersPretainedModelHelper.get_pretrained_vocabulary_file_keys())}.')
    parser.add_argument(
        '--pytorch_transformers_model_cache_dir_tokenizer',
        default=os.path.join(
            ConfigurationHelper.MODEL_TOKENIZER_CACHE_DIRECTORY_STAMPED,
            'pytorch_transformers_cache'),
        type=str,
        required=False,
        help='Pytorch Transformers model tokenizer cache directory.')
    parser.add_argument(
        '--pytorch_transformers_model_saved_dir_tokenizer',
        default=os.path.join(
            ConfigurationHelper.MODEL_TOKENIZER_CACHE_DIRECTORY_STAMPED,
            'pytorch_transformers'),
        type=str,
        required=False,
        help='Pytorch Transformers model tokenizer cache directory.')
    args: argparse.Namespace = parser.parse_args()
    # ------------------------------------------------------------------------
    # model_specific = BertForSequenceClassification.from_pretrained(
    #     pretrained_model_name_or_path='bert-base-uncased',
    #     cache_dir=args.pytorch_transformers_model_cache_dir_learner)
    # optimizer_configuration_specific = PytorchLanguageUnderstandingTransformersPretainedModelHelper.get_optimizer_scheduer_configuration(
    #     model=model_specific,
    #     optimizer_number_training_optimization_steps=10)
    # DebuggingHelper.write_line_to_system_console_out(
    #     f'optimizer_configuration_specific ={str(optimizer_configuration_specific )}')
    # ------------------------------------------------------------------------
    model_key = 'bert-base-cased'
    config = PytorchLanguageUnderstandingTransformersPretainedModelHelper.config_from_pretrained(
        pretrained_model_name_or_path=model_key)
    DebuggingHelper.write_line_to_system_console_out(
        f'config={str(config)}')
    tokenizer = PytorchLanguageUnderstandingTransformersPretainedModelHelper.tokenizer_from_pretrained( \
        pretrained_model_name_or_path=model_key,
        cache_dir=args.pytorch_transformers_model_cache_dir_tokenizer)
    tokenizer.save_pretrained(
        os.path.join(
            args.pytorch_transformers_model_saved_dir_tokenizer,
            model_key))
    DebuggingHelper.write_line_to_system_console_out(
        f'tokenizer={str(tokenizer)}')
    model = PytorchLanguageUnderstandingTransformersPretainedModelHelper.pretrained_model_from_pretrained(
        pretrained_model_name_or_path=model_key,
        cache_dir=args.pytorch_transformers_model_cache_dir_learner)
    model.eval()
    model.save_pretrained(
        os.path.join(
            args.pytorch_transformers_model_saved_dir_learner,
            model_key,
            'pretrained_model'))
    DebuggingHelper.write_line_to_system_console_out(
        f'BERT pretrained model={str(model)}')
    model = PytorchLanguageUnderstandingTransformersPretainedModelHelper.model_from_pretrained(
        pretrained_model_name_or_path=model_key,
        cache_dir=args.pytorch_transformers_model_cache_dir_learner)
    model.eval()
    model.save_pretrained(
        os.path.join(
            args.pytorch_transformers_model_saved_dir_learner,
            model_key,
            'model'))
    DebuggingHelper.write_line_to_system_console_out(
        f'BERT model={str(model)}')
    model = PytorchLanguageUnderstandingTransformersPretainedModelHelper.sequence_classification_model_from_pretrained(
        pretrained_model_name_or_path=model_key,
        cache_dir=args.pytorch_transformers_model_cache_dir_learner)
    model.eval()
    model.save_pretrained(
        os.path.join(
            args.pytorch_transformers_model_saved_dir_learner,
            model_key,
            'sequence_classification'))
    DebuggingHelper.write_line_to_system_console_out(
        f'BERT sequence classification model={str(model)}')
    model = PytorchLanguageUnderstandingTransformersPretainedModelHelper.token_classification_model_from_pretrained(
        pretrained_model_name_or_path=model_key,
        cache_dir=args.pytorch_transformers_model_cache_dir_learner)
    model.eval()
    model.save_pretrained(
        os.path.join(
            args.pytorch_transformers_model_saved_dir_learner,
            model_key,
            'token_classification'))
    DebuggingHelper.write_line_to_system_console_out(
        f'BERT token classification model={str(model)}')
    model = PytorchLanguageUnderstandingTransformersPretainedModelHelper.question_answering_model_from_pretrained(
        pretrained_model_name_or_path=model_key,
        cache_dir=args.pytorch_transformers_model_cache_dir_learner)
    model.eval()
    model.save_pretrained(
        os.path.join(
            args.pytorch_transformers_model_saved_dir_learner,
            model_key,
            'question_answering'))
    DebuggingHelper.write_line_to_system_console_out(
        f'BERT question answering model={str(model)}')
    optimizer_configuration = PytorchLanguageUnderstandingTransformersPretainedModelHelper.get_optimizer_scheduer_configuration(
        model=model,
        optimizer_number_training_optimization_steps=10)
    DebuggingHelper.write_line_to_system_console_out(
        f'optimizer_configuration={str(optimizer_configuration)}')
    # ------------------------------------------------------------------------
    model_key = 'bert-base-uncased'
    config = PytorchLanguageUnderstandingTransformersPretainedModelHelper.config_from_pretrained(
        pretrained_model_name_or_path=model_key)
    DebuggingHelper.write_line_to_system_console_out(
        f'config={str(config)}')
    tokenizer = PytorchLanguageUnderstandingTransformersPretainedModelHelper.tokenizer_from_pretrained( \
        pretrained_model_name_or_path=model_key,
        cache_dir=args.pytorch_transformers_model_cache_dir_tokenizer)
    tokenizer.save_pretrained(
        os.path.join(
            args.pytorch_transformers_model_saved_dir_tokenizer,
            model_key))
    DebuggingHelper.write_line_to_system_console_out(
        f'tokenizer={str(tokenizer)}')
    model = PytorchLanguageUnderstandingTransformersPretainedModelHelper.pretrained_model_from_pretrained(
        pretrained_model_name_or_path=model_key,
        cache_dir=args.pytorch_transformers_model_cache_dir_learner)
    model.eval()
    model.save_pretrained(
        os.path.join(
            args.pytorch_transformers_model_saved_dir_learner,
            model_key,
            'pretrained_model'))
    DebuggingHelper.write_line_to_system_console_out(
        f'BERT pretrained model={str(model)}')
    model = PytorchLanguageUnderstandingTransformersPretainedModelHelper.model_from_pretrained(
        pretrained_model_name_or_path=model_key,
        cache_dir=args.pytorch_transformers_model_cache_dir_learner)
    model.eval()
    model.save_pretrained(
        os.path.join(
            args.pytorch_transformers_model_saved_dir_learner,
            model_key,
            'model'))
    DebuggingHelper.write_line_to_system_console_out(
        f'BERT model={str(model)}')
    model = PytorchLanguageUnderstandingTransformersPretainedModelHelper.sequence_classification_model_from_pretrained(
        pretrained_model_name_or_path=model_key,
        cache_dir=args.pytorch_transformers_model_cache_dir_learner)
    model.eval()
    model.save_pretrained(
        os.path.join(
            args.pytorch_transformers_model_saved_dir_learner,
            model_key,
            'sequence_classification'))
    DebuggingHelper.write_line_to_system_console_out(
        f'BERT sequence classification model={str(model)}')
    model = PytorchLanguageUnderstandingTransformersPretainedModelHelper.token_classification_model_from_pretrained(
        pretrained_model_name_or_path=model_key,
        cache_dir=args.pytorch_transformers_model_cache_dir_learner)
    model.eval()
    model.save_pretrained(
        os.path.join(
            args.pytorch_transformers_model_saved_dir_learner,
            model_key,
            'token_classification'))
    DebuggingHelper.write_line_to_system_console_out(
        f'BERT token classification model={str(model)}')
    model = PytorchLanguageUnderstandingTransformersPretainedModelHelper.question_answering_model_from_pretrained(
        pretrained_model_name_or_path=model_key,
        cache_dir=args.pytorch_transformers_model_cache_dir_learner)
    model.eval()
    model.save_pretrained(
        os.path.join(
            args.pytorch_transformers_model_saved_dir_learner,
            model_key,
            'question_answering'))
    DebuggingHelper.write_line_to_system_console_out(
        f'BERT question answering model={str(model)}')
    optimizer_configuration = PytorchLanguageUnderstandingTransformersPretainedModelHelper.get_optimizer_scheduer_configuration(
        model=model,
        optimizer_number_training_optimization_steps=10)
    DebuggingHelper.write_line_to_system_console_out(
        f'optimizer_configuration={str(optimizer_configuration)}')
    # ------------------------------------------------------------------------
    model_key = 'bert-base-multilingual-cased'
    config = PytorchLanguageUnderstandingTransformersPretainedModelHelper.config_from_pretrained(
        pretrained_model_name_or_path=model_key)
    DebuggingHelper.write_line_to_system_console_out(
        f'config={str(config)}')
    tokenizer = PytorchLanguageUnderstandingTransformersPretainedModelHelper.tokenizer_from_pretrained( \
        pretrained_model_name_or_path=model_key,
        cache_dir=args.pytorch_transformers_model_cache_dir_tokenizer)
    tokenizer.save_pretrained(
        os.path.join(
            args.pytorch_transformers_model_saved_dir_tokenizer,
            model_key))
    DebuggingHelper.write_line_to_system_console_out(
        f'tokenizer={str(tokenizer)}')
    model = PytorchLanguageUnderstandingTransformersPretainedModelHelper.pretrained_model_from_pretrained(
        pretrained_model_name_or_path=model_key,
        cache_dir=args.pytorch_transformers_model_cache_dir_learner)
    model.eval()
    model.save_pretrained(
        os.path.join(
            args.pytorch_transformers_model_saved_dir_learner,
            model_key,
            'pretrained_model'))
    DebuggingHelper.write_line_to_system_console_out(
        f'BERT pretrained model={str(model)}')
    model = PytorchLanguageUnderstandingTransformersPretainedModelHelper.model_from_pretrained(
        pretrained_model_name_or_path=model_key,
        cache_dir=args.pytorch_transformers_model_cache_dir_learner)
    model.eval()
    model.save_pretrained(
        os.path.join(
            args.pytorch_transformers_model_saved_dir_learner,
            model_key,
            'model'))
    DebuggingHelper.write_line_to_system_console_out(
        f'BERT model={str(model)}')
    model = PytorchLanguageUnderstandingTransformersPretainedModelHelper.sequence_classification_model_from_pretrained(
        pretrained_model_name_or_path=model_key,
        cache_dir=args.pytorch_transformers_model_cache_dir_learner)
    model.eval()
    model.save_pretrained(
        os.path.join(
            args.pytorch_transformers_model_saved_dir_learner,
            model_key,
            'sequence_classification'))
    DebuggingHelper.write_line_to_system_console_out(
        f'BERT sequence classification model={str(model)}')
    model = PytorchLanguageUnderstandingTransformersPretainedModelHelper.token_classification_model_from_pretrained(
        pretrained_model_name_or_path=model_key,
        cache_dir=args.pytorch_transformers_model_cache_dir_learner)
    model.eval()
    model.save_pretrained(
        os.path.join(
            args.pytorch_transformers_model_saved_dir_learner,
            model_key,
            'token_classification'))
    DebuggingHelper.write_line_to_system_console_out(
        f'BERT token classification model={str(model)}')
    model = PytorchLanguageUnderstandingTransformersPretainedModelHelper.question_answering_model_from_pretrained(
        pretrained_model_name_or_path=model_key,
        cache_dir=args.pytorch_transformers_model_cache_dir_learner)
    model.eval()
    model.save_pretrained(
        os.path.join(
            args.pytorch_transformers_model_saved_dir_learner,
            model_key,
            'question_answering'))
    DebuggingHelper.write_line_to_system_console_out(
        f'BERT question answering model={str(model)}')
    optimizer_configuration = PytorchLanguageUnderstandingTransformersPretainedModelHelper.get_optimizer_scheduer_configuration(
        model=model,
        optimizer_number_training_optimization_steps=10)
    DebuggingHelper.write_line_to_system_console_out(
        f'optimizer_configuration={str(optimizer_configuration)}')
    # ------------------------------------------------------------------------
    model_key = 'bert-base-multilingual-uncased'
    config = PytorchLanguageUnderstandingTransformersPretainedModelHelper.config_from_pretrained(
        pretrained_model_name_or_path=model_key)
    DebuggingHelper.write_line_to_system_console_out(
        f'config={str(config)}')
    tokenizer = PytorchLanguageUnderstandingTransformersPretainedModelHelper.tokenizer_from_pretrained( \
        pretrained_model_name_or_path=model_key,
        cache_dir=args.pytorch_transformers_model_cache_dir_tokenizer)
    tokenizer.save_pretrained(
        os.path.join(
            args.pytorch_transformers_model_saved_dir_tokenizer,
            model_key))
    DebuggingHelper.write_line_to_system_console_out(
        f'tokenizer={str(tokenizer)}')
    model = PytorchLanguageUnderstandingTransformersPretainedModelHelper.pretrained_model_from_pretrained(
        pretrained_model_name_or_path=model_key,
        cache_dir=args.pytorch_transformers_model_cache_dir_learner)
    model.eval()
    model.save_pretrained(
        os.path.join(
            args.pytorch_transformers_model_saved_dir_learner,
            model_key,
            'pretrained_model'))
    DebuggingHelper.write_line_to_system_console_out(
        f'BERT pretrained model={str(model)}')
    model = PytorchLanguageUnderstandingTransformersPretainedModelHelper.model_from_pretrained(
        pretrained_model_name_or_path=model_key,
        cache_dir=args.pytorch_transformers_model_cache_dir_learner)
    model.eval()
    model.save_pretrained(
        os.path.join(
            args.pytorch_transformers_model_saved_dir_learner,
            model_key,
            'model'))
    DebuggingHelper.write_line_to_system_console_out(
        f'BERT model={str(model)}')
    model = PytorchLanguageUnderstandingTransformersPretainedModelHelper.sequence_classification_model_from_pretrained(
        pretrained_model_name_or_path=model_key,
        cache_dir=args.pytorch_transformers_model_cache_dir_learner)
    model.eval()
    model.save_pretrained(
        os.path.join(
            args.pytorch_transformers_model_saved_dir_learner,
            model_key,
            'sequence_classification'))
    DebuggingHelper.write_line_to_system_console_out(
        f'BERT sequence classification model={str(model)}')
    model = PytorchLanguageUnderstandingTransformersPretainedModelHelper.token_classification_model_from_pretrained(
        pretrained_model_name_or_path=model_key,
        cache_dir=args.pytorch_transformers_model_cache_dir_learner)
    model.eval()
    model.save_pretrained(
        os.path.join(
            args.pytorch_transformers_model_saved_dir_learner,
            model_key,
            'token_classification'))
    DebuggingHelper.write_line_to_system_console_out(
        f'BERT token classification model={str(model)}')
    model = PytorchLanguageUnderstandingTransformersPretainedModelHelper.question_answering_model_from_pretrained(
        pretrained_model_name_or_path=model_key,
        cache_dir=args.pytorch_transformers_model_cache_dir_learner)
    model.eval()
    model.save_pretrained(
        os.path.join(
            args.pytorch_transformers_model_saved_dir_learner,
            model_key,
            'question_answering'))
    DebuggingHelper.write_line_to_system_console_out(
        f'BERT question answering model={str(model)}')
    optimizer_configuration = PytorchLanguageUnderstandingTransformersPretainedModelHelper.get_optimizer_scheduer_configuration(
        model=model,
        optimizer_number_training_optimization_steps=10)
    DebuggingHelper.write_line_to_system_console_out(
        f'optimizer_configuration={str(optimizer_configuration)}')
    # ------------------------------------------------------------------------
    model_key = 'bert-large-cased'
    config = PytorchLanguageUnderstandingTransformersPretainedModelHelper.config_from_pretrained(
        pretrained_model_name_or_path=model_key)
    DebuggingHelper.write_line_to_system_console_out(
        f'config={str(config)}')
    tokenizer = PytorchLanguageUnderstandingTransformersPretainedModelHelper.tokenizer_from_pretrained( \
        pretrained_model_name_or_path=model_key,
        cache_dir=args.pytorch_transformers_model_cache_dir_tokenizer)
    tokenizer.save_pretrained(
        os.path.join(
            args.pytorch_transformers_model_saved_dir_tokenizer,
            model_key))
    DebuggingHelper.write_line_to_system_console_out(
        f'tokenizer={str(tokenizer)}')
    model = PytorchLanguageUnderstandingTransformersPretainedModelHelper.pretrained_model_from_pretrained(
        pretrained_model_name_or_path=model_key,
        cache_dir=args.pytorch_transformers_model_cache_dir_learner)
    model.eval()
    model.save_pretrained(
        os.path.join(
            args.pytorch_transformers_model_saved_dir_learner,
            model_key,
            'pretrained_model'))
    DebuggingHelper.write_line_to_system_console_out(
        f'BERT pretrained model={str(model)}')
    model = PytorchLanguageUnderstandingTransformersPretainedModelHelper.model_from_pretrained(
        pretrained_model_name_or_path=model_key,
        cache_dir=args.pytorch_transformers_model_cache_dir_learner)
    model.eval()
    model.save_pretrained(
        os.path.join(
            args.pytorch_transformers_model_saved_dir_learner,
            model_key,
            'model'))
    DebuggingHelper.write_line_to_system_console_out(
        f'BERT model={str(model)}')
    model = PytorchLanguageUnderstandingTransformersPretainedModelHelper.sequence_classification_model_from_pretrained(
        pretrained_model_name_or_path=model_key,
        cache_dir=args.pytorch_transformers_model_cache_dir_learner)
    model.eval()
    model.save_pretrained(
        os.path.join(
            args.pytorch_transformers_model_saved_dir_learner,
            model_key,
            'sequence_classification'))
    DebuggingHelper.write_line_to_system_console_out(
        f'BERT sequence classification model={str(model)}')
    model = PytorchLanguageUnderstandingTransformersPretainedModelHelper.token_classification_model_from_pretrained(
        pretrained_model_name_or_path=model_key,
        cache_dir=args.pytorch_transformers_model_cache_dir_learner)
    model.eval()
    model.save_pretrained(
        os.path.join(
            args.pytorch_transformers_model_saved_dir_learner,
            model_key,
            'token_classification'))
    DebuggingHelper.write_line_to_system_console_out(
        f'BERT token classification model={str(model)}')
    model = PytorchLanguageUnderstandingTransformersPretainedModelHelper.question_answering_model_from_pretrained(
        pretrained_model_name_or_path=model_key,
        cache_dir=args.pytorch_transformers_model_cache_dir_learner)
    model.eval()
    model.save_pretrained(
        os.path.join(
            args.pytorch_transformers_model_saved_dir_learner,
            model_key,
            'question_answering'))
    DebuggingHelper.write_line_to_system_console_out(
        f'BERT question answering model={str(model)}')
    optimizer_configuration = PytorchLanguageUnderstandingTransformersPretainedModelHelper.get_optimizer_scheduer_configuration(
        model=model,
        optimizer_number_training_optimization_steps=10)
    DebuggingHelper.write_line_to_system_console_out(
        f'optimizer_configuration={str(optimizer_configuration)}')
    # ------------------------------------------------------------------------
    model_key = 'bert-large-uncased'
    config = PytorchLanguageUnderstandingTransformersPretainedModelHelper.config_from_pretrained(
        pretrained_model_name_or_path=model_key)
    DebuggingHelper.write_line_to_system_console_out(
        f'config={str(config)}')
    tokenizer = PytorchLanguageUnderstandingTransformersPretainedModelHelper.tokenizer_from_pretrained( \
        pretrained_model_name_or_path=model_key,
        cache_dir=args.pytorch_transformers_model_cache_dir_tokenizer)
    tokenizer.save_pretrained(
        os.path.join(
            args.pytorch_transformers_model_saved_dir_tokenizer,
            model_key))
    DebuggingHelper.write_line_to_system_console_out(
        f'tokenizer={str(tokenizer)}')
    model = PytorchLanguageUnderstandingTransformersPretainedModelHelper.pretrained_model_from_pretrained(
        pretrained_model_name_or_path=model_key,
        cache_dir=args.pytorch_transformers_model_cache_dir_learner)
    model.eval()
    model.save_pretrained(
        os.path.join(
            args.pytorch_transformers_model_saved_dir_learner,
            model_key,
            'pretrained_model'))
    DebuggingHelper.write_line_to_system_console_out(
        f'BERT pretrained model={str(model)}')
    model = PytorchLanguageUnderstandingTransformersPretainedModelHelper.model_from_pretrained(
        pretrained_model_name_or_path=model_key,
        cache_dir=args.pytorch_transformers_model_cache_dir_learner)
    model.eval()
    model.save_pretrained(
        os.path.join(
            args.pytorch_transformers_model_saved_dir_learner,
            model_key,
            'model'))
    DebuggingHelper.write_line_to_system_console_out(
        f'BERT model={str(model)}')
    model = PytorchLanguageUnderstandingTransformersPretainedModelHelper.sequence_classification_model_from_pretrained(
        pretrained_model_name_or_path=model_key,
        cache_dir=args.pytorch_transformers_model_cache_dir_learner)
    model.eval()
    model.save_pretrained(
        os.path.join(
            args.pytorch_transformers_model_saved_dir_learner,
            model_key,
            'sequence_classification'))
    DebuggingHelper.write_line_to_system_console_out(
        f'BERT sequence classification model={str(model)}')
    model = PytorchLanguageUnderstandingTransformersPretainedModelHelper.token_classification_model_from_pretrained(
        pretrained_model_name_or_path=model_key,
        cache_dir=args.pytorch_transformers_model_cache_dir_learner)
    model.eval()
    model.save_pretrained(
        os.path.join(
            args.pytorch_transformers_model_saved_dir_learner,
            model_key,
            'token_classification'))
    DebuggingHelper.write_line_to_system_console_out(
        f'BERT token classification model={str(model)}')
    model = PytorchLanguageUnderstandingTransformersPretainedModelHelper.question_answering_model_from_pretrained(
        pretrained_model_name_or_path=model_key,
        cache_dir=args.pytorch_transformers_model_cache_dir_learner)
    model.eval()
    model.save_pretrained(
        os.path.join(
            args.pytorch_transformers_model_saved_dir_learner,
            model_key,
            'question_answering'))
    DebuggingHelper.write_line_to_system_console_out(
        f'BERT question answering model={str(model)}')
    optimizer_configuration = PytorchLanguageUnderstandingTransformersPretainedModelHelper.get_optimizer_scheduer_configuration(
        model=model,
        optimizer_number_training_optimization_steps=10)
    DebuggingHelper.write_line_to_system_console_out(
        f'optimizer_configuration={str(optimizer_configuration)}')
    # ------------------------------------------------------------------------

def main_test_roberta():
    """
    The main_test_roberta() function can quickly test PytorchLanguageUnderstandingTransformersPretainedModelHelper functions.
    """
    # ---- NOTE-PYLINT ---- R0915: Too many statements
    # pylint: disable=R0915
    # ---- NOTE-PYLINT ---- C0301: Line too long
    # pylint: disable=C0301
    pytorch_transformers_pretrained_model_keys = \
        PytorchLanguageUnderstandingTransformersPretainedModelHelper.get_pretrained_model_keys()
    DebuggingHelper.write_line_to_system_console_out(
        f'Pytorch Transformers pretrained model keys={str(pytorch_transformers_pretrained_model_keys)}')
    parser = argparse.ArgumentParser()
    parser.add_argument(
        '--pytorch_transformers_pretrained_model_key',
        default='roberta-base', # ---- default='roberta-base', # ---- default='roberta-large'
        type=str,
        required=False,
        help=f'pytorch_transformers pre-trained model keys: '
             f'{str(pytorch_transformers_pretrained_model_keys)}.')
    parser.add_argument(
        '--pytorch_transformers_model_cache_dir_learner',
        default=os.path.join(
            ConfigurationHelper.MODEL_CACHE_DIRECTORY_STAMPED,
            'pytorch_transformers_cache'),
        type=str,
        required=False,
        help='pytorch_transformers model cache directory.')
    parser.add_argument(
        '--pytorch_transformers_model_saved_dir_learner',
        default=os.path.join(
            ConfigurationHelper.MODEL_CACHE_DIRECTORY_STAMPED,
            'pytorch_transformers'),
        type=str,
        required=False,
        help='pytorch_transformers model cache directory.')
    parser.add_argument(
        '--pytorch_transformers_model_tokenizer',
        default='roberta-base', # ---- default='roberta-base', # ---- default='roberta-large'
        type=str,
        required=False,
        help=f'Pytorch Transformers pre-trained model tokenizer vocabulary files: '
             f'{str(PytorchLanguageUnderstandingTransformersPretainedModelHelper.get_pretrained_vocabulary_file_keys())}.')
    parser.add_argument(
        '--pytorch_transformers_model_cache_dir_tokenizer',
        default=os.path.join(
            ConfigurationHelper.MODEL_TOKENIZER_CACHE_DIRECTORY_STAMPED,
            'pytorch_transformers_cache'),
        type=str,
        required=False,
        help='Pytorch Transformers model tokenizer cache directory.')
    parser.add_argument(
        '--pytorch_transformers_model_saved_dir_tokenizer',
        default=os.path.join(
            ConfigurationHelper.MODEL_TOKENIZER_CACHE_DIRECTORY_STAMPED,
            'pytorch_transformers'),
        type=str,
        required=False,
        help='Pytorch Transformers model tokenizer cache directory.')
    args: argparse.Namespace = parser.parse_args()
    # ------------------------------------------------------------------------
    # model_specific = RorertaForSequenceClassification.from_pretrained(
    #     pretrained_model_name_or_path='roberta-base',
    #     cache_dir=args.pytorch_transformers_model_cache_dir_learner)
    # optimizer_configuration_specific = PytorchLanguageUnderstandingTransformersPretainedModelHelper.get_optimizer_scheduer_configuration(
    #     model=model_specific,
    #     optimizer_number_training_optimization_steps=10)
    # DebuggingHelper.write_line_to_system_console_out(
    #     f'optimizer_configuration_specific ={str(optimizer_configuration_specific )}')
    # ------------------------------------------------------------------------
    model_key = 'roberta-base'
    config = PytorchLanguageUnderstandingTransformersPretainedModelHelper.config_from_pretrained(
        pretrained_model_name_or_path=model_key)
    DebuggingHelper.write_line_to_system_console_out(
        f'config={config}',
        utf8_conversion=False)
    tokenizer = PytorchLanguageUnderstandingTransformersPretainedModelHelper.tokenizer_from_pretrained( \
        pretrained_model_name_or_path=model_key,
        cache_dir=args.pytorch_transformers_model_cache_dir_tokenizer)
    tokenizer.save_pretrained(
        os.path.join(
            args.pytorch_transformers_model_saved_dir_tokenizer,
            model_key))
    DebuggingHelper.write_line_to_system_console_out(
        f'tokenizer={tokenizer}',
        utf8_conversion=False)
    model = PytorchLanguageUnderstandingTransformersPretainedModelHelper.pretrained_model_from_pretrained(
        pretrained_model_name_or_path=model_key,
        cache_dir=args.pytorch_transformers_model_cache_dir_learner)
    model.eval()
    model.save_pretrained(
        os.path.join(
            args.pytorch_transformers_model_saved_dir_learner,
            model_key,
            'pretrained_model'))
    DebuggingHelper.write_line_to_system_console_out(
        f'ROBERTA pretrained model={str(model)}')
    model = PytorchLanguageUnderstandingTransformersPretainedModelHelper.model_from_pretrained(
        pretrained_model_name_or_path=model_key,
        cache_dir=args.pytorch_transformers_model_cache_dir_learner)
    model.eval()
    model.save_pretrained(
        os.path.join(
            args.pytorch_transformers_model_saved_dir_learner,
            model_key,
            'model'))
    DebuggingHelper.write_line_to_system_console_out(
        f'ROBERTA model={str(model)}')
    model = PytorchLanguageUnderstandingTransformersPretainedModelHelper.sequence_classification_model_from_pretrained(
        pretrained_model_name_or_path=model_key,
        cache_dir=args.pytorch_transformers_model_cache_dir_learner)
    model.eval()
    model.save_pretrained(
        os.path.join(
            args.pytorch_transformers_model_saved_dir_learner,
            model_key,
            'sequence_classification'))
    DebuggingHelper.write_line_to_system_console_out(
        f'ROBERTA sequence classification model={model}',
        utf8_conversion=False)
    model = PytorchLanguageUnderstandingTransformersPretainedModelHelper.token_classification_model_from_pretrained(
        pretrained_model_name_or_path=model_key,
        cache_dir=args.pytorch_transformers_model_cache_dir_learner)
    model.eval()
    model.save_pretrained(
        os.path.join(
            args.pytorch_transformers_model_saved_dir_learner,
            model_key,
            'token_classification'))
    DebuggingHelper.write_line_to_system_console_out(
        f'ROBERTA token classification model={str(model)}')
    # ==== model = PytorchLanguageUnderstandingTransformersPretainedModelHelper.question_answering_model_from_pretrained(
    # ====     pretrained_model_name_or_path=model_key,
    # ====     cache_dir=args.pytorch_transformers_model_cache_dir_learner)
    # ==== model.eval()
    # ==== model.save_pretrained(
    # ====     os.path.join(
    # ====         args.pytorch_transformers_model_saved_dir_learner,
    # ====         model_key,
    # ====         'question_answering'))
    # ==== DebuggingHelper.write_line_to_system_console_out(
    # ====     f'ROBERTA question answering model={str(model)}')
    optimizer_configuration = PytorchLanguageUnderstandingTransformersPretainedModelHelper.get_optimizer_scheduer_configuration(
        model=model,
        optimizer_number_training_optimization_steps=10)
    DebuggingHelper.write_line_to_system_console_out(
        f'optimizer_configuration={optimizer_configuration}',
        utf8_conversion=False)
    # ------------------------------------------------------------------------

def main_test_xlm():
    """
    The main_test_xlm() function can quickly test PytorchLanguageUnderstandingTransformersPretainedModelHelper functions.
    """
    # ---- NOTE-PYLINT ---- R0915: Too many statements
    # pylint: disable=R0915
    # ---- NOTE-PYLINT ---- C0301: Line too long
    # pylint: disable=C0301
    pytorch_transformers_pretrained_model_keys = \
        PytorchLanguageUnderstandingTransformersPretainedModelHelper.get_pretrained_model_keys()
    DebuggingHelper.write_line_to_system_console_out(
        f'Pytorch Transformers pretrained model keys={str(pytorch_transformers_pretrained_model_keys)}')
    parser = argparse.ArgumentParser()
    parser.add_argument(
        '--pytorch_transformers_pretrained_model_key',
        default='bert-base-uncased', # ---- default='bert-base-uncased', # ---- default='bert-large-uncased', # ---- default='bert-base-cased', # ---- default='bert-large-cased', # ---- default='xlnet-large-cased'
        type=str,
        required=False,
        help=f'pytorch_transformers pre-trained model keys: '
             f'{str(pytorch_transformers_pretrained_model_keys)}.')
    parser.add_argument(
        '--pytorch_transformers_model_cache_dir_learner',
        default=os.path.join(
            ConfigurationHelper.MODEL_CACHE_DIRECTORY_STAMPED,
            'pytorch_transformers_cache'),
        type=str,
        required=False,
        help='pytorch_transformers model cache directory.')
    parser.add_argument(
        '--pytorch_transformers_model_saved_dir_learner',
        default=os.path.join(
            ConfigurationHelper.MODEL_CACHE_DIRECTORY_STAMPED,
            'pytorch_transformers'),
        type=str,
        required=False,
        help='pytorch_transformers model cache directory.')
    parser.add_argument(
        '--pytorch_transformers_model_tokenizer',
        default='bert-base-uncased', # ---- default='bert-base-uncased', # ---- default='bert-large-uncased', # ---- default='bert-base-cased', # ---- default='bert-large-cased', # ---- default='xlnet-large-cased'
        type=str,
        required=False,
        help=f'Pytorch Transformers pre-trained model tokenizer vocabulary files: '
             f'{str(PytorchLanguageUnderstandingTransformersPretainedModelHelper.get_pretrained_vocabulary_file_keys())}.')
    parser.add_argument(
        '--pytorch_transformers_model_cache_dir_tokenizer',
        default=os.path.join(
            ConfigurationHelper.MODEL_TOKENIZER_CACHE_DIRECTORY_STAMPED,
            'pytorch_transformers_cache'),
        type=str,
        required=False,
        help='Pytorch Transformers model tokenizer cache directory.')
    parser.add_argument(
        '--pytorch_transformers_model_saved_dir_tokenizer',
        default=os.path.join(
            ConfigurationHelper.MODEL_TOKENIZER_CACHE_DIRECTORY_STAMPED,
            'pytorch_transformers'),
        type=str,
        required=False,
        help='Pytorch Transformers model tokenizer cache directory.')
    args: argparse.Namespace = parser.parse_args()
    # ------------------------------------------------------------------------
    model_key = 'xlm-mlm-ende-1024'
    config = PytorchLanguageUnderstandingTransformersPretainedModelHelper.config_from_pretrained(
        pretrained_model_name_or_path=model_key)
    DebuggingHelper.write_line_to_system_console_out(
        f'config={str(config)}')
    tokenizer = PytorchLanguageUnderstandingTransformersPretainedModelHelper.tokenizer_from_pretrained( \
        pretrained_model_name_or_path=model_key,
        cache_dir=args.pytorch_transformers_model_cache_dir_tokenizer)
    tokenizer.save_pretrained(
        os.path.join(
            args.pytorch_transformers_model_saved_dir_tokenizer,
            model_key))
    DebuggingHelper.write_line_to_system_console_out(
        f'tokenizer={str(tokenizer)}')
    model = PytorchLanguageUnderstandingTransformersPretainedModelHelper.pretrained_model_from_pretrained(
        pretrained_model_name_or_path=model_key,
        cache_dir=args.pytorch_transformers_model_cache_dir_learner)
    model.eval()
    model.save_pretrained(
        os.path.join(
            args.pytorch_transformers_model_saved_dir_learner,
            model_key,
            'pretrained_model'))
    DebuggingHelper.write_line_to_system_console_out(
        f'XLM pretrained model={str(model)}')
    model = PytorchLanguageUnderstandingTransformersPretainedModelHelper.model_from_pretrained(
        pretrained_model_name_or_path=model_key,
        cache_dir=args.pytorch_transformers_model_cache_dir_learner)
    model.eval()
    model.save_pretrained(
        os.path.join(
            args.pytorch_transformers_model_saved_dir_learner,
            model_key,
            'model'))
    DebuggingHelper.write_line_to_system_console_out(
        f'XLM model={str(model)}')
    model = PytorchLanguageUnderstandingTransformersPretainedModelHelper.sequence_classification_model_from_pretrained(
        pretrained_model_name_or_path=model_key,
        cache_dir=args.pytorch_transformers_model_cache_dir_learner)
    model.eval()
    model.save_pretrained(
        os.path.join(
            args.pytorch_transformers_model_saved_dir_learner,
            model_key,
            'sequence_classification'))
    # ---- NOTE-TODO ---- 'xlm-mlm-en-2048' can be too big to deserialize:
    # ----       File "D:\anaconda3\lib\site-packages\pytorch_transformers\modeling_utils.py", line 406, in from_pretrained
    # ----         state_dict = torch.load(resolved_archive_file, map_location='cpu')
    # ----       File "D:\anaconda3\lib\site-packages\torch\serialization.py", line 386, in load
    # ----         return _load(f, map_location, pickle_module, **pickle_load_args)
    # ----       File "D:\anaconda3\lib\site-packages\torch\serialization.py", line 580, in _load
    # ----         deserialized_objects[key]._set_from_file(f, offset, f_should_read_directly)
    # ----       OSError: [Errno 22] Invalid argument
    DebuggingHelper.write_line_to_system_console_out(
        f'XLM sequence classification model={str(model)}')
    model = PytorchLanguageUnderstandingTransformersPretainedModelHelper.token_classification_model_from_pretrained(
        pretrained_model_name_or_path=model_key,
        cache_dir=args.pytorch_transformers_model_cache_dir_learner)
    model.eval()
    model.save_pretrained(
        os.path.join(
            args.pytorch_transformers_model_saved_dir_learner,
            model_key,
            'token_classification'))
    DebuggingHelper.write_line_to_system_console_out(
        f'XLM token classification model={str(model)}')
    model = PytorchLanguageUnderstandingTransformersPretainedModelHelper.question_answering_model_from_pretrained(
        pretrained_model_name_or_path=model_key,
        cache_dir=args.pytorch_transformers_model_cache_dir_learner)
    model.eval()
    model.save_pretrained(
        os.path.join(
            args.pytorch_transformers_model_saved_dir_learner,
            model_key,
            'question_answering'))
    DebuggingHelper.write_line_to_system_console_out(
        f'XLM question answering model={str(model)}')
    optimizer_configuration = PytorchLanguageUnderstandingTransformersPretainedModelHelper.get_optimizer_scheduer_configuration(
        model=model,
        optimizer_number_training_optimization_steps=10)
    DebuggingHelper.write_line_to_system_console_out(
        f'optimizer_configuration={str(optimizer_configuration)}')

def main_test_xlnet():
    """
    The main_test_xlnet() function can quickly test PytorchLanguageUnderstandingTransformersPretainedModelHelper functions.
    """
    # ---- NOTE-PYLINT ---- R0915: Too many statements
    # pylint: disable=R0915
    # ---- NOTE-PYLINT ---- C0301: Line too long
    # pylint: disable=C0301
    pytorch_transformers_pretrained_model_keys = \
        PytorchLanguageUnderstandingTransformersPretainedModelHelper.get_pretrained_model_keys()
    DebuggingHelper.write_line_to_system_console_out(
        f'Pytorch Transformers pretrained model keys={str(pytorch_transformers_pretrained_model_keys)}')
    parser = argparse.ArgumentParser()
    parser.add_argument(
        '--pytorch_transformers_pretrained_model_key',
        default='bert-base-uncased', # ---- default='bert-base-uncased', # ---- default='bert-large-uncased', # ---- default='bert-base-cased', # ---- default='bert-large-cased', # ---- default='xlnet-large-cased'
        type=str,
        required=False,
        help=f'pytorch_transformers pre-trained model keys: '
             f'{str(pytorch_transformers_pretrained_model_keys)}.')
    parser.add_argument(
        '--pytorch_transformers_model_cache_dir_learner',
        default=os.path.join(
            ConfigurationHelper.MODEL_CACHE_DIRECTORY_STAMPED,
            'pytorch_transformers_cache'),
        type=str,
        required=False,
        help='pytorch_transformers model cache directory.')
    parser.add_argument(
        '--pytorch_transformers_model_saved_dir_learner',
        default=os.path.join(
            ConfigurationHelper.MODEL_CACHE_DIRECTORY_STAMPED,
            'pytorch_transformers'),
        type=str,
        required=False,
        help='pytorch_transformers model cache directory.')
    parser.add_argument(
        '--pytorch_transformers_model_tokenizer',
        default='bert-base-uncased', # ---- default='bert-base-uncased', # ---- default='bert-large-uncased', # ---- default='bert-base-cased', # ---- default='bert-large-cased', # ---- default='xlnet-large-cased'
        type=str,
        required=False,
        help=f'Pytorch Transformers pre-trained model tokenizer vocabulary files: '
             f'{str(PytorchLanguageUnderstandingTransformersPretainedModelHelper.get_pretrained_vocabulary_file_keys())}.')
    parser.add_argument(
        '--pytorch_transformers_model_cache_dir_tokenizer',
        default=os.path.join(
            ConfigurationHelper.MODEL_TOKENIZER_CACHE_DIRECTORY_STAMPED,
            'pytorch_transformers_cache'),
        type=str,
        required=False,
        help='Pytorch Transformers model tokenizer cache directory.')
    parser.add_argument(
        '--pytorch_transformers_model_saved_dir_tokenizer',
        default=os.path.join(
            ConfigurationHelper.MODEL_TOKENIZER_CACHE_DIRECTORY_STAMPED,
            'pytorch_transformers'),
        type=str,
        required=False,
        help='Pytorch Transformers model tokenizer cache directory.')
    args: argparse.Namespace = parser.parse_args()
    # ------------------------------------------------------------------------
    model_key = 'xlnet-base-cased'
    config = PytorchLanguageUnderstandingTransformersPretainedModelHelper.config_from_pretrained(
        pretrained_model_name_or_path=model_key)
    DebuggingHelper.write_line_to_system_console_out(
        f'config={str(config)}')
    tokenizer = PytorchLanguageUnderstandingTransformersPretainedModelHelper.tokenizer_from_pretrained( \
        pretrained_model_name_or_path=model_key,
        cache_dir=args.pytorch_transformers_model_cache_dir_tokenizer)
    tokenizer.save_pretrained(
        os.path.join(
            args.pytorch_transformers_model_saved_dir_tokenizer,
            model_key))
    DebuggingHelper.write_line_to_system_console_out(
        f'tokenizer={str(tokenizer)}')
    model = PytorchLanguageUnderstandingTransformersPretainedModelHelper.pretrained_model_from_pretrained(
        pretrained_model_name_or_path=model_key,
        cache_dir=args.pytorch_transformers_model_cache_dir_learner)
    model.eval()
    model.save_pretrained(
        os.path.join(
            args.pytorch_transformers_model_saved_dir_learner,
            model_key,
            'pretrained_model'))
    DebuggingHelper.write_line_to_system_console_out(
        f'XLNet pretrained model={str(model)}')
    model = PytorchLanguageUnderstandingTransformersPretainedModelHelper.model_from_pretrained(
        pretrained_model_name_or_path=model_key,
        cache_dir=args.pytorch_transformers_model_cache_dir_learner)
    model.eval()
    model.save_pretrained(
        os.path.join(
            args.pytorch_transformers_model_saved_dir_learner,
            model_key,
            'model'))
    DebuggingHelper.write_line_to_system_console_out(
        f'XLNet model={str(model)}')
    model = PytorchLanguageUnderstandingTransformersPretainedModelHelper.sequence_classification_model_from_pretrained(
        pretrained_model_name_or_path=model_key,
        cache_dir=args.pytorch_transformers_model_cache_dir_learner)
    model.eval()
    model.save_pretrained(
        os.path.join(
            args.pytorch_transformers_model_saved_dir_learner,
            model_key,
            'sequence_classification'))
    DebuggingHelper.write_line_to_system_console_out(
        f'XLNet sequence classification model={str(model)}')
    model = PytorchLanguageUnderstandingTransformersPretainedModelHelper.token_classification_model_from_pretrained(
        pretrained_model_name_or_path=model_key,
        cache_dir=args.pytorch_transformers_model_cache_dir_learner)
    model.eval()
    model.save_pretrained(
        os.path.join(
            args.pytorch_transformers_model_saved_dir_learner,
            model_key,
            'token_classification'))
    DebuggingHelper.write_line_to_system_console_out(
        f'XLNet token classification model={str(model)}')
    model = PytorchLanguageUnderstandingTransformersPretainedModelHelper.question_answering_model_from_pretrained(
        pretrained_model_name_or_path=model_key,
        cache_dir=args.pytorch_transformers_model_cache_dir_learner)
    model.eval()
    model.save_pretrained(
        os.path.join(
            args.pytorch_transformers_model_saved_dir_learner,
            model_key,
            'question_answering'))
    DebuggingHelper.write_line_to_system_console_out(
        f'XLNet question answering model={str(model)}')
    optimizer_configuration = PytorchLanguageUnderstandingTransformersPretainedModelHelper.get_optimizer_scheduer_configuration(
        model=model,
        optimizer_number_training_optimization_steps=10)
    DebuggingHelper.write_line_to_system_console_out(
        f'optimizer_configuration={str(optimizer_configuration)}')
    # ------------------------------------------------------------------------
    model_key = 'xlnet-large-cased'
    config = PytorchLanguageUnderstandingTransformersPretainedModelHelper.config_from_pretrained(
        pretrained_model_name_or_path=model_key)
    DebuggingHelper.write_line_to_system_console_out(
        f'config={str(config)}')
    tokenizer = PytorchLanguageUnderstandingTransformersPretainedModelHelper.tokenizer_from_pretrained( \
        pretrained_model_name_or_path=model_key,
        cache_dir=args.pytorch_transformers_model_cache_dir_tokenizer)
    tokenizer.save_pretrained(
        os.path.join(
            args.pytorch_transformers_model_saved_dir_tokenizer,
            model_key))
    DebuggingHelper.write_line_to_system_console_out(
        f'tokenizer={str(tokenizer)}')
    model = PytorchLanguageUnderstandingTransformersPretainedModelHelper.pretrained_model_from_pretrained(
        pretrained_model_name_or_path=model_key,
        cache_dir=args.pytorch_transformers_model_cache_dir_learner)
    model.eval()
    model.save_pretrained(
        os.path.join(
            args.pytorch_transformers_model_saved_dir_learner,
            model_key,
            'pretrained_model'))
    DebuggingHelper.write_line_to_system_console_out(
        f'BERT pretrained model={str(model)}')
    model = PytorchLanguageUnderstandingTransformersPretainedModelHelper.model_from_pretrained(
        pretrained_model_name_or_path=model_key,
        cache_dir=args.pytorch_transformers_model_cache_dir_learner)
    model.eval()
    model.save_pretrained(
        os.path.join(
            args.pytorch_transformers_model_saved_dir_learner,
            model_key,
            'model'))
    DebuggingHelper.write_line_to_system_console_out(
        f'XLNet model={str(model)}')
    model = PytorchLanguageUnderstandingTransformersPretainedModelHelper.sequence_classification_model_from_pretrained(
        pretrained_model_name_or_path=model_key,
        cache_dir=args.pytorch_transformers_model_cache_dir_learner)
    model.eval()
    model.save_pretrained(
        os.path.join(
            args.pytorch_transformers_model_saved_dir_learner,
            model_key,
            'sequence_classification'))
    DebuggingHelper.write_line_to_system_console_out(
        f'XLNet sequence classification model={str(model)}')
    model = PytorchLanguageUnderstandingTransformersPretainedModelHelper.token_classification_model_from_pretrained(
        pretrained_model_name_or_path=model_key,
        cache_dir=args.pytorch_transformers_model_cache_dir_learner)
    model.eval()
    model.save_pretrained(
        os.path.join(
            args.pytorch_transformers_model_saved_dir_learner,
            model_key,
            'token_classification'))
    DebuggingHelper.write_line_to_system_console_out(
        f'XLNet token classification model={str(model)}')
    model = PytorchLanguageUnderstandingTransformersPretainedModelHelper.question_answering_model_from_pretrained(
        pretrained_model_name_or_path=model_key,
        cache_dir=args.pytorch_transformers_model_cache_dir_learner)
    model.eval()
    model.save_pretrained(
        os.path.join(
            args.pytorch_transformers_model_saved_dir_learner,
            model_key,
            'question_answering'))
    DebuggingHelper.write_line_to_system_console_out(
        f'XLNet question answering model={str(model)}')
    optimizer_configuration = PytorchLanguageUnderstandingTransformersPretainedModelHelper.get_optimizer_scheduer_configuration(
        model=model,
        optimizer_number_training_optimization_steps=10)
    DebuggingHelper.write_line_to_system_console_out(
        f'optimizer_configuration={str(optimizer_configuration)}')
    # ------------------------------------------------------------------------

if __name__ == '__main__':
    # main_test_model_tokenizer_bert_base_new()
    # main_test_tokenizer_bert_base_uncased()
    # main_test_model_tokenizer_roberta_base_new()
    # main_test_tokenizer_roberta_base_new()
    # main_test_tokenizer_roberta_base()
    # main_test_get_model_named_parameters_bert()
    main_test_bert()
    # main_test_roberta()
    # main_test_xlm()
    # main_test_xlnet()

    # ---- NOTE ---- main_test_model_tokenizer_roberta_base_new():
    # ---- NOTE ----
    # ---- NOTE ----     STDOUT-INFO: [2020-07-08T08:40:09.091616][main_test_model_tokenizer_roberta_base_new @ app_pytorch_language_understanding_transformers_helper.py:182][10]: DTE model=RobertaModel(
    # ---- NOTE ----       (embeddings): RobertaEmbeddings(
    # ---- NOTE ----         (word_embeddings): Embedding(50265, 768, padding_idx=1)
    # ---- NOTE ----         (position_embeddings): Embedding(514, 768, padding_idx=1)
    # ---- NOTE ----         (token_type_embeddings): Embedding(1, 768)
    # ---- NOTE ----         (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    # ---- NOTE ----         (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----       )
    # ---- NOTE ----       (encoder): BertEncoder(
    # ---- NOTE ----         (layer): ModuleList(
    # ---- NOTE ----           (0): BertLayer(
    # ---- NOTE ----             (attention): BertAttention(
    # ---- NOTE ----               (self): BertSelfAttention(
    # ---- NOTE ----                 (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                 (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                 (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertSelfOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (intermediate): BertIntermediate(
    # ---- NOTE ----               (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----             )
    # ---- NOTE ----             (output): BertOutput(
    # ---- NOTE ----               (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----               (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----           )
    # ---- NOTE ----           (1): BertLayer(
    # ---- NOTE ----             (attention): BertAttention(
    # ---- NOTE ----               (self): BertSelfAttention(
    # ---- NOTE ----                 (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                 (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                 (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertSelfOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (intermediate): BertIntermediate(
    # ---- NOTE ----               (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----             )
    # ---- NOTE ----             (output): BertOutput(
    # ---- NOTE ----               (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----               (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----           )
    # ---- NOTE ----           (2): BertLayer(
    # ---- NOTE ----             (attention): BertAttention(
    # ---- NOTE ----               (self): BertSelfAttention(
    # ---- NOTE ----                 (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                 (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                 (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertSelfOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (intermediate): BertIntermediate(
    # ---- NOTE ----               (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----             )
    # ---- NOTE ----             (output): BertOutput(
    # ---- NOTE ----               (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----               (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----           )
    # ---- NOTE ----         )
    # ---- NOTE ----       )
    # ---- NOTE ----       (pooler): BertPooler(
    # ---- NOTE ----         (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----         (activation): Tanh()
    # ---- NOTE ----       )
    # ---- NOTE ----     )

    # ---- NOTE ---- main_test_roberta():
    # ---- NOTE ----
    # ---- NOTE ----     STDOUT-INFO: [2020-07-08T01:09:10.693557][main_test_roberta @ app_pytorch_language_understanding_transformers_helper.py:894][10]: b"Pytorch Transformers pretrained model keys=['bert-base-uncased', 'bert-large-uncased', 'bert-base-cased', 'bert-large-cased', 'bert-base-multilingual-uncased', 'bert-base-multilingual-cased', 'bert-base-chinese', 'bert-base-german-cased', 'bert-large-uncased-whole-word-masking', 'bert-large-cased-whole-word-masking', 'bert-large-uncased-whole-word-masking-finetuned-squad', 'bert-large-cased-whole-word-masking-finetuned-squad', 'bert-base-cased-finetuned-mrpc', 'bert-base-german-dbmdz-cased', 'bert-base-german-dbmdz-uncased', 'bert-base-japanese', 'bert-base-japanese-whole-word-masking', 'bert-base-japanese-char', 'bert-base-japanese-char-whole-word-masking', 'bert-base-finnish-cased-v1', 'bert-base-finnish-uncased-v1', 'bert-base-dutch-cased', 'roberta-base', 'roberta-large', 'roberta-large-mnli', 'distilroberta-base', 'roberta-base-openai-detector', 'roberta-large-openai-detector', 'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl', 'distilgpt2', 'openai-gpt', 'transfo-xl-wt103', 'xlm-mlm-en-2048', 'xlm-mlm-ende-1024', 'xlm-mlm-enfr-1024', 'xlm-mlm-enro-1024', 'xlm-mlm-tlm-xnli15-1024', 'xlm-mlm-xnli15-1024', 'xlm-clm-enfr-1024', 'xlm-clm-ende-1024', 'xlm-mlm-17-1280', 'xlm-mlm-100-1280', 'xlnet-base-cased', 'xlnet-large-cased']"
    # ---- NOTE ----     STDOUT-INFO: [2020-07-08T01:09:11.227849][main_test_roberta @ app_pytorch_language_understanding_transformers_helper.py:943][10]: config=RobertaConfig {
    # ---- NOTE ----       "_num_labels": 2,
    # ---- NOTE ----       "architectures": [
    # ---- NOTE ----         "RobertaForMaskedLM"
    # ---- NOTE ----       ],
    # ---- NOTE ----       "attention_probs_dropout_prob": 0.1,
    # ---- NOTE ----       "bad_words_ids": null,
    # ---- NOTE ----       "bos_token_id": 0,
    # ---- NOTE ----       "decoder_start_token_id": null,
    # ---- NOTE ----       "do_sample": false,
    # ---- NOTE ----       "early_stopping": false,
    # ---- NOTE ----       "eos_token_id": 2,
    # ---- NOTE ----       "finetuning_task": null,
    # ---- NOTE ----       "hidden_act": "gelu",
    # ---- NOTE ----       "hidden_dropout_prob": 0.1,
    # ---- NOTE ----       "hidden_size": 768,
    # ---- NOTE ----       "id2label": {
    # ---- NOTE ----         "0": "LABEL_0",
    # ---- NOTE ----         "1": "LABEL_1"
    # ---- NOTE ----       },
    # ---- NOTE ----       "initializer_range": 0.02,
    # ---- NOTE ----       "intermediate_size": 3072,
    # ---- NOTE ----       "is_decoder": false,
    # ---- NOTE ----       "is_encoder_decoder": false,
    # ---- NOTE ----       "label2id": {
    # ---- NOTE ----         "LABEL_0": 0,
    # ---- NOTE ----         "LABEL_1": 1
    # ---- NOTE ----       },
    # ---- NOTE ----       "layer_norm_eps": 1e-05,
    # ---- NOTE ----       "length_penalty": 1.0,
    # ---- NOTE ----       "max_length": 20,
    # ---- NOTE ----       "max_position_embeddings": 514,
    # ---- NOTE ----       "min_length": 0,
    # ---- NOTE ----       "model_type": "roberta",
    # ---- NOTE ----       "no_repeat_ngram_size": 0,
    # ---- NOTE ----       "num_attention_heads": 12,
    # ---- NOTE ----       "num_beams": 1,
    # ---- NOTE ----       "num_hidden_layers": 12,
    # ---- NOTE ----       "num_return_sequences": 1,
    # ---- NOTE ----       "output_attentions": false,
    # ---- NOTE ----       "output_hidden_states": false,
    # ---- NOTE ----       "output_past": true,
    # ---- NOTE ----       "pad_token_id": 1,
    # ---- NOTE ----       "prefix": null,
    # ---- NOTE ----       "pruned_heads": {},
    # ---- NOTE ----       "repetition_penalty": 1.0,
    # ---- NOTE ----       "task_specific_params": null,
    # ---- NOTE ----       "temperature": 1.0,
    # ---- NOTE ----       "top_k": 50,
    # ---- NOTE ----       "top_p": 1.0,
    # ---- NOTE ----       "torchscript": false,
    # ---- NOTE ----       "type_vocab_size": 1,
    # ---- NOTE ----       "use_bfloat16": false,
    # ---- NOTE ----       "vocab_size": 50265
    # ---- NOTE ----     }
    # ---- NOTE ----
    # ---- NOTE ----     Downloading: 100%|| 899k/899k [00:00<00:00, 1.71MB/s]
    # ---- NOTE ----     Downloading: 100%|| 456k/456k [00:00<00:00, 1.04MB/s]
    # ---- NOTE ----     STDOUT-INFO: [2020-07-08T01:09:14.193242][main_test_roberta @ app_pytorch_language_understanding_transformers_helper.py:949][10]: tokenizer=<transformers.tokenization_roberta.RobertaTokenizer object at 0x000001C1D34CEEC8>
    # ---- NOTE ----     Downloading: 100%|| 481/481 [00:00<00:00, 166kB/s]
    # ---- NOTE ----     Downloading: 100%|| 501M/501M [01:03<00:00, 7.91MB/s]
    # ---- NOTE ----     STDOUT-INFO: [2020-07-08T01:10:27.351313][main_test_roberta @ app_pytorch_language_understanding_transformers_helper.py:956][10]: ROBERTA sequence classification model=RobertaForSequenceClassification(
    # ---- NOTE ----       (roberta): RobertaModel(
    # ---- NOTE ----         (embeddings): RobertaEmbeddings(
    # ---- NOTE ----           (word_embeddings): Embedding(50265, 768, padding_idx=1)
    # ---- NOTE ----           (position_embeddings): Embedding(514, 768, padding_idx=1)
    # ---- NOTE ----           (token_type_embeddings): Embedding(1, 768)
    # ---- NOTE ----           (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    # ---- NOTE ----           (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----         )
    # ---- NOTE ----         (encoder): BertEncoder(
    # ---- NOTE ----           (layer): ModuleList(
    # ---- NOTE ----             (0): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (1): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (2): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (3): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (4): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (5): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (6): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (7): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (8): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (9): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (10): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (11): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----           )
    # ---- NOTE ----         )
    # ---- NOTE ----         (pooler): BertPooler(
    # ---- NOTE ----           (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----           (activation): Tanh()
    # ---- NOTE ----         )
    # ---- NOTE ----       )
    # ---- NOTE ----       (classifier): RobertaClassificationHead(
    # ---- NOTE ----         (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----         (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----         (out_proj): Linear(in_features=768, out_features=2, bias=True)
    # ---- NOTE ----       )
    # ---- NOTE ----     )
    # ---- NOTE ----     STDOUT-INFO: [2020-07-08T01:10:27.374402][main_test_roberta @ app_pytorch_language_understanding_transformers_helper.py:968][10]: optimizer_configuration={'optimizer': AdamW (
    # ---- NOTE ----     Parameter Group 0
    # ---- NOTE ----         betas: (0.9, 0.999)
    # ---- NOTE ----         correct_bias: True
    # ---- NOTE ----         eps: 1e-06
    # ---- NOTE ----         lr: 5e-05
    # ---- NOTE ----         weight_decay: 0.0
    # ---- NOTE ----
    # ---- NOTE ----     Parameter Group 1
    # ---- NOTE ----         betas: (0.9, 0.999)
    # ---- NOTE ----         correct_bias: True
    # ---- NOTE ----         eps: 1e-06
    # ---- NOTE ----         lr: 5e-05
    # ---- NOTE ----         weight_decay: 0.0
    # ---- NOTE ----     ), 'optimizer_number_training_optimization_steps': 10, 'device_use_gpu_fp16': False, 'optimizer_learning_rate': 5e-05, 'optimizer_warmup_proportion': 0.1, 'optimizer_warmup_steps': 0, 'optimizer_adam_epsilon': 1e-06, 'optimizer_weight_decay': 0.0, 'device_gpu_fp16_opt_level': None}

    # ---- NOTE ---- main_test_all()
    # ---- NOTE ----
    # ---- NOTE ----     (base) D:\git\EmbedML\private\hunyang\project\LanguageUnderstandingOpenSource\src\python\model\language_understanding\helper>python app_pytorch_language_understanding_transformers_helper.py
    # ---- NOTE ----     STDOUT-INFO: [2019-08-13T14:43:30.705032][main @ app_pytorch_language_understanding_transformers_helper.py:31][10]: Pytorch Transformers pretrained model keys=['bert-base-uncased', 'bert-large-uncased', 'bert-base-cased', 'bert-large-cased', 'bert-base-multilingual-uncased', 'bert-base-multilingual-cased', 'bert-base-chinese', 'bert-base-german-cased', 'bert-large-uncased-whole-word-masking', 'bert-large-cased-whole-word-masking', 'bert-large-uncased-whole-word-masking-finetuned-squad', 'bert-large-cased-whole-word-masking-finetuned-squad', 'bert-base-cased-finetuned-mrpc', 'gpt2', 'gpt2-medium', 'openai-gpt', 'transfo-xl-wt103', 'xlm-mlm-en-2048', 'xlm-mlm-ende-1024', 'xlm-mlm-enfr-1024', 'xlm-mlm-enro-1024', 'xlm-mlm-tlm-xnli15-1024', 'xlm-mlm-xnli15-1024', 'xlm-clm-enfr-1024', 'xlm-clm-ende-1024', 'xlnet-base-cased', 'xlnet-large-cased']
    # ---- NOTE ----     STDOUT-INFO: [2019-08-13T14:43:31.091574][main @ app_pytorch_language_understanding_transformers_helper.py:77][10]: config={
    # ---- NOTE ----       "attention_probs_dropout_prob": 0.1,
    # ---- NOTE ----       "finetuning_task": null,
    # ---- NOTE ----       "hidden_act": "gelu",
    # ---- NOTE ----       "hidden_dropout_prob": 0.1,
    # ---- NOTE ----       "hidden_size": 768,
    # ---- NOTE ----       "initializer_range": 0.02,
    # ---- NOTE ----       "intermediate_size": 3072,
    # ---- NOTE ----       "layer_norm_eps": 1e-12,
    # ---- NOTE ----       "max_position_embeddings": 512,
    # ---- NOTE ----       "num_attention_heads": 12,
    # ---- NOTE ----       "num_hidden_layers": 12,
    # ---- NOTE ----       "num_labels": 2,
    # ---- NOTE ----       "output_attentions": false,
    # ---- NOTE ----       "output_hidden_states": false,
    # ---- NOTE ----       "torchscript": false,
    # ---- NOTE ----       "type_vocab_size": 2,
    # ---- NOTE ----       "vocab_size": 28996
    # ---- NOTE ----     }
    # ---- NOTE ----
    # ---- NOTE ----     The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.
    # ---- NOTE ----     STDOUT-INFO: [2019-08-13T14:43:31.447572][main @ app_pytorch_language_understanding_transformers_helper.py:81][10]: tokenizer=<pytorch_transformers.tokenization_bert.BertTokenizer object at 0x000001A4448CB5F8>
    # ---- NOTE ----     100%|| 435779157/435779157 [00:21<00:00, 20714513.56B/s]
    # ---- NOTE ----     STDOUT-INFO: [2019-08-13T14:43:56.935353][main @ app_pytorch_language_understanding_transformers_helper.py:87][10]: BERT sequence classification model=BertForSequenceClassification(
    # ---- NOTE ----       (bert): BertModel(
    # ---- NOTE ----         (embeddings): BertEmbeddings(
    # ---- NOTE ----           (word_embeddings): Embedding(28996, 768, padding_idx=0)
    # ---- NOTE ----           (position_embeddings): Embedding(512, 768)
    # ---- NOTE ----           (token_type_embeddings): Embedding(2, 768)
    # ---- NOTE ----           (LayerNorm): BertLayerNorm()
    # ---- NOTE ----           (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----         )
    # ---- NOTE ----         (encoder): BertEncoder(
    # ---- NOTE ----           (layer): ModuleList(
    # ---- NOTE ----             (0): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (1): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (2): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (3): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (4): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (5): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (6): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (7): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (8): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (9): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (10): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (11): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----           )
    # ---- NOTE ----         )
    # ---- NOTE ----         (pooler): BertPooler(
    # ---- NOTE ----           (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----           (activation): Tanh()
    # ---- NOTE ----         )
    # ---- NOTE ----       )
    # ---- NOTE ----       (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----       (classifier): Linear(in_features=768, out_features=2, bias=True)
    # ---- NOTE ----     )
    # ---- NOTE ----     STDOUT-INFO: [2019-08-13T14:44:00.482352][main @ app_pytorch_language_understanding_transformers_helper.py:93][10]: BERT question answering model=BertForQuestionAnswering(
    # ---- NOTE ----       (bert): BertModel(
    # ---- NOTE ----         (embeddings): BertEmbeddings(
    # ---- NOTE ----           (word_embeddings): Embedding(28996, 768, padding_idx=0)
    # ---- NOTE ----           (position_embeddings): Embedding(512, 768)
    # ---- NOTE ----           (token_type_embeddings): Embedding(2, 768)
    # ---- NOTE ----           (LayerNorm): BertLayerNorm()
    # ---- NOTE ----           (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----         )
    # ---- NOTE ----         (encoder): BertEncoder(
    # ---- NOTE ----           (layer): ModuleList(
    # ---- NOTE ----             (0): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (1): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (2): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (3): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (4): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (5): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (6): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (7): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (8): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (9): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (10): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (11): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----           )
    # ---- NOTE ----         )
    # ---- NOTE ----         (pooler): BertPooler(
    # ---- NOTE ----           (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----           (activation): Tanh()
    # ---- NOTE ----         )
    # ---- NOTE ----       )
    # ---- NOTE ----       (qa_outputs): Linear(in_features=768, out_features=2, bias=True)
    # ---- NOTE ----     )
    # ---- NOTE ----     STDOUT-INFO: [2019-08-13T14:44:00.492352][main @ app_pytorch_language_understanding_transformers_helper.py:98][10]: optimizer_configuration={'optimizer': AdamW (
    # ---- NOTE ----     Parameter Group 0
    # ---- NOTE ----         betas: (0.9, 0.999)
    # ---- NOTE ----         correct_bias: True
    # ---- NOTE ----         eps: 1e-06
    # ---- NOTE ----         initial_lr: 5e-05
    # ---- NOTE ----         lr: 5e-05
    # ---- NOTE ----         weight_decay: 0.0
    # ---- NOTE ----
    # ---- NOTE ----     Parameter Group 1
    # ---- NOTE ----         betas: (0.9, 0.999)
    # ---- NOTE ----         correct_bias: True
    # ---- NOTE ----         eps: 1e-06
    # ---- NOTE ----         initial_lr: 5e-05
    # ---- NOTE ----         lr: 5e-05
    # ---- NOTE ----         weight_decay: 0.0
    # ---- NOTE ----     ), 'scheduler': <pytorch_transformers.optimization.WarmupLinearSchedule object at 0x000001A4448CBDA0>, 'optimizer_number_training_optimization_steps': 10, 'device_use_gpu_fp16': False, 'optimizer_learning_rate': 5e-05, 'optimizer_warmup_proportion': 0.1, 'optimizer_warmup_steps': 0, 'optimizer_adam_epsilon': 1e-06, 'optimizer_weight_decay': 0.0, 'device_gpu_fp16_opt_level': None}
    # ---- NOTE ----     STDOUT-INFO: [2019-08-13T14:44:00.827352][main @ app_pytorch_language_understanding_transformers_helper.py:104][10]: config={
    # ---- NOTE ----       "attention_probs_dropout_prob": 0.1,
    # ---- NOTE ----       "finetuning_task": null,
    # ---- NOTE ----       "hidden_act": "gelu",
    # ---- NOTE ----       "hidden_dropout_prob": 0.1,
    # ---- NOTE ----       "hidden_size": 768,
    # ---- NOTE ----       "initializer_range": 0.02,
    # ---- NOTE ----       "intermediate_size": 3072,
    # ---- NOTE ----       "layer_norm_eps": 1e-12,
    # ---- NOTE ----       "max_position_embeddings": 512,
    # ---- NOTE ----       "num_attention_heads": 12,
    # ---- NOTE ----       "num_hidden_layers": 12,
    # ---- NOTE ----       "num_labels": 2,
    # ---- NOTE ----       "output_attentions": false,
    # ---- NOTE ----       "output_hidden_states": false,
    # ---- NOTE ----       "torchscript": false,
    # ---- NOTE ----       "type_vocab_size": 2,
    # ---- NOTE ----       "vocab_size": 30522
    # ---- NOTE ----     }
    # ---- NOTE ----
    # ---- NOTE ----     STDOUT-INFO: [2019-08-13T14:44:01.259091][main @ app_pytorch_language_understanding_transformers_helper.py:108][10]: tokenizer=<pytorch_transformers.tokenization_bert.BertTokenizer object at 0x000001A444891C88>
    # ---- NOTE ----     100%|| 440473133/440473133 [00:16<00:00, 26090131.24B/s]
    # ---- NOTE ----     STDOUT-INFO: [2019-08-13T14:44:22.684742][main @ app_pytorch_language_understanding_transformers_helper.py:114][10]: BERT sequence classification model=BertForSequenceClassification(
    # ---- NOTE ----       (bert): BertModel(
    # ---- NOTE ----         (embeddings): BertEmbeddings(
    # ---- NOTE ----           (word_embeddings): Embedding(30522, 768, padding_idx=0)
    # ---- NOTE ----           (position_embeddings): Embedding(512, 768)
    # ---- NOTE ----           (token_type_embeddings): Embedding(2, 768)
    # ---- NOTE ----           (LayerNorm): BertLayerNorm()
    # ---- NOTE ----           (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----         )
    # ---- NOTE ----         (encoder): BertEncoder(
    # ---- NOTE ----           (layer): ModuleList(
    # ---- NOTE ----             (0): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (1): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (2): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (3): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (4): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (5): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (6): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (7): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (8): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (9): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (10): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (11): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----           )
    # ---- NOTE ----         )
    # ---- NOTE ----         (pooler): BertPooler(
    # ---- NOTE ----           (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----           (activation): Tanh()
    # ---- NOTE ----         )
    # ---- NOTE ----       )
    # ---- NOTE ----       (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----       (classifier): Linear(in_features=768, out_features=2, bias=True)
    # ---- NOTE ----     )
    # ---- NOTE ----     STDOUT-INFO: [2019-08-13T14:44:26.438245][main @ app_pytorch_language_understanding_transformers_helper.py:120][10]: BERT question answering model=BertForQuestionAnswering(
    # ---- NOTE ----       (bert): BertModel(
    # ---- NOTE ----         (embeddings): BertEmbeddings(
    # ---- NOTE ----           (word_embeddings): Embedding(30522, 768, padding_idx=0)
    # ---- NOTE ----           (position_embeddings): Embedding(512, 768)
    # ---- NOTE ----           (token_type_embeddings): Embedding(2, 768)
    # ---- NOTE ----           (LayerNorm): BertLayerNorm()
    # ---- NOTE ----           (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----         )
    # ---- NOTE ----         (encoder): BertEncoder(
    # ---- NOTE ----           (layer): ModuleList(
    # ---- NOTE ----             (0): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (1): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (2): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (3): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (4): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (5): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (6): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (7): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (8): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (9): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (10): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (11): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----           )
    # ---- NOTE ----         )
    # ---- NOTE ----         (pooler): BertPooler(
    # ---- NOTE ----           (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----           (activation): Tanh()
    # ---- NOTE ----         )
    # ---- NOTE ----       )
    # ---- NOTE ----       (qa_outputs): Linear(in_features=768, out_features=2, bias=True)
    # ---- NOTE ----     )
    # ---- NOTE ----     STDOUT-INFO: [2019-08-13T14:44:26.447245][main @ app_pytorch_language_understanding_transformers_helper.py:125][10]: optimizer_configuration={'optimizer': AdamW (
    # ---- NOTE ----     Parameter Group 0
    # ---- NOTE ----         betas: (0.9, 0.999)
    # ---- NOTE ----         correct_bias: True
    # ---- NOTE ----         eps: 1e-06
    # ---- NOTE ----         initial_lr: 5e-05
    # ---- NOTE ----         lr: 5e-05
    # ---- NOTE ----         weight_decay: 0.0
    # ---- NOTE ----
    # ---- NOTE ----     Parameter Group 1
    # ---- NOTE ----         betas: (0.9, 0.999)
    # ---- NOTE ----         correct_bias: True
    # ---- NOTE ----         eps: 1e-06
    # ---- NOTE ----         initial_lr: 5e-05
    # ---- NOTE ----         lr: 5e-05
    # ---- NOTE ----         weight_decay: 0.0
    # ---- NOTE ----     ), 'scheduler': <pytorch_transformers.optimization.WarmupLinearSchedule object at 0x000001A43FC5DA90>, 'optimizer_number_training_optimization_steps': 10, 'device_use_gpu_fp16': False, 'optimizer_learning_rate': 5e-05, 'optimizer_warmup_proportion': 0.1, 'optimizer_warmup_steps': 0, 'optimizer_adam_epsilon': 1e-06, 'optimizer_weight_decay': 0.0, 'device_gpu_fp16_opt_level': None}
    # ---- NOTE ----     100%|| 521/521 [00:00<00:00, 505957.95B/s]
    # ---- NOTE ----     STDOUT-INFO: [2019-08-13T14:44:27.342183][main @ app_pytorch_language_understanding_transformers_helper.py:131][10]: config={
    # ---- NOTE ----       "attention_probs_dropout_prob": 0.1,
    # ---- NOTE ----       "directionality": "bidi",
    # ---- NOTE ----       "finetuning_task": null,
    # ---- NOTE ----       "hidden_act": "gelu",
    # ---- NOTE ----       "hidden_dropout_prob": 0.1,
    # ---- NOTE ----       "hidden_size": 768,
    # ---- NOTE ----       "initializer_range": 0.02,
    # ---- NOTE ----       "intermediate_size": 3072,
    # ---- NOTE ----       "layer_norm_eps": 1e-12,
    # ---- NOTE ----       "max_position_embeddings": 512,
    # ---- NOTE ----       "num_attention_heads": 12,
    # ---- NOTE ----       "num_hidden_layers": 12,
    # ---- NOTE ----       "num_labels": 2,
    # ---- NOTE ----       "output_attentions": false,
    # ---- NOTE ----       "output_hidden_states": false,
    # ---- NOTE ----       "pooler_fc_size": 768,
    # ---- NOTE ----       "pooler_num_attention_heads": 12,
    # ---- NOTE ----       "pooler_num_fc_layers": 3,
    # ---- NOTE ----       "pooler_size_per_head": 128,
    # ---- NOTE ----       "pooler_type": "first_token_transform",
    # ---- NOTE ----       "torchscript": false,
    # ---- NOTE ----       "type_vocab_size": 2,
    # ---- NOTE ----       "vocab_size": 119547
    # ---- NOTE ----     }
    # ---- NOTE ----
    # ---- NOTE ----     The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.
    # ---- NOTE ----     100%|| 995526/995526 [00:00<00:00, 2070759.09B/s]
    # ---- NOTE ----     STDOUT-INFO: [2019-08-13T14:44:28.853095][main @ app_pytorch_language_understanding_transformers_helper.py:135][10]: tokenizer=<pytorch_transformers.tokenization_bert.BertTokenizer object at 0x000001A444E6C048>
    # ---- NOTE ----     100%|| 714314041/714314041 [00:29<00:00, 23990566.13B/s]
    # ---- NOTE ----     STDOUT-INFO: [2019-08-13T14:45:05.492183][main @ app_pytorch_language_understanding_transformers_helper.py:141][10]: BERT sequence classification model=BertForSequenceClassification(
    # ---- NOTE ----       (bert): BertModel(
    # ---- NOTE ----         (embeddings): BertEmbeddings(
    # ---- NOTE ----           (word_embeddings): Embedding(119547, 768, padding_idx=0)
    # ---- NOTE ----           (position_embeddings): Embedding(512, 768)
    # ---- NOTE ----           (token_type_embeddings): Embedding(2, 768)
    # ---- NOTE ----           (LayerNorm): BertLayerNorm()
    # ---- NOTE ----           (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----         )
    # ---- NOTE ----         (encoder): BertEncoder(
    # ---- NOTE ----           (layer): ModuleList(
    # ---- NOTE ----             (0): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (1): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (2): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (3): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (4): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (5): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (6): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (7): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (8): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (9): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (10): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (11): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----           )
    # ---- NOTE ----         )
    # ---- NOTE ----         (pooler): BertPooler(
    # ---- NOTE ----           (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----           (activation): Tanh()
    # ---- NOTE ----         )
    # ---- NOTE ----       )
    # ---- NOTE ----       (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----       (classifier): Linear(in_features=768, out_features=2, bias=True)
    # ---- NOTE ----     )
    # ---- NOTE ----     STDOUT-INFO: [2019-08-13T14:45:10.598172][main @ app_pytorch_language_understanding_transformers_helper.py:147][10]: BERT question answering model=BertForQuestionAnswering(
    # ---- NOTE ----       (bert): BertModel(
    # ---- NOTE ----         (embeddings): BertEmbeddings(
    # ---- NOTE ----           (word_embeddings): Embedding(119547, 768, padding_idx=0)
    # ---- NOTE ----           (position_embeddings): Embedding(512, 768)
    # ---- NOTE ----           (token_type_embeddings): Embedding(2, 768)
    # ---- NOTE ----           (LayerNorm): BertLayerNorm()
    # ---- NOTE ----           (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----         )
    # ---- NOTE ----         (encoder): BertEncoder(
    # ---- NOTE ----           (layer): ModuleList(
    # ---- NOTE ----             (0): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (1): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (2): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (3): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (4): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (5): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (6): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (7): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (8): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (9): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (10): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (11): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----           )
    # ---- NOTE ----         )
    # ---- NOTE ----         (pooler): BertPooler(
    # ---- NOTE ----           (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----           (activation): Tanh()
    # ---- NOTE ----         )
    # ---- NOTE ----       )
    # ---- NOTE ----       (qa_outputs): Linear(in_features=768, out_features=2, bias=True)
    # ---- NOTE ----     )
    # ---- NOTE ----     STDOUT-INFO: [2019-08-13T14:45:10.606172][main @ app_pytorch_language_understanding_transformers_helper.py:152][10]: optimizer_configuration={'optimizer': AdamW (
    # ---- NOTE ----     Parameter Group 0
    # ---- NOTE ----         betas: (0.9, 0.999)
    # ---- NOTE ----         correct_bias: True
    # ---- NOTE ----         eps: 1e-06
    # ---- NOTE ----         initial_lr: 5e-05
    # ---- NOTE ----         lr: 5e-05
    # ---- NOTE ----         weight_decay: 0.0
    # ---- NOTE ----
    # ---- NOTE ----     Parameter Group 1
    # ---- NOTE ----         betas: (0.9, 0.999)
    # ---- NOTE ----         correct_bias: True
    # ---- NOTE ----         eps: 1e-06
    # ---- NOTE ----         initial_lr: 5e-05
    # ---- NOTE ----         lr: 5e-05
    # ---- NOTE ----         weight_decay: 0.0
    # ---- NOTE ----     ), 'scheduler': <pytorch_transformers.optimization.WarmupLinearSchedule object at 0x000001A43FCE0400>, 'optimizer_number_training_optimization_steps': 10, 'device_use_gpu_fp16': False, 'optimizer_learning_rate': 5e-05, 'optimizer_warmup_proportion': 0.1, 'optimizer_warmup_steps': 0, 'optimizer_adam_epsilon': 1e-06, 'optimizer_weight_decay': 0.0, 'device_gpu_fp16_opt_level': None}
    # ---- NOTE ----     100%|| 521/521 [00:00<00:00, 173734.49B/s]
    # ---- NOTE ----     STDOUT-INFO: [2019-08-13T14:45:11.491164][main @ app_pytorch_language_understanding_transformers_helper.py:158][10]: config={
    # ---- NOTE ----       "attention_probs_dropout_prob": 0.1,
    # ---- NOTE ----       "directionality": "bidi",
    # ---- NOTE ----       "finetuning_task": null,
    # ---- NOTE ----       "hidden_act": "gelu",
    # ---- NOTE ----       "hidden_dropout_prob": 0.1,
    # ---- NOTE ----       "hidden_size": 768,
    # ---- NOTE ----       "initializer_range": 0.02,
    # ---- NOTE ----       "intermediate_size": 3072,
    # ---- NOTE ----       "layer_norm_eps": 1e-12,
    # ---- NOTE ----       "max_position_embeddings": 512,
    # ---- NOTE ----       "num_attention_heads": 12,
    # ---- NOTE ----       "num_hidden_layers": 12,
    # ---- NOTE ----       "num_labels": 2,
    # ---- NOTE ----       "output_attentions": false,
    # ---- NOTE ----       "output_hidden_states": false,
    # ---- NOTE ----       "pooler_fc_size": 768,
    # ---- NOTE ----       "pooler_num_attention_heads": 12,
    # ---- NOTE ----       "pooler_num_fc_layers": 3,
    # ---- NOTE ----       "pooler_size_per_head": 128,
    # ---- NOTE ----       "pooler_type": "first_token_transform",
    # ---- NOTE ----       "torchscript": false,
    # ---- NOTE ----       "type_vocab_size": 2,
    # ---- NOTE ----       "vocab_size": 105879
    # ---- NOTE ----     }
    # ---- NOTE ----
    # ---- NOTE ----     100%|| 871891/871891 [00:00<00:00, 1891205.04B/s]
    # ---- NOTE ----     STDOUT-INFO: [2019-08-13T14:45:13.127250][main @ app_pytorch_language_understanding_transformers_helper.py:162][10]: tokenizer=<pytorch_transformers.tokenization_bert.BertTokenizer object at 0x000001A43FC5D080>
    # ---- NOTE ----     100%|| 672271273/672271273 [00:39<00:00, 17113452.12B/s]
    # ---- NOTE ----     STDOUT-INFO: [2019-08-13T14:45:58.952569][main @ app_pytorch_language_understanding_transformers_helper.py:168][10]: BERT sequence classification model=BertForSequenceClassification(
    # ---- NOTE ----       (bert): BertModel(
    # ---- NOTE ----         (embeddings): BertEmbeddings(
    # ---- NOTE ----           (word_embeddings): Embedding(105879, 768, padding_idx=0)
    # ---- NOTE ----           (position_embeddings): Embedding(512, 768)
    # ---- NOTE ----           (token_type_embeddings): Embedding(2, 768)
    # ---- NOTE ----           (LayerNorm): BertLayerNorm()
    # ---- NOTE ----           (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----         )
    # ---- NOTE ----         (encoder): BertEncoder(
    # ---- NOTE ----           (layer): ModuleList(
    # ---- NOTE ----             (0): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (1): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (2): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (3): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (4): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (5): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (6): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (7): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (8): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (9): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (10): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (11): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----           )
    # ---- NOTE ----         )
    # ---- NOTE ----         (pooler): BertPooler(
    # ---- NOTE ----           (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----           (activation): Tanh()
    # ---- NOTE ----         )
    # ---- NOTE ----       )
    # ---- NOTE ----       (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----       (classifier): Linear(in_features=768, out_features=2, bias=True)
    # ---- NOTE ----     )
    # ---- NOTE ----     STDOUT-INFO: [2019-08-13T14:46:03.895122][main @ app_pytorch_language_understanding_transformers_helper.py:174][10]: BERT question answering model=BertForQuestionAnswering(
    # ---- NOTE ----       (bert): BertModel(
    # ---- NOTE ----         (embeddings): BertEmbeddings(
    # ---- NOTE ----           (word_embeddings): Embedding(105879, 768, padding_idx=0)
    # ---- NOTE ----           (position_embeddings): Embedding(512, 768)
    # ---- NOTE ----           (token_type_embeddings): Embedding(2, 768)
    # ---- NOTE ----           (LayerNorm): BertLayerNorm()
    # ---- NOTE ----           (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----         )
    # ---- NOTE ----         (encoder): BertEncoder(
    # ---- NOTE ----           (layer): ModuleList(
    # ---- NOTE ----             (0): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (1): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (2): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (3): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (4): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (5): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (6): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (7): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (8): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (9): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (10): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (11): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----           )
    # ---- NOTE ----         )
    # ---- NOTE ----         (pooler): BertPooler(
    # ---- NOTE ----           (dense): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----           (activation): Tanh()
    # ---- NOTE ----         )
    # ---- NOTE ----       )
    # ---- NOTE ----       (qa_outputs): Linear(in_features=768, out_features=2, bias=True)
    # ---- NOTE ----     )
    # ---- NOTE ----     STDOUT-INFO: [2019-08-13T14:46:03.903124][main @ app_pytorch_language_understanding_transformers_helper.py:179][10]: optimizer_configuration={'optimizer': AdamW (
    # ---- NOTE ----     Parameter Group 0
    # ---- NOTE ----         betas: (0.9, 0.999)
    # ---- NOTE ----         correct_bias: True
    # ---- NOTE ----         eps: 1e-06
    # ---- NOTE ----         initial_lr: 5e-05
    # ---- NOTE ----         lr: 5e-05
    # ---- NOTE ----         weight_decay: 0.0
    # ---- NOTE ----
    # ---- NOTE ----     Parameter Group 1
    # ---- NOTE ----         betas: (0.9, 0.999)
    # ---- NOTE ----         correct_bias: True
    # ---- NOTE ----         eps: 1e-06
    # ---- NOTE ----         initial_lr: 5e-05
    # ---- NOTE ----         lr: 5e-05
    # ---- NOTE ----         weight_decay: 0.0
    # ---- NOTE ----     ), 'scheduler': <pytorch_transformers.optimization.WarmupLinearSchedule object at 0x000001A444E6C048>, 'optimizer_number_training_optimization_steps': 10, 'device_use_gpu_fp16': False, 'optimizer_learning_rate': 5e-05, 'optimizer_warmup_proportion': 0.1, 'optimizer_warmup_steps': 0, 'optimizer_adam_epsilon': 1e-06, 'optimizer_weight_decay': 0.0, 'device_gpu_fp16_opt_level': None}
    # ---- NOTE ----     STDOUT-INFO: [2019-08-13T14:46:04.327166][main @ app_pytorch_language_understanding_transformers_helper.py:185][10]: config={
    # ---- NOTE ----       "attention_probs_dropout_prob": 0.1,
    # ---- NOTE ----       "directionality": "bidi",
    # ---- NOTE ----       "finetuning_task": null,
    # ---- NOTE ----       "hidden_act": "gelu",
    # ---- NOTE ----       "hidden_dropout_prob": 0.1,
    # ---- NOTE ----       "hidden_size": 1024,
    # ---- NOTE ----       "initializer_range": 0.02,
    # ---- NOTE ----       "intermediate_size": 4096,
    # ---- NOTE ----       "layer_norm_eps": 1e-12,
    # ---- NOTE ----       "max_position_embeddings": 512,
    # ---- NOTE ----       "num_attention_heads": 16,
    # ---- NOTE ----       "num_hidden_layers": 24,
    # ---- NOTE ----       "num_labels": 2,
    # ---- NOTE ----       "output_attentions": false,
    # ---- NOTE ----       "output_hidden_states": false,
    # ---- NOTE ----       "pooler_fc_size": 768,
    # ---- NOTE ----       "pooler_num_attention_heads": 12,
    # ---- NOTE ----       "pooler_num_fc_layers": 3,
    # ---- NOTE ----       "pooler_size_per_head": 128,
    # ---- NOTE ----       "pooler_type": "first_token_transform",
    # ---- NOTE ----       "torchscript": false,
    # ---- NOTE ----       "type_vocab_size": 2,
    # ---- NOTE ----       "vocab_size": 28996
    # ---- NOTE ----     }
    # ---- NOTE ----
    # ---- NOTE ----     The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.
    # ---- NOTE ----     STDOUT-INFO: [2019-08-13T14:46:04.898114][main @ app_pytorch_language_understanding_transformers_helper.py:189][10]: tokenizer=<pytorch_transformers.tokenization_bert.BertTokenizer object at 0x000001A444E44710>
    # ---- NOTE ----     100%|| 1338740706/1338740706 [01:42<00:00, 13096689.89B/s]
    # ---- NOTE ----     STDOUT-INFO: [2019-08-13T14:47:58.933359][main @ app_pytorch_language_understanding_transformers_helper.py:195][10]: BERT sequence classification model=BertForSequenceClassification(
    # ---- NOTE ----       (bert): BertModel(
    # ---- NOTE ----         (embeddings): BertEmbeddings(
    # ---- NOTE ----           (word_embeddings): Embedding(28996, 1024, padding_idx=0)
    # ---- NOTE ----           (position_embeddings): Embedding(512, 1024)
    # ---- NOTE ----           (token_type_embeddings): Embedding(2, 1024)
    # ---- NOTE ----           (LayerNorm): BertLayerNorm()
    # ---- NOTE ----           (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----         )
    # ---- NOTE ----         (encoder): BertEncoder(
    # ---- NOTE ----           (layer): ModuleList(
    # ---- NOTE ----             (0): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (1): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (2): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (3): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (4): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (5): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (6): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (7): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (8): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (9): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (10): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (11): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (12): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (13): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (14): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (15): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (16): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (17): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (18): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (19): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (20): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (21): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (22): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (23): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----           )
    # ---- NOTE ----         )
    # ---- NOTE ----         (pooler): BertPooler(
    # ---- NOTE ----           (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----           (activation): Tanh()
    # ---- NOTE ----         )
    # ---- NOTE ----       )
    # ---- NOTE ----       (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----       (classifier): Linear(in_features=1024, out_features=2, bias=True)
    # ---- NOTE ----     )
    # ---- NOTE ----     STDOUT-INFO: [2019-08-13T14:48:08.649366][main @ app_pytorch_language_understanding_transformers_helper.py:201][10]: BERT question answering model=BertForQuestionAnswering(
    # ---- NOTE ----       (bert): BertModel(
    # ---- NOTE ----         (embeddings): BertEmbeddings(
    # ---- NOTE ----           (word_embeddings): Embedding(28996, 1024, padding_idx=0)
    # ---- NOTE ----           (position_embeddings): Embedding(512, 1024)
    # ---- NOTE ----           (token_type_embeddings): Embedding(2, 1024)
    # ---- NOTE ----           (LayerNorm): BertLayerNorm()
    # ---- NOTE ----           (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----         )
    # ---- NOTE ----         (encoder): BertEncoder(
    # ---- NOTE ----           (layer): ModuleList(
    # ---- NOTE ----             (0): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (1): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (2): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (3): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (4): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (5): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (6): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (7): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (8): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (9): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (10): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (11): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (12): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (13): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (14): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (15): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (16): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (17): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (18): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (19): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (20): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (21): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (22): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (23): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----           )
    # ---- NOTE ----         )
    # ---- NOTE ----         (pooler): BertPooler(
    # ---- NOTE ----           (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----           (activation): Tanh()
    # ---- NOTE ----         )
    # ---- NOTE ----       )
    # ---- NOTE ----       (qa_outputs): Linear(in_features=1024, out_features=2, bias=True)
    # ---- NOTE ----     )
    # ---- NOTE ----     STDOUT-INFO: [2019-08-13T14:48:08.665367][main @ app_pytorch_language_understanding_transformers_helper.py:206][10]: optimizer_configuration={'optimizer': AdamW (
    # ---- NOTE ----     Parameter Group 0
    # ---- NOTE ----         betas: (0.9, 0.999)
    # ---- NOTE ----         correct_bias: True
    # ---- NOTE ----         eps: 1e-06
    # ---- NOTE ----         initial_lr: 5e-05
    # ---- NOTE ----         lr: 5e-05
    # ---- NOTE ----         weight_decay: 0.0
    # ---- NOTE ----
    # ---- NOTE ----     Parameter Group 1
    # ---- NOTE ----         betas: (0.9, 0.999)
    # ---- NOTE ----         correct_bias: True
    # ---- NOTE ----         eps: 1e-06
    # ---- NOTE ----         initial_lr: 5e-05
    # ---- NOTE ----         lr: 5e-05
    # ---- NOTE ----         weight_decay: 0.0
    # ---- NOTE ----     ), 'scheduler': <pytorch_transformers.optimization.WarmupLinearSchedule object at 0x000001A444F26400>, 'optimizer_number_training_optimization_steps': 10, 'device_use_gpu_fp16': False, 'optimizer_learning_rate': 5e-05, 'optimizer_warmup_proportion': 0.1, 'optimizer_warmup_steps': 0, 'optimizer_adam_epsilon': 1e-06, 'optimizer_weight_decay': 0.0, 'device_gpu_fp16_opt_level': None}
    # ---- NOTE ----     STDOUT-INFO: [2019-08-13T14:48:09.071392][main @ app_pytorch_language_understanding_transformers_helper.py:212][10]: config={
    # ---- NOTE ----       "attention_probs_dropout_prob": 0.1,
    # ---- NOTE ----       "finetuning_task": null,
    # ---- NOTE ----       "hidden_act": "gelu",
    # ---- NOTE ----       "hidden_dropout_prob": 0.1,
    # ---- NOTE ----       "hidden_size": 1024,
    # ---- NOTE ----       "initializer_range": 0.02,
    # ---- NOTE ----       "intermediate_size": 4096,
    # ---- NOTE ----       "layer_norm_eps": 1e-12,
    # ---- NOTE ----       "max_position_embeddings": 512,
    # ---- NOTE ----       "num_attention_heads": 16,
    # ---- NOTE ----       "num_hidden_layers": 24,
    # ---- NOTE ----       "num_labels": 2,
    # ---- NOTE ----       "output_attentions": false,
    # ---- NOTE ----       "output_hidden_states": false,
    # ---- NOTE ----       "torchscript": false,
    # ---- NOTE ----       "type_vocab_size": 2,
    # ---- NOTE ----       "vocab_size": 30522
    # ---- NOTE ----     }
    # ---- NOTE ----
    # ---- NOTE ----     STDOUT-INFO: [2019-08-13T14:48:09.545367][main @ app_pytorch_language_understanding_transformers_helper.py:216][10]: tokenizer=<pytorch_transformers.tokenization_bert.BertTokenizer object at 0x000001A43FC66550>
    # ---- NOTE ----     100%|| 1344997306/1344997306 [01:00<00:00, 22166165.86B/s]
    # ---- NOTE ----     STDOUT-INFO: [2019-08-13T14:49:21.766269][main @ app_pytorch_language_understanding_transformers_helper.py:222][10]: BERT sequence classification model=BertForSequenceClassification(
    # ---- NOTE ----       (bert): BertModel(
    # ---- NOTE ----         (embeddings): BertEmbeddings(
    # ---- NOTE ----           (word_embeddings): Embedding(30522, 1024, padding_idx=0)
    # ---- NOTE ----           (position_embeddings): Embedding(512, 1024)
    # ---- NOTE ----           (token_type_embeddings): Embedding(2, 1024)
    # ---- NOTE ----           (LayerNorm): BertLayerNorm()
    # ---- NOTE ----           (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----         )
    # ---- NOTE ----         (encoder): BertEncoder(
    # ---- NOTE ----           (layer): ModuleList(
    # ---- NOTE ----             (0): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (1): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (2): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (3): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (4): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (5): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (6): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (7): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (8): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (9): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (10): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (11): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (12): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (13): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (14): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (15): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (16): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (17): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (18): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (19): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (20): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (21): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (22): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (23): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----           )
    # ---- NOTE ----         )
    # ---- NOTE ----         (pooler): BertPooler(
    # ---- NOTE ----           (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----           (activation): Tanh()
    # ---- NOTE ----         )
    # ---- NOTE ----       )
    # ---- NOTE ----       (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----       (classifier): Linear(in_features=1024, out_features=2, bias=True)
    # ---- NOTE ----     )
    # ---- NOTE ----     STDOUT-INFO: [2019-08-13T14:49:31.252028][main @ app_pytorch_language_understanding_transformers_helper.py:228][10]: BERT question answering model=BertForQuestionAnswering(
    # ---- NOTE ----       (bert): BertModel(
    # ---- NOTE ----         (embeddings): BertEmbeddings(
    # ---- NOTE ----           (word_embeddings): Embedding(30522, 1024, padding_idx=0)
    # ---- NOTE ----           (position_embeddings): Embedding(512, 1024)
    # ---- NOTE ----           (token_type_embeddings): Embedding(2, 1024)
    # ---- NOTE ----           (LayerNorm): BertLayerNorm()
    # ---- NOTE ----           (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----         )
    # ---- NOTE ----         (encoder): BertEncoder(
    # ---- NOTE ----           (layer): ModuleList(
    # ---- NOTE ----             (0): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (1): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (2): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (3): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (4): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (5): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (6): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (7): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (8): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (9): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (10): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (11): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (12): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (13): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (14): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (15): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (16): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (17): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (18): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (19): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (20): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (21): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (22): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----             (23): BertLayer(
    # ---- NOTE ----               (attention): BertAttention(
    # ---- NOTE ----                 (self): BertSelfAttention(
    # ---- NOTE ----                   (query): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (key): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (value): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----                 (output): BertSelfOutput(
    # ---- NOTE ----                   (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----                   (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                   (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----                 )
    # ---- NOTE ----               )
    # ---- NOTE ----               (intermediate): BertIntermediate(
    # ---- NOTE ----                 (dense): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               )
    # ---- NOTE ----               (output): BertOutput(
    # ---- NOTE ----                 (dense): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----                 (LayerNorm): BertLayerNorm()
    # ---- NOTE ----                 (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----               )
    # ---- NOTE ----             )
    # ---- NOTE ----           )
    # ---- NOTE ----         )
    # ---- NOTE ----         (pooler): BertPooler(
    # ---- NOTE ----           (dense): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----           (activation): Tanh()
    # ---- NOTE ----         )
    # ---- NOTE ----       )
    # ---- NOTE ----       (qa_outputs): Linear(in_features=1024, out_features=2, bias=True)
    # ---- NOTE ----     )
    # ---- NOTE ----     STDOUT-INFO: [2019-08-13T14:49:31.265064][main @ app_pytorch_language_understanding_transformers_helper.py:233][10]: optimizer_configuration={'optimizer': AdamW (
    # ---- NOTE ----     Parameter Group 0
    # ---- NOTE ----         betas: (0.9, 0.999)
    # ---- NOTE ----         correct_bias: True
    # ---- NOTE ----         eps: 1e-06
    # ---- NOTE ----         initial_lr: 5e-05
    # ---- NOTE ----         lr: 5e-05
    # ---- NOTE ----         weight_decay: 0.0
    # ---- NOTE ----
    # ---- NOTE ----     Parameter Group 1
    # ---- NOTE ----         betas: (0.9, 0.999)
    # ---- NOTE ----         correct_bias: True
    # ---- NOTE ----         eps: 1e-06
    # ---- NOTE ----         initial_lr: 5e-05
    # ---- NOTE ----         lr: 5e-05
    # ---- NOTE ----         weight_decay: 0.0
    # ---- NOTE ----     ), 'scheduler': <pytorch_transformers.optimization.WarmupLinearSchedule object at 0x000001A444F264A8>, 'optimizer_number_training_optimization_steps': 10, 'device_use_gpu_fp16': False, 'optimizer_learning_rate': 5e-05, 'optimizer_warmup_proportion': 0.1, 'optimizer_warmup_steps': 0, 'optimizer_adam_epsilon': 1e-06, 'optimizer_weight_decay': 0.0, 'device_gpu_fp16_opt_level': None}
    # ---- NOTE ----     STDOUT-INFO: [2019-08-13T14:49:31.600467][main @ app_pytorch_language_understanding_transformers_helper.py:239][10]: config={
    # ---- NOTE ----       "asm": false,
    # ---- NOTE ----       "attention_dropout": 0.1,
    # ---- NOTE ----       "bos_index": 0,
    # ---- NOTE ----       "causal": false,
    # ---- NOTE ----       "dropout": 0.1,
    # ---- NOTE ----       "emb_dim": 1024,
    # ---- NOTE ----       "embed_init_std": 0.02209708691207961,
    # ---- NOTE ----       "end_n_top": 5,
    # ---- NOTE ----       "eos_index": 1,
    # ---- NOTE ----       "finetuning_task": null,
    # ---- NOTE ----       "gelu_activation": true,
    # ---- NOTE ----       "id2lang": {
    # ---- NOTE ----         "0": "de",
    # ---- NOTE ----         "1": "en"
    # ---- NOTE ----       },
    # ---- NOTE ----       "init_std": 0.02,
    # ---- NOTE ----       "is_encoder": true,
    # ---- NOTE ----       "lang2id": {
    # ---- NOTE ----         "de": 0,
    # ---- NOTE ----         "en": 1
    # ---- NOTE ----       },
    # ---- NOTE ----       "layer_norm_eps": 1e-12,
    # ---- NOTE ----       "mask_index": 5,
    # ---- NOTE ----       "max_position_embeddings": 512,
    # ---- NOTE ----       "max_vocab": -1,
    # ---- NOTE ----       "min_count": 0,
    # ---- NOTE ----       "n_heads": 8,
    # ---- NOTE ----       "n_langs": 2,
    # ---- NOTE ----       "n_layers": 6,
    # ---- NOTE ----       "n_words": 64699,
    # ---- NOTE ----       "num_labels": 2,
    # ---- NOTE ----       "output_attentions": false,
    # ---- NOTE ----       "output_hidden_states": false,
    # ---- NOTE ----       "pad_index": 2,
    # ---- NOTE ----       "same_enc_dec": true,
    # ---- NOTE ----       "share_inout_emb": true,
    # ---- NOTE ----       "sinusoidal_embeddings": false,
    # ---- NOTE ----       "start_n_top": 5,
    # ---- NOTE ----       "summary_activation": null,
    # ---- NOTE ----       "summary_first_dropout": 0.1,
    # ---- NOTE ----       "summary_proj_to_labels": true,
    # ---- NOTE ----       "summary_type": "first",
    # ---- NOTE ----       "summary_use_proj": true,
    # ---- NOTE ----       "torchscript": false,
    # ---- NOTE ----       "unk_index": 3
    # ---- NOTE ----     }
    # ---- NOTE ----
    # ---- NOTE ----     STDOUT-INFO: [2019-08-13T14:49:33.769525][main @ app_pytorch_language_understanding_transformers_helper.py:243][10]: tokenizer=<pytorch_transformers.tokenization_xlm.XLMTokenizer object at 0x000001A43FC5D208>
    # ---- NOTE ----     100%|| 834711002/834711002 [00:46<00:00, 17832650.92B/s]
    # ---- NOTE ----     STDOUT-INFO: [2019-08-13T14:50:26.242903][main @ app_pytorch_language_understanding_transformers_helper.py:257][10]: XLM sequence classification model=XLMForSequenceClassification(
    # ---- NOTE ----       (transformer): XLMModel(
    # ---- NOTE ----         (position_embeddings): Embedding(512, 1024)
    # ---- NOTE ----         (lang_embeddings): Embedding(2, 1024)
    # ---- NOTE ----         (embeddings): Embedding(64699, 1024, padding_idx=2)
    # ---- NOTE ----         (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    # ---- NOTE ----         (attentions): ModuleList(
    # ---- NOTE ----           (0): MultiHeadAttention(
    # ---- NOTE ----             (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----             (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----             (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----             (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----           )
    # ---- NOTE ----           (1): MultiHeadAttention(
    # ---- NOTE ----             (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----             (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----             (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----             (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----           )
    # ---- NOTE ----           (2): MultiHeadAttention(
    # ---- NOTE ----             (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----             (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----             (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----             (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----           )
    # ---- NOTE ----           (3): MultiHeadAttention(
    # ---- NOTE ----             (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----             (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----             (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----             (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----           )
    # ---- NOTE ----           (4): MultiHeadAttention(
    # ---- NOTE ----             (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----             (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----             (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----             (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----           )
    # ---- NOTE ----           (5): MultiHeadAttention(
    # ---- NOTE ----             (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----             (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----             (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----             (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----           )
    # ---- NOTE ----         )
    # ---- NOTE ----         (layer_norm1): ModuleList(
    # ---- NOTE ----           (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    # ---- NOTE ----           (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    # ---- NOTE ----           (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    # ---- NOTE ----           (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    # ---- NOTE ----           (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    # ---- NOTE ----           (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    # ---- NOTE ----         )
    # ---- NOTE ----         (ffns): ModuleList(
    # ---- NOTE ----           (0): TransformerFFN(
    # ---- NOTE ----             (lin1): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----             (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----           )
    # ---- NOTE ----           (1): TransformerFFN(
    # ---- NOTE ----             (lin1): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----             (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----           )
    # ---- NOTE ----           (2): TransformerFFN(
    # ---- NOTE ----             (lin1): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----             (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----           )
    # ---- NOTE ----           (3): TransformerFFN(
    # ---- NOTE ----             (lin1): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----             (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----           )
    # ---- NOTE ----           (4): TransformerFFN(
    # ---- NOTE ----             (lin1): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----             (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----           )
    # ---- NOTE ----           (5): TransformerFFN(
    # ---- NOTE ----             (lin1): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----             (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----           )
    # ---- NOTE ----         )
    # ---- NOTE ----         (layer_norm2): ModuleList(
    # ---- NOTE ----           (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    # ---- NOTE ----           (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    # ---- NOTE ----           (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    # ---- NOTE ----           (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    # ---- NOTE ----           (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    # ---- NOTE ----           (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    # ---- NOTE ----         )
    # ---- NOTE ----       )
    # ---- NOTE ----       (sequence_summary): SequenceSummary(
    # ---- NOTE ----         (summary): Linear(in_features=1024, out_features=2, bias=True)
    # ---- NOTE ----         (activation): Identity()
    # ---- NOTE ----         (first_dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----         (last_dropout): Identity()
    # ---- NOTE ----       )
    # ---- NOTE ----     )
    # ---- NOTE ----     STDOUT-INFO: [2019-08-13T14:50:30.538480][main @ app_pytorch_language_understanding_transformers_helper.py:263][10]: XLM question answering model=XLMForQuestionAnswering(
    # ---- NOTE ----       (transformer): XLMModel(
    # ---- NOTE ----         (position_embeddings): Embedding(512, 1024)
    # ---- NOTE ----         (lang_embeddings): Embedding(2, 1024)
    # ---- NOTE ----         (embeddings): Embedding(64699, 1024, padding_idx=2)
    # ---- NOTE ----         (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    # ---- NOTE ----         (attentions): ModuleList(
    # ---- NOTE ----           (0): MultiHeadAttention(
    # ---- NOTE ----             (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----             (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----             (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----             (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----           )
    # ---- NOTE ----           (1): MultiHeadAttention(
    # ---- NOTE ----             (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----             (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----             (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----             (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----           )
    # ---- NOTE ----           (2): MultiHeadAttention(
    # ---- NOTE ----             (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----             (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----             (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----             (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----           )
    # ---- NOTE ----           (3): MultiHeadAttention(
    # ---- NOTE ----             (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----             (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----             (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----             (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----           )
    # ---- NOTE ----           (4): MultiHeadAttention(
    # ---- NOTE ----             (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----             (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----             (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----             (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----           )
    # ---- NOTE ----           (5): MultiHeadAttention(
    # ---- NOTE ----             (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----             (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----             (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----             (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----           )
    # ---- NOTE ----         )
    # ---- NOTE ----         (layer_norm1): ModuleList(
    # ---- NOTE ----           (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    # ---- NOTE ----           (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    # ---- NOTE ----           (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    # ---- NOTE ----           (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    # ---- NOTE ----           (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    # ---- NOTE ----           (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    # ---- NOTE ----         )
    # ---- NOTE ----         (ffns): ModuleList(
    # ---- NOTE ----           (0): TransformerFFN(
    # ---- NOTE ----             (lin1): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----             (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----           )
    # ---- NOTE ----           (1): TransformerFFN(
    # ---- NOTE ----             (lin1): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----             (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----           )
    # ---- NOTE ----           (2): TransformerFFN(
    # ---- NOTE ----             (lin1): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----             (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----           )
    # ---- NOTE ----           (3): TransformerFFN(
    # ---- NOTE ----             (lin1): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----             (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----           )
    # ---- NOTE ----           (4): TransformerFFN(
    # ---- NOTE ----             (lin1): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----             (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----           )
    # ---- NOTE ----           (5): TransformerFFN(
    # ---- NOTE ----             (lin1): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----             (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----           )
    # ---- NOTE ----         )
    # ---- NOTE ----         (layer_norm2): ModuleList(
    # ---- NOTE ----           (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    # ---- NOTE ----           (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    # ---- NOTE ----           (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    # ---- NOTE ----           (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    # ---- NOTE ----           (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    # ---- NOTE ----           (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    # ---- NOTE ----         )
    # ---- NOTE ----       )
    # ---- NOTE ----       (qa_outputs): SQuADHead(
    # ---- NOTE ----         (start_logits): PoolerStartLogits(
    # ---- NOTE ----           (dense): Linear(in_features=1024, out_features=1, bias=True)
    # ---- NOTE ----         )
    # ---- NOTE ----         (end_logits): PoolerEndLogits(
    # ---- NOTE ----           (dense_0): Linear(in_features=2048, out_features=1024, bias=True)
    # ---- NOTE ----           (activation): Tanh()
    # ---- NOTE ----           (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    # ---- NOTE ----           (dense_1): Linear(in_features=1024, out_features=1, bias=True)
    # ---- NOTE ----         )
    # ---- NOTE ----         (answer_class): PoolerAnswerClass(
    # ---- NOTE ----           (dense_0): Linear(in_features=2048, out_features=1024, bias=True)
    # ---- NOTE ----           (activation): Tanh()
    # ---- NOTE ----           (dense_1): Linear(in_features=1024, out_features=1, bias=False)
    # ---- NOTE ----         )
    # ---- NOTE ----       )
    # ---- NOTE ----     )
    # ---- NOTE ----     STDOUT-INFO: [2019-08-13T14:50:30.547483][main @ app_pytorch_language_understanding_transformers_helper.py:268][10]: optimizer_configuration={'optimizer': AdamW (
    # ---- NOTE ----     Parameter Group 0
    # ---- NOTE ----         betas: (0.9, 0.999)
    # ---- NOTE ----         correct_bias: True
    # ---- NOTE ----         eps: 1e-06
    # ---- NOTE ----         initial_lr: 5e-05
    # ---- NOTE ----         lr: 5e-05
    # ---- NOTE ----         weight_decay: 0.0
    # ---- NOTE ----
    # ---- NOTE ----     Parameter Group 1
    # ---- NOTE ----         betas: (0.9, 0.999)
    # ---- NOTE ----         correct_bias: True
    # ---- NOTE ----         eps: 1e-06
    # ---- NOTE ----         initial_lr: 5e-05
    # ---- NOTE ----         lr: 5e-05
    # ---- NOTE ----         weight_decay: 0.0
    # ---- NOTE ----     ), 'scheduler': <pytorch_transformers.optimization.WarmupLinearSchedule object at 0x000001A446A6AF98>, 'optimizer_number_training_optimization_steps': 10, 'device_use_gpu_fp16': False, 'optimizer_learning_rate': 5e-05, 'optimizer_warmup_proportion': 0.1, 'optimizer_warmup_steps': 0, 'optimizer_adam_epsilon': 1e-06, 'optimizer_weight_decay': 0.0, 'device_gpu_fp16_opt_level': None}
    # ---- NOTE ----     STDOUT-INFO: [2019-08-13T14:50:30.947480][main @ app_pytorch_language_understanding_transformers_helper.py:274][10]: config={
    # ---- NOTE ----       "attn_type": "bi",
    # ---- NOTE ----       "bi_data": false,
    # ---- NOTE ----       "clamp_len": -1,
    # ---- NOTE ----       "d_head": 64,
    # ---- NOTE ----       "d_inner": 3072,
    # ---- NOTE ----       "d_model": 768,
    # ---- NOTE ----       "dropout": 0.1,
    # ---- NOTE ----       "end_n_top": 5,
    # ---- NOTE ----       "ff_activation": "gelu",
    # ---- NOTE ----       "finetuning_task": null,
    # ---- NOTE ----       "initializer_range": 0.02,
    # ---- NOTE ----       "layer_norm_eps": 1e-12,
    # ---- NOTE ----       "mem_len": null,
    # ---- NOTE ----       "n_head": 12,
    # ---- NOTE ----       "n_layer": 12,
    # ---- NOTE ----       "n_token": 32000,
    # ---- NOTE ----       "num_labels": 2,
    # ---- NOTE ----       "output_attentions": false,
    # ---- NOTE ----       "output_hidden_states": false,
    # ---- NOTE ----       "reuse_len": null,
    # ---- NOTE ----       "same_length": false,
    # ---- NOTE ----       "start_n_top": 5,
    # ---- NOTE ----       "summary_activation": "tanh",
    # ---- NOTE ----       "summary_last_dropout": 0.1,
    # ---- NOTE ----       "summary_type": "last",
    # ---- NOTE ----       "summary_use_proj": true,
    # ---- NOTE ----       "torchscript": false,
    # ---- NOTE ----       "untie_r": true
    # ---- NOTE ----     }
    # ---- NOTE ----
    # ---- NOTE ----     STDOUT-INFO: [2019-08-13T14:50:31.452160][main @ app_pytorch_language_understanding_transformers_helper.py:278][10]: tokenizer=<pytorch_transformers.tokenization_xlnet.XLNetTokenizer object at 0x000001A4450C2198>
    # ---- NOTE ----     100%|| 467042463/467042463 [00:33<00:00, 13804198.50B/s]
    # ---- NOTE ----     STDOUT-INFO: [2019-08-13T14:51:10.046367][main @ app_pytorch_language_understanding_transformers_helper.py:284][10]: XLNet sequence classification model=XLNetForSequenceClassification(
    # ---- NOTE ----       (transformer): XLNetModel(
    # ---- NOTE ----         (word_embedding): Embedding(32000, 768)
    # ---- NOTE ----         (layer): ModuleList(
    # ---- NOTE ----           (0): XLNetLayer(
    # ---- NOTE ----             (rel_attn): XLNetRelativeAttention(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (ff): XLNetFeedForward(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (layer_1): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               (layer_2): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----           )
    # ---- NOTE ----           (1): XLNetLayer(
    # ---- NOTE ----             (rel_attn): XLNetRelativeAttention(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (ff): XLNetFeedForward(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (layer_1): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               (layer_2): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----           )
    # ---- NOTE ----           (2): XLNetLayer(
    # ---- NOTE ----             (rel_attn): XLNetRelativeAttention(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (ff): XLNetFeedForward(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (layer_1): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               (layer_2): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----           )
    # ---- NOTE ----           (3): XLNetLayer(
    # ---- NOTE ----             (rel_attn): XLNetRelativeAttention(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (ff): XLNetFeedForward(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (layer_1): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               (layer_2): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----           )
    # ---- NOTE ----           (4): XLNetLayer(
    # ---- NOTE ----             (rel_attn): XLNetRelativeAttention(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (ff): XLNetFeedForward(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (layer_1): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               (layer_2): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----           )
    # ---- NOTE ----           (5): XLNetLayer(
    # ---- NOTE ----             (rel_attn): XLNetRelativeAttention(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (ff): XLNetFeedForward(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (layer_1): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               (layer_2): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----           )
    # ---- NOTE ----           (6): XLNetLayer(
    # ---- NOTE ----             (rel_attn): XLNetRelativeAttention(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (ff): XLNetFeedForward(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (layer_1): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               (layer_2): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----           )
    # ---- NOTE ----           (7): XLNetLayer(
    # ---- NOTE ----             (rel_attn): XLNetRelativeAttention(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (ff): XLNetFeedForward(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (layer_1): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               (layer_2): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----           )
    # ---- NOTE ----           (8): XLNetLayer(
    # ---- NOTE ----             (rel_attn): XLNetRelativeAttention(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (ff): XLNetFeedForward(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (layer_1): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               (layer_2): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----           )
    # ---- NOTE ----           (9): XLNetLayer(
    # ---- NOTE ----             (rel_attn): XLNetRelativeAttention(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (ff): XLNetFeedForward(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (layer_1): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               (layer_2): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----           )
    # ---- NOTE ----           (10): XLNetLayer(
    # ---- NOTE ----             (rel_attn): XLNetRelativeAttention(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (ff): XLNetFeedForward(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (layer_1): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               (layer_2): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----           )
    # ---- NOTE ----           (11): XLNetLayer(
    # ---- NOTE ----             (rel_attn): XLNetRelativeAttention(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (ff): XLNetFeedForward(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (layer_1): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               (layer_2): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----           )
    # ---- NOTE ----         )
    # ---- NOTE ----         (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----       )
    # ---- NOTE ----       (sequence_summary): SequenceSummary(
    # ---- NOTE ----         (summary): Linear(in_features=768, out_features=768, bias=True)
    # ---- NOTE ----         (activation): Tanh()
    # ---- NOTE ----         (first_dropout): Identity()
    # ---- NOTE ----         (last_dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----       )
    # ---- NOTE ----       (logits_proj): Linear(in_features=768, out_features=2, bias=True)
    # ---- NOTE ----     )
    # ---- NOTE ----     STDOUT-INFO: [2019-08-13T14:51:13.647720][main @ app_pytorch_language_understanding_transformers_helper.py:290][10]: XLNet question answering model=XLNetForQuestionAnswering(
    # ---- NOTE ----       (transformer): XLNetModel(
    # ---- NOTE ----         (word_embedding): Embedding(32000, 768)
    # ---- NOTE ----         (layer): ModuleList(
    # ---- NOTE ----           (0): XLNetLayer(
    # ---- NOTE ----             (rel_attn): XLNetRelativeAttention(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (ff): XLNetFeedForward(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (layer_1): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               (layer_2): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----           )
    # ---- NOTE ----           (1): XLNetLayer(
    # ---- NOTE ----             (rel_attn): XLNetRelativeAttention(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (ff): XLNetFeedForward(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (layer_1): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               (layer_2): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----           )
    # ---- NOTE ----           (2): XLNetLayer(
    # ---- NOTE ----             (rel_attn): XLNetRelativeAttention(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (ff): XLNetFeedForward(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (layer_1): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               (layer_2): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----           )
    # ---- NOTE ----           (3): XLNetLayer(
    # ---- NOTE ----             (rel_attn): XLNetRelativeAttention(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (ff): XLNetFeedForward(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (layer_1): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               (layer_2): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----           )
    # ---- NOTE ----           (4): XLNetLayer(
    # ---- NOTE ----             (rel_attn): XLNetRelativeAttention(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (ff): XLNetFeedForward(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (layer_1): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               (layer_2): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----           )
    # ---- NOTE ----           (5): XLNetLayer(
    # ---- NOTE ----             (rel_attn): XLNetRelativeAttention(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (ff): XLNetFeedForward(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (layer_1): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               (layer_2): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----           )
    # ---- NOTE ----           (6): XLNetLayer(
    # ---- NOTE ----             (rel_attn): XLNetRelativeAttention(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (ff): XLNetFeedForward(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (layer_1): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               (layer_2): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----           )
    # ---- NOTE ----           (7): XLNetLayer(
    # ---- NOTE ----             (rel_attn): XLNetRelativeAttention(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (ff): XLNetFeedForward(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (layer_1): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               (layer_2): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----           )
    # ---- NOTE ----           (8): XLNetLayer(
    # ---- NOTE ----             (rel_attn): XLNetRelativeAttention(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (ff): XLNetFeedForward(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (layer_1): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               (layer_2): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----           )
    # ---- NOTE ----           (9): XLNetLayer(
    # ---- NOTE ----             (rel_attn): XLNetRelativeAttention(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (ff): XLNetFeedForward(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (layer_1): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               (layer_2): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----           )
    # ---- NOTE ----           (10): XLNetLayer(
    # ---- NOTE ----             (rel_attn): XLNetRelativeAttention(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (ff): XLNetFeedForward(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (layer_1): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               (layer_2): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----           )
    # ---- NOTE ----           (11): XLNetLayer(
    # ---- NOTE ----             (rel_attn): XLNetRelativeAttention(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (ff): XLNetFeedForward(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (layer_1): Linear(in_features=768, out_features=3072, bias=True)
    # ---- NOTE ----               (layer_2): Linear(in_features=3072, out_features=768, bias=True)
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----           )
    # ---- NOTE ----         )
    # ---- NOTE ----         (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----       )
    # ---- NOTE ----       (start_logits): PoolerStartLogits(
    # ---- NOTE ----         (dense): Linear(in_features=768, out_features=1, bias=True)
    # ---- NOTE ----       )
    # ---- NOTE ----       (end_logits): PoolerEndLogits(
    # ---- NOTE ----         (dense_0): Linear(in_features=1536, out_features=768, bias=True)
    # ---- NOTE ----         (activation): Tanh()
    # ---- NOTE ----         (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
    # ---- NOTE ----         (dense_1): Linear(in_features=768, out_features=1, bias=True)
    # ---- NOTE ----       )
    # ---- NOTE ----       (answer_class): PoolerAnswerClass(
    # ---- NOTE ----         (dense_0): Linear(in_features=1536, out_features=768, bias=True)
    # ---- NOTE ----         (activation): Tanh()
    # ---- NOTE ----         (dense_1): Linear(in_features=768, out_features=1, bias=False)
    # ---- NOTE ----       )
    # ---- NOTE ----     )
    # ---- NOTE ----     STDOUT-INFO: [2019-08-13T14:51:13.654718][main @ app_pytorch_language_understanding_transformers_helper.py:295][10]: optimizer_configuration={'optimizer': AdamW (
    # ---- NOTE ----     Parameter Group 0
    # ---- NOTE ----         betas: (0.9, 0.999)
    # ---- NOTE ----         correct_bias: True
    # ---- NOTE ----         eps: 1e-06
    # ---- NOTE ----         initial_lr: 5e-05
    # ---- NOTE ----         lr: 5e-05
    # ---- NOTE ----         weight_decay: 0.0
    # ---- NOTE ----
    # ---- NOTE ----     Parameter Group 1
    # ---- NOTE ----         betas: (0.9, 0.999)
    # ---- NOTE ----         correct_bias: True
    # ---- NOTE ----         eps: 1e-06
    # ---- NOTE ----         initial_lr: 5e-05
    # ---- NOTE ----         lr: 5e-05
    # ---- NOTE ----         weight_decay: 0.0
    # ---- NOTE ----     ), 'scheduler': <pytorch_transformers.optimization.WarmupLinearSchedule object at 0x000001A446AC2940>, 'optimizer_number_training_optimization_steps': 10, 'device_use_gpu_fp16': False, 'optimizer_learning_rate': 5e-05, 'optimizer_warmup_proportion': 0.1, 'optimizer_warmup_steps': 0, 'optimizer_adam_epsilon': 1e-06, 'optimizer_weight_decay': 0.0, 'device_gpu_fp16_opt_level': None}
    # ---- NOTE ----     STDOUT-INFO: [2019-08-13T14:51:14.076719][main @ app_pytorch_language_understanding_transformers_helper.py:301][10]: config={
    # ---- NOTE ----       "attn_type": "bi",
    # ---- NOTE ----       "bi_data": false,
    # ---- NOTE ----       "clamp_len": -1,
    # ---- NOTE ----       "d_head": 64,
    # ---- NOTE ----       "d_inner": 4096,
    # ---- NOTE ----       "d_model": 1024,
    # ---- NOTE ----       "dropatt": 0.1,
    # ---- NOTE ----       "dropout": 0.1,
    # ---- NOTE ----       "end_n_top": 5,
    # ---- NOTE ----       "ff_activation": "gelu",
    # ---- NOTE ----       "finetuning_task": null,
    # ---- NOTE ----       "init": "normal",
    # ---- NOTE ----       "init_range": 0.1,
    # ---- NOTE ----       "init_std": 0.02,
    # ---- NOTE ----       "initializer_range": 0.02,
    # ---- NOTE ----       "layer_norm_eps": 1e-12,
    # ---- NOTE ----       "max_position_embeddings": 512,
    # ---- NOTE ----       "mem_len": null,
    # ---- NOTE ----       "n_head": 16,
    # ---- NOTE ----       "n_layer": 24,
    # ---- NOTE ----       "n_token": 32000,
    # ---- NOTE ----       "num_labels": 2,
    # ---- NOTE ----       "output_attentions": false,
    # ---- NOTE ----       "output_hidden_states": false,
    # ---- NOTE ----       "reuse_len": null,
    # ---- NOTE ----       "same_length": false,
    # ---- NOTE ----       "start_n_top": 5,
    # ---- NOTE ----       "summary_activation": "tanh",
    # ---- NOTE ----       "summary_last_dropout": 0.1,
    # ---- NOTE ----       "summary_type": "last",
    # ---- NOTE ----       "summary_use_proj": true,
    # ---- NOTE ----       "torchscript": false,
    # ---- NOTE ----       "untie_r": true
    # ---- NOTE ----     }
    # ---- NOTE ----
    # ---- NOTE ----     STDOUT-INFO: [2019-08-13T14:51:14.559720][main @ app_pytorch_language_understanding_transformers_helper.py:305][10]: tokenizer=<pytorch_transformers.tokenization_xlnet.XLNetTokenizer object at 0x000001A4450C2588>
    # ---- NOTE ----     100%|| 1441285815/1441285815 [01:11<00:00, 20271029.76B/s]
    # ---- NOTE ----     STDOUT-INFO: [2019-08-13T14:52:37.549364][main @ app_pytorch_language_understanding_transformers_helper.py:311][10]: XLNet sequence classification model=XLNetForSequenceClassification(
    # ---- NOTE ----       (transformer): XLNetModel(
    # ---- NOTE ----         (word_embedding): Embedding(32000, 1024)
    # ---- NOTE ----         (layer): ModuleList(
    # ---- NOTE ----           (0): XLNetLayer(
    # ---- NOTE ----             (rel_attn): XLNetRelativeAttention(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (ff): XLNetFeedForward(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----           )
    # ---- NOTE ----           (1): XLNetLayer(
    # ---- NOTE ----             (rel_attn): XLNetRelativeAttention(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (ff): XLNetFeedForward(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----           )
    # ---- NOTE ----           (2): XLNetLayer(
    # ---- NOTE ----             (rel_attn): XLNetRelativeAttention(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (ff): XLNetFeedForward(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----           )
    # ---- NOTE ----           (3): XLNetLayer(
    # ---- NOTE ----             (rel_attn): XLNetRelativeAttention(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (ff): XLNetFeedForward(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----           )
    # ---- NOTE ----           (4): XLNetLayer(
    # ---- NOTE ----             (rel_attn): XLNetRelativeAttention(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (ff): XLNetFeedForward(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----           )
    # ---- NOTE ----           (5): XLNetLayer(
    # ---- NOTE ----             (rel_attn): XLNetRelativeAttention(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (ff): XLNetFeedForward(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----           )
    # ---- NOTE ----           (6): XLNetLayer(
    # ---- NOTE ----             (rel_attn): XLNetRelativeAttention(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (ff): XLNetFeedForward(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----           )
    # ---- NOTE ----           (7): XLNetLayer(
    # ---- NOTE ----             (rel_attn): XLNetRelativeAttention(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (ff): XLNetFeedForward(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----           )
    # ---- NOTE ----           (8): XLNetLayer(
    # ---- NOTE ----             (rel_attn): XLNetRelativeAttention(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (ff): XLNetFeedForward(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----           )
    # ---- NOTE ----           (9): XLNetLayer(
    # ---- NOTE ----             (rel_attn): XLNetRelativeAttention(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (ff): XLNetFeedForward(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----           )
    # ---- NOTE ----           (10): XLNetLayer(
    # ---- NOTE ----             (rel_attn): XLNetRelativeAttention(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (ff): XLNetFeedForward(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----           )
    # ---- NOTE ----           (11): XLNetLayer(
    # ---- NOTE ----             (rel_attn): XLNetRelativeAttention(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (ff): XLNetFeedForward(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----           )
    # ---- NOTE ----           (12): XLNetLayer(
    # ---- NOTE ----             (rel_attn): XLNetRelativeAttention(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (ff): XLNetFeedForward(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----           )
    # ---- NOTE ----           (13): XLNetLayer(
    # ---- NOTE ----             (rel_attn): XLNetRelativeAttention(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (ff): XLNetFeedForward(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----           )
    # ---- NOTE ----           (14): XLNetLayer(
    # ---- NOTE ----             (rel_attn): XLNetRelativeAttention(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (ff): XLNetFeedForward(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----           )
    # ---- NOTE ----           (15): XLNetLayer(
    # ---- NOTE ----             (rel_attn): XLNetRelativeAttention(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (ff): XLNetFeedForward(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----           )
    # ---- NOTE ----           (16): XLNetLayer(
    # ---- NOTE ----             (rel_attn): XLNetRelativeAttention(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (ff): XLNetFeedForward(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----           )
    # ---- NOTE ----           (17): XLNetLayer(
    # ---- NOTE ----             (rel_attn): XLNetRelativeAttention(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (ff): XLNetFeedForward(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----           )
    # ---- NOTE ----           (18): XLNetLayer(
    # ---- NOTE ----             (rel_attn): XLNetRelativeAttention(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (ff): XLNetFeedForward(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----           )
    # ---- NOTE ----           (19): XLNetLayer(
    # ---- NOTE ----             (rel_attn): XLNetRelativeAttention(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (ff): XLNetFeedForward(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----           )
    # ---- NOTE ----           (20): XLNetLayer(
    # ---- NOTE ----             (rel_attn): XLNetRelativeAttention(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (ff): XLNetFeedForward(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----           )
    # ---- NOTE ----           (21): XLNetLayer(
    # ---- NOTE ----             (rel_attn): XLNetRelativeAttention(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (ff): XLNetFeedForward(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----           )
    # ---- NOTE ----           (22): XLNetLayer(
    # ---- NOTE ----             (rel_attn): XLNetRelativeAttention(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (ff): XLNetFeedForward(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----           )
    # ---- NOTE ----           (23): XLNetLayer(
    # ---- NOTE ----             (rel_attn): XLNetRelativeAttention(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (ff): XLNetFeedForward(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----           )
    # ---- NOTE ----         )
    # ---- NOTE ----         (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----       )
    # ---- NOTE ----       (sequence_summary): SequenceSummary(
    # ---- NOTE ----         (summary): Linear(in_features=1024, out_features=1024, bias=True)
    # ---- NOTE ----         (activation): Tanh()
    # ---- NOTE ----         (first_dropout): Identity()
    # ---- NOTE ----         (last_dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----       )
    # ---- NOTE ----       (logits_proj): Linear(in_features=1024, out_features=2, bias=True)
    # ---- NOTE ----     )
    # ---- NOTE ----     STDOUT-INFO: [2019-08-13T14:52:46.549320][main @ app_pytorch_language_understanding_transformers_helper.py:317][10]: XLNet question answering model=XLNetForQuestionAnswering(
    # ---- NOTE ----       (transformer): XLNetModel(
    # ---- NOTE ----         (word_embedding): Embedding(32000, 1024)
    # ---- NOTE ----         (layer): ModuleList(
    # ---- NOTE ----           (0): XLNetLayer(
    # ---- NOTE ----             (rel_attn): XLNetRelativeAttention(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (ff): XLNetFeedForward(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----           )
    # ---- NOTE ----           (1): XLNetLayer(
    # ---- NOTE ----             (rel_attn): XLNetRelativeAttention(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (ff): XLNetFeedForward(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----           )
    # ---- NOTE ----           (2): XLNetLayer(
    # ---- NOTE ----             (rel_attn): XLNetRelativeAttention(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (ff): XLNetFeedForward(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----           )
    # ---- NOTE ----           (3): XLNetLayer(
    # ---- NOTE ----             (rel_attn): XLNetRelativeAttention(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (ff): XLNetFeedForward(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----           )
    # ---- NOTE ----           (4): XLNetLayer(
    # ---- NOTE ----             (rel_attn): XLNetRelativeAttention(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (ff): XLNetFeedForward(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----           )
    # ---- NOTE ----           (5): XLNetLayer(
    # ---- NOTE ----             (rel_attn): XLNetRelativeAttention(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (ff): XLNetFeedForward(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----           )
    # ---- NOTE ----           (6): XLNetLayer(
    # ---- NOTE ----             (rel_attn): XLNetRelativeAttention(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (ff): XLNetFeedForward(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----           )
    # ---- NOTE ----           (7): XLNetLayer(
    # ---- NOTE ----             (rel_attn): XLNetRelativeAttention(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (ff): XLNetFeedForward(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----           )
    # ---- NOTE ----           (8): XLNetLayer(
    # ---- NOTE ----             (rel_attn): XLNetRelativeAttention(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (ff): XLNetFeedForward(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----           )
    # ---- NOTE ----           (9): XLNetLayer(
    # ---- NOTE ----             (rel_attn): XLNetRelativeAttention(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (ff): XLNetFeedForward(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----           )
    # ---- NOTE ----           (10): XLNetLayer(
    # ---- NOTE ----             (rel_attn): XLNetRelativeAttention(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (ff): XLNetFeedForward(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----           )
    # ---- NOTE ----           (11): XLNetLayer(
    # ---- NOTE ----             (rel_attn): XLNetRelativeAttention(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (ff): XLNetFeedForward(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----           )
    # ---- NOTE ----           (12): XLNetLayer(
    # ---- NOTE ----             (rel_attn): XLNetRelativeAttention(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (ff): XLNetFeedForward(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----           )
    # ---- NOTE ----           (13): XLNetLayer(
    # ---- NOTE ----             (rel_attn): XLNetRelativeAttention(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (ff): XLNetFeedForward(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----           )
    # ---- NOTE ----           (14): XLNetLayer(
    # ---- NOTE ----             (rel_attn): XLNetRelativeAttention(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (ff): XLNetFeedForward(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----           )
    # ---- NOTE ----           (15): XLNetLayer(
    # ---- NOTE ----             (rel_attn): XLNetRelativeAttention(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (ff): XLNetFeedForward(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----           )
    # ---- NOTE ----           (16): XLNetLayer(
    # ---- NOTE ----             (rel_attn): XLNetRelativeAttention(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (ff): XLNetFeedForward(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----           )
    # ---- NOTE ----           (17): XLNetLayer(
    # ---- NOTE ----             (rel_attn): XLNetRelativeAttention(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (ff): XLNetFeedForward(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----           )
    # ---- NOTE ----           (18): XLNetLayer(
    # ---- NOTE ----             (rel_attn): XLNetRelativeAttention(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (ff): XLNetFeedForward(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----           )
    # ---- NOTE ----           (19): XLNetLayer(
    # ---- NOTE ----             (rel_attn): XLNetRelativeAttention(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (ff): XLNetFeedForward(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----           )
    # ---- NOTE ----           (20): XLNetLayer(
    # ---- NOTE ----             (rel_attn): XLNetRelativeAttention(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (ff): XLNetFeedForward(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----           )
    # ---- NOTE ----           (21): XLNetLayer(
    # ---- NOTE ----             (rel_attn): XLNetRelativeAttention(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (ff): XLNetFeedForward(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----           )
    # ---- NOTE ----           (22): XLNetLayer(
    # ---- NOTE ----             (rel_attn): XLNetRelativeAttention(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (ff): XLNetFeedForward(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----           )
    # ---- NOTE ----           (23): XLNetLayer(
    # ---- NOTE ----             (rel_attn): XLNetRelativeAttention(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (ff): XLNetFeedForward(
    # ---- NOTE ----               (layer_norm): XLNetLayerNorm()
    # ---- NOTE ----               (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
    # ---- NOTE ----               (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
    # ---- NOTE ----               (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----             )
    # ---- NOTE ----             (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----           )
    # ---- NOTE ----         )
    # ---- NOTE ----         (dropout): Dropout(p=0.1, inplace=False)
    # ---- NOTE ----       )
    # ---- NOTE ----       (start_logits): PoolerStartLogits(
    # ---- NOTE ----         (dense): Linear(in_features=1024, out_features=1, bias=True)
    # ---- NOTE ----       )
    # ---- NOTE ----       (end_logits): PoolerEndLogits(
    # ---- NOTE ----         (dense_0): Linear(in_features=2048, out_features=1024, bias=True)
    # ---- NOTE ----         (activation): Tanh()
    # ---- NOTE ----         (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    # ---- NOTE ----         (dense_1): Linear(in_features=1024, out_features=1, bias=True)
    # ---- NOTE ----       )
    # ---- NOTE ----       (answer_class): PoolerAnswerClass(
    # ---- NOTE ----         (dense_0): Linear(in_features=2048, out_features=1024, bias=True)
    # ---- NOTE ----         (activation): Tanh()
    # ---- NOTE ----         (dense_1): Linear(in_features=1024, out_features=1, bias=False)
    # ---- NOTE ----       )
    # ---- NOTE ----     )
    # ---- NOTE ----     STDOUT-INFO: [2019-08-13T14:52:46.560318][main @ app_pytorch_language_understanding_transformers_helper.py:322][10]: optimizer_configuration={'optimizer': AdamW (
    # ---- NOTE ----     Parameter Group 0
    # ---- NOTE ----         betas: (0.9, 0.999)
    # ---- NOTE ----         correct_bias: True
    # ---- NOTE ----         eps: 1e-06
    # ---- NOTE ----         initial_lr: 5e-05
    # ---- NOTE ----         lr: 5e-05
    # ---- NOTE ----         weight_decay: 0.0
    # ---- NOTE ----
    # ---- NOTE ----     Parameter Group 1
    # ---- NOTE ----         betas: (0.9, 0.999)
    # ---- NOTE ----         correct_bias: True
    # ---- NOTE ----         eps: 1e-06
    # ---- NOTE ----         initial_lr: 5e-05
    # ---- NOTE ----         lr: 5e-05
    # ---- NOTE ----         weight_decay: 0.0
    # ---- NOTE ----     ), 'scheduler': <pytorch_transformers.optimization.WarmupLinearSchedule object at 0x000001A44684C978>, 'optimizer_number_training_optimization_steps': 10, 'device_use_gpu_fp16': False, 'optimizer_learning_rate': 5e-05, 'optimizer_warmup_proportion': 0.1, 'optimizer_warmup_steps': 0, 'optimizer_adam_epsilon': 1e-06, 'optimizer_weight_decay': 0.0, 'device_gpu_fp16_opt_level': None}
